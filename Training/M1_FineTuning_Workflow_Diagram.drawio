<mxfile host="65bd71144e">
    <diagram name="M1 Fine-Tuning Workflow" id="workflow-diagram">
        <mxGraphModel dx="1516" dy="711" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="1169" pageHeight="827" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="title" value="M1 Fine-Tuning to GGUF to Docker/Ollama Workflow" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=20;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="400" y="20" width="400" height="30" as="geometry"/>
                </mxCell>
                <object label="A2: LoRA Fine-Tuning (Metal MPS)" tooltip="A2 represents the native Mac M1 fine-tuning approach using PyTorch with Metal Performance Shaders (MPS) backend. This approach provides full control over the training process with PyTorch + PEFT (Parameter Efficient Fine-Tuning) libraries, leveraging Apple&#39;s Metal framework for GPU acceleration on Mac M1/M2/M3 chips. A2 is ideal for developers who need complete control over the training loop, want to use established PyTorch workflows, and have sufficient unified memory (28GB+ recommended). The approach produces LoRA adapter weights that can be converted to GGUF format for deployment in Ollama or merged with base models for production use. Key advantages include native Mac compatibility, full PyTorch ecosystem support, and the ability to resume training from checkpoints. However, training is slower compared to optimized frameworks and lacks advanced memory optimizations like bitsandbytes (bnb). Bitsandbytes is a library that provides quantization and optimization techniques for deep learning models, offering 8-bit optimizers, 4-bit/8-bit quantization, and gradient checkpointing to reduce GPU memory usage. However, bnb has limited support on Mac M1 compared to CUDA systems, which is why A2 requires more memory (~28GB) and has slower training speeds. The output artifacts include adapter_model.safetensors files with configuration, which can be converted to GGUF adapters or merged into full models for deployment." id="a2-box">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e1f5fe;strokeColor=#01579b;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="50" y="80" width="200" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="a2-details" value="PyTorch + PEFT + Metal&#xa;fp16/bf16 format&#xa;~28GB unified memory&#xa;2-3h per epoch&#xa;Output: adapter_model.safetensors" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=10;" parent="1" vertex="1">
                    <mxGeometry x="50" y="125" width="200" height="80" as="geometry"/>
                </mxCell>
                <object label="B1: llama.cpp GGUF LoRA" tooltip="B1 represents the lightweight CPU-based fine-tuning approach using llama.cpp&#39;s native GGUF LoRA training capabilities. This approach bypasses PyTorch entirely and works directly with quantized GGUF models, making it ideal for resource-constrained environments or when you want to avoid Python dependencies. B1 uses llama.cpp&#39;s C++ finetune implementation, which is highly optimized for CPU inference and training. The approach is particularly suitable for systems with limited memory (6-12GB RAM) and provides a lightweight alternative to PyTorch-based training. Key advantages include no Python dependencies, lower memory requirements, and direct GGUF output format. However, it offers minimal training features compared to PyTorch, is CPU-bound (slower than GPU training), and has limited customization options. The output is a native adapter.gguf file that can be used directly in Ollama or merged with base models. This approach is ideal for quick testing, CPU-only environments, or when you need a lightweight training solution without the complexity of PyTorch setup." id="b1-box">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#f3e5f5;strokeColor=#4a148c;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="50" y="220" width="200" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="b1-details" value="llama.cpp finetune (C++)&#xa;Q4/Q8 quantization&#xa;6-12GB RAM&#xa;CPU-bound (slower)&#xa;Output: adapter.gguf" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=10;" parent="1" vertex="1">
                    <mxGeometry x="50" y="265" width="200" height="80" as="geometry"/>
                </mxCell>
                <object label="C: Merged Full Model Conversion" tooltip="C represents the production-ready model conversion phase that merges LoRA adapters with base models and converts them to optimized GGUF format for deployment. This approach takes LoRA weights from either A2 (PyTorch-trained) or B1 (llama.cpp-trained) and merges them with the original base model to create a single, self-contained model file. The process involves merging LoRA weights into the base model using PyTorch, then converting the merged model to GGUF format using llama.cpp&#39;s conversion tools, followed by quantization to reduce file size and improve inference speed. This approach is ideal for creating production-ready models that can be deployed directly in Ollama or packaged for Docker. Key advantages include single-file deployment, fast Ollama loading, production-ready quantization, and elimination of adapter dependencies. The process typically takes 15-25 minutes for a 7B model and requires 8-12GB RAM. The output is a merged.Q4_K_M.gguf file that contains the complete model with all fine-tuned weights integrated, ready for immediate deployment without any additional setup or adapter files." id="c-box">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e8f5e8;strokeColor=#1b5e20;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="440" y="135" width="200" height="70" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="c-details" value="Merge LoRA to HF to GGUF&#xa;8-12GB RAM&#xa;15-25 min total&#xa;Quantize after merge&#xa;Output: merged.Q4_K_M.gguf" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=10;" parent="1" vertex="1">
                    <mxGeometry x="445" y="209" width="200" height="77" as="geometry"/>
                </mxCell>
                <object label="D: Docker Desktop Models" tooltip="D represents the final deployment phase using Docker Desktop&#39;s native model management system. This approach packages the merged GGUF model from C into an OCI (Open Container Initiative) model artifact that can be managed and run directly through Docker Desktop&#39;s Models tab or Docker Compose. D provides the most user-friendly deployment experience with one-click model launching, built-in model management, and seamless integration with Docker&#39;s ecosystem. The approach uses Docker&#39;s Model Runner API to serve models with minimal resource overhead and instant startup times. Key advantages include native Docker Desktop integration, one-click deployment, portable model artifacts, and professional-grade model serving capabilities. The system supports both GUI-based model management through Docker Desktop&#39;s Models tab and programmatic deployment via Docker Compose with models: blocks. This approach is ideal for production deployments, model distribution, and scenarios requiring professional-grade model serving infrastructure. The output is a portable OCI model artifact that can be easily shared, versioned, and deployed across different environments while maintaining consistent performance and reliability." id="d-box">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fff3e0;strokeColor=#e65100;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="850" y="150" width="200" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="d-details" value="OCI packaged model&#xa;Negligible memory (run-only)&#xa;Instant launch&#xa;Docker Model Runner&#xa;Output: OCI model artifact" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=top;whiteSpace=wrap;rounded=0;fontSize=10;" parent="1" vertex="1">
                    <mxGeometry x="854" y="195" width="200" height="80" as="geometry"/>
                </mxCell>
                <mxCell id="a2-to-c" value="Convert LoRA to Merged Model" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeColor=#1976d2;strokeWidth=3;" parent="1" source="a2-box" target="c-box" edge="1">
                    <mxGeometry relative="1" as="geometry">
                        <Array as="points">
                            <mxPoint x="470" y="100"/>
                            <mxPoint x="470" y="100"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
                <mxCell id="b1-to-c" value="Merge GGUF LoRA to Full Model" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeColor=#7b1fa2;strokeWidth=3;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="b1-box" target="c-box" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="c-to-d" value="Package for Docker Deployment" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeColor=#388e3c;strokeWidth=3;" parent="1" source="c-box" target="d-box" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <object label="WORKFLOW EXPLANATION:&#xa;A2 and B1 are alternative starting points for fine-tuning&#xa;Both can be converted/merged to C (production model)&#xa;C is then packaged for D (Docker deployment)" tooltip="This workflow explanation provides a comprehensive overview of the fine-tuning to deployment pipeline, highlighting the modular and flexible nature of the approach. The workflow is designed with multiple entry points and conversion paths to accommodate different use cases, resource constraints, and deployment requirements. A2 (PyTorch + Metal) and B1 (llama.cpp) represent alternative starting points for fine-tuning, each optimized for different scenarios - A2 for full control and native Mac performance, B1 for lightweight CPU-based training. Both approaches can be converted or merged to C (production model conversion), which serves as the central hub for creating production-ready models. C takes LoRA adapters from either A2 or B1 and merges them with base models, then converts to optimized GGUF format with quantization. Finally, C&#39;s output is packaged for D (Docker deployment), which provides professional-grade model serving through Docker Desktop&#39;s native model management system. This modular design allows users to choose the most appropriate path based on their resources, requirements, and deployment goals, while maintaining compatibility and interoperability between all approaches." id="workflow-explanation">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#e3f2fd;align=left;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="50" y="350" width="500" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="lifecycle-title" value="Recommended Hybrid Lifecycle (Step-by-Step)" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=16;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="50" y="450" width="400" height="30" as="geometry"/>
                </mxCell>
                <object label="1. Fine-tune LoRA&#xa;Mac M1 Metal PEFT" tooltip="Step 1 represents the initial fine-tuning phase using LoRA (Low-Rank Adaptation) on Mac M1 with Metal Performance Shaders (MPS) backend. This step uses PyTorch with PEFT (Parameter Efficient Fine-Tuning) libraries to train LoRA adapters on your custom dataset. The process leverages Apple&#39;s Metal framework for GPU acceleration, making it ideal for Mac M1/M2/M3 systems with sufficient unified memory (28GB+ recommended). This approach provides full control over the training process, supports gradient checkpointing and accumulation, and allows for checkpoint resumption. The training typically takes 2-3 hours per epoch for a 7B model, depending on dataset size and sequence length. The output is adapter_model.safetensors files with configuration, which contain the learned LoRA weights that can be applied to the base model. This step is crucial for creating domain-specific or task-specific adaptations of large language models while maintaining efficiency and compatibility with the original model architecture." id="step1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e1f5fe;strokeColor=#01579b;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="50" y="510" width="120" height="50" as="geometry"/>
                    </mxCell>
                </object>
                <object label="2. Convert to GGUF&#xa;Mac or Linux CPU" tooltip="Step 2 involves converting the LoRA adapter weights from PyTorch format to GGUF (GGML Universal Format) for compatibility with llama.cpp and Ollama. This conversion process uses specialized tools like convert_lora_to_gguf.py to transform the adapter_model.safetensors files into adapter.gguf format. The conversion can be performed on Mac or Linux CPU systems and typically takes 5-10 minutes for a 7B model. This step is essential for making the LoRA adapter compatible with the llama.cpp ecosystem and Ollama deployment. The GGUF format is optimized for inference and provides better performance characteristics compared to PyTorch formats. The output adapter.gguf file can be directly used in Ollama with the ADAPTER directive in Modelfiles, allowing for efficient LoRA-based inference without the need to merge the adapter with the base model. This step bridges the gap between PyTorch training and production deployment in the llama.cpp ecosystem." id="step2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#f3e5f5;strokeColor=#4a148c;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="200" y="510" width="120" height="50" as="geometry"/>
                    </mxCell>
                </object>
                <object label="3. Test in Ollama&#xa;Apply ADAPTER" tooltip="Step 3 involves testing the converted LoRA adapter in Ollama to validate the fine-tuning results before proceeding to production deployment. This step uses Ollama&#39;s ADAPTER directive in Modelfiles to apply the adapter.gguf file to a base model, creating a working LoRA model for testing. The process involves creating a Modelfile with &#39;FROM base_model&#39; and &#39;ADAPTER ./adapter.gguf&#39; directives, then using &#39;ollama create&#39; and &#39;ollama run&#39; commands to test the model. This step is crucial for validating that the fine-tuning was successful, checking model performance on test cases, and ensuring the adapter works correctly with the base model. The testing phase allows for quick iteration and validation before committing to the more time-consuming merge and quantization process. This step typically takes 1-2 minutes to set up and allows for immediate testing of the fine-tuned model&#39;s capabilities, response quality, and performance characteristics in a production-like environment." id="step3">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e8f5e8;strokeColor=#1b5e20;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="350" y="510" width="120" height="50" as="geometry"/>
                    </mxCell>
                </object>
                <object label="4. Merge for Production&#xa;HF to GGUF to Quantize" tooltip="Step 4 represents the production model creation phase where LoRA adapters are merged with base models and converted to optimized GGUF format with quantization. This process involves merging the LoRA weights into the base Hugging Face model using PyTorch, then converting the merged model to GGUF format using llama.cpp&#39;s conversion tools, followed by quantization to reduce file size and improve inference speed. The merge process typically takes 8-15 minutes, conversion takes 5-10 minutes, and quantization takes 5-8 minutes, totaling 15-25 minutes for a 7B model. This step creates a single, self-contained model file (merged.Q4_K_M.gguf) that contains all the fine-tuned weights integrated into the base model. The quantized model is optimized for production deployment with reduced memory requirements and faster inference speeds. This step is essential for creating production-ready models that can be deployed without adapter dependencies and provides the best performance characteristics for end-user applications." id="step4">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fff3e0;strokeColor=#e65100;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="500" y="510" width="120" height="50" as="geometry"/>
                    </mxCell>
                </object>
                <object label="5. Package for Docker&#xa;Tar and import to Models" tooltip="Step 5 involves packaging the merged and quantized GGUF model for Docker Desktop deployment. This process creates a portable OCI (Open Container Initiative) model artifact that can be managed through Docker Desktop&#39;s native model management system. The packaging involves creating a tar archive of the model file with proper metadata and checksums, then importing it into Docker Desktop&#39;s Models tab using &#39;docker models import&#39; or the GUI import functionality. This step typically takes 1-2 minutes and creates a portable model artifact that can be easily shared, versioned, and deployed across different environments. The packaged model includes all necessary metadata for Docker Desktop&#39;s Model Runner to serve the model efficiently. This step bridges the gap between the production-ready GGUF model and Docker&#39;s containerized deployment infrastructure, enabling professional-grade model serving with minimal resource overhead and instant startup capabilities." id="step5">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fce4ec;strokeColor=#880e4f;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="650" y="510" width="120" height="50" as="geometry"/>
                    </mxCell>
                </object>
                <object label="6. Run in Docker Desktop&#xa;Launch and connect" tooltip="Step 6 represents the final deployment phase where the packaged model is launched and made available through Docker Desktop&#39;s native model management system. This step involves launching the model through Docker Desktop&#39;s Models tab or Docker Compose, creating a hosted model endpoint that can be accessed by applications and services. The deployment uses Docker&#39;s Model Runner API to serve the model with minimal resource overhead and instant startup capabilities. This step provides professional-grade model serving with built-in management, monitoring, and scaling capabilities. The deployed model can be accessed via REST APIs, integrated with applications through Docker Compose, or managed through Docker Desktop&#39;s GUI interface. This step typically takes seconds to launch and provides a production-ready model serving infrastructure that can handle real-world workloads. The final result is a fully functional, containerized model deployment that can be easily managed, scaled, and integrated into larger application architectures." id="step6">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#f1f8e9;strokeColor=#33691e;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="800" y="510" width="120" height="50" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="lifecycle-arrow1" value="" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeColor=#666666;strokeWidth=2;" parent="1" source="step1" target="step2" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="lifecycle-arrow2" value="" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeColor=#666666;strokeWidth=2;" parent="1" source="step2" target="step3" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="lifecycle-arrow3" value="" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeColor=#666666;strokeWidth=2;" parent="1" source="step3" target="step4" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="lifecycle-arrow4" value="" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeColor=#666666;strokeWidth=2;" parent="1" source="step4" target="step5" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="lifecycle-arrow5" value="" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeColor=#666666;strokeWidth=2;" parent="1" source="step5" target="step6" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="metrics-title" value="Performance Metrics (Mac M1 32GB)" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=16;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="55" y="610" width="300" height="30" as="geometry"/>
                </mxCell>
                <object label="Memory: A2(~28GB) B1(6-12GB) C(8-12GB) D(Negligible)&#xa;Speed: A2(2-3h/epoch) B1(CPU-bound) C(15-25min) D(Instant)&#xa;Inference: A2(22-30 tok/s) B1(20-25 tok/s) C(22-30 tok/s) D(25 tok/s)&#xa;Ease: A2(4 stars) B1(2 stars) C(3 stars) D(4 stars)" tooltip="This performance metrics table provides comprehensive benchmarks and measurements for the fine-tuning workflow on Mac M1 32GB systems. The metrics cover memory usage, training speed, inference performance, and ease of use across all four approaches (A2, B1, C, D). Memory requirements range from negligible (D) to ~28GB unified (A2), with B1 and C requiring 6-12GB and 8-12GB respectively. Training speeds vary from 2-3 hours per epoch (A2) to CPU-bound performance (B1), with C taking 15-25 minutes for merge and conversion. Inference speeds are consistent across approaches at 22-30 tokens/second for A2 and C, 20-25 tokens/second for B1, and 25 tokens/second for D. Ease of use ratings range from 2 stars (B1) to 4 stars (A2 and D), with C receiving 3 stars. These metrics help users make informed decisions about which approach best fits their hardware capabilities, time constraints, and performance requirements. The benchmarks are based on real-world testing with 7B models and provide practical guidance for Mac M1 users planning their fine-tuning and deployment strategy." id="metrics-table">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#f5f5f5;align=left;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=9;" parent="1" vertex="1">
                        <mxGeometry x="55" y="640" width="310" height="80" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="timing-title" value="Build Timing (Route C to D)" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="520" y="610" width="200" height="30" as="geometry"/>
                </mxCell>
                <object label="Merge LoRA to HF: 8-15 min&#xa;Convert HF to GGUF: 5-10 min&#xa;Quantize GGUF: 5-8 min&#xa;Package for Docker: 1-2 min&#xa;&#xa;Total: 20-35 minutes (7B model)&#xa;Updates: 15-25 minutes" tooltip="This timing breakdown provides detailed estimates for the production model creation pipeline (Route C to D) on Mac M1 32GB systems. The merge process (8-15 min) involves integrating LoRA adapter weights with the base Hugging Face model using PyTorch, creating a single merged model with all fine-tuned weights integrated. The conversion step (5-10 min) transforms the merged Hugging Face model to GGUF format using llama.cpp&#39;s conversion tools, optimizing the model for inference performance. Quantization (5-8 min) reduces the model size and improves inference speed by converting weights to lower precision (typically Q4_K_M), while maintaining acceptable quality. The packaging step (1-2 min) creates a tar archive with proper metadata for Docker Desktop import. The total time of 20-35 minutes for a 7B model includes all steps, while subsequent updates (15-25 min) skip the initial merge and focus on conversion and quantization. These timings are based on real-world testing with Mac M1 32GB systems and provide practical guidance for production deployment planning." id="timing-details">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#f5f5f5;align=left;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="520" y="640" width="250" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="compat-title" value="Interchangeability Matrix" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=16;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="50" y="770" width="210" height="30" as="geometry"/>
                </mxCell>
                <object label="A2 to B1: Convert LoRA to GGUF&lt;br&gt;B1 to C: Merge or export GGUF&lt;br&gt;C to D: D runs C&#39;s GGUF&lt;br&gt;A2 to C to D: Full loop compatible&lt;br&gt;&lt;br&gt;All approaches share same base checkpoint&lt;br&gt;and tokenizer lineage for seamless transitions" tooltip="This interchangeability matrix demonstrates the modular and flexible nature of the fine-tuning workflow, showing how all four approaches (A2, B1, C, D) can be seamlessly interconnected and converted between each other. The matrix reveals that A2 (PyTorch LoRA) can be converted to B1 (GGUF LoRA) using convert_lora_to_gguf.py, allowing users to switch from PyTorch-based training to llama.cpp-based deployment. B1 can be merged or exported to C (production model) by integrating the GGUF LoRA with the base model. C&#39;s output can be directly consumed by D (Docker deployment) without any additional conversion. The full A2 to C to D loop represents the complete production pipeline from training to deployment. Most importantly, all approaches share the same base checkpoint and tokenizer lineage, ensuring compatibility and preventing conflicts between different workflow stages. This modular design allows users to start with any approach and transition to others based on their changing requirements, resource constraints, or deployment needs, while maintaining full compatibility and avoiding the need to retrain or reconfigure models." id="compat-matrix">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#e8f5e8;align=center;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="50" y="800" width="280" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="pros-cons-title" value="Key Trade-offs" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=16;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="520" y="770" width="130" height="30" as="geometry"/>
                </mxCell>
                <object label="&lt;div&gt;&lt;span style=&quot;background-color: transparent;&quot;&gt;Advantages:&lt;/span&gt;&lt;/div&gt;A2: Native Mac, full control&lt;br&gt;B1: Lightweight, no Python deps&lt;br&gt;C: One final GGUF, fast Ollama load&lt;br&gt;D: Portable, 1-click run&lt;br&gt;&lt;br&gt;Disadvantages:&lt;br&gt;A2: Slow training, no bnb optimizations&lt;br&gt;B1: Minimal training features&lt;br&gt;C: Needs conversion each update&lt;br&gt;D: Not trainable, runtime only" tooltip="This trade-offs analysis provides a comprehensive comparison of the advantages and disadvantages across all four fine-tuning approaches (A2, B1, C, D) to help users make informed decisions based on their specific requirements and constraints. A2 (PyTorch LoRA) offers native Mac compatibility with full control over the training process, but suffers from slow training speeds and lacks advanced memory optimizations like bitsandbytes (bnb). Bitsandbytes is a library that provides quantization and optimization techniques for deep learning models, offering 8-bit optimizers, 4-bit/8-bit quantization, and gradient checkpointing to reduce GPU memory usage. However, Bitsandbytes has limited support on Mac M1 compared to CUDA systems, which is why A2 requires more memory (~28GB) and has slower training speeds. B1 (llama.cpp GGUF LoRA) provides a lightweight solution with no Python dependencies, making it ideal for resource-constrained environments, but offers minimal training features and is CPU-bound. C (Merged Model) creates a single, optimized GGUF file for fast Ollama loading, but requires conversion for each update and lacks the flexibility of adapter-based approaches. D (Docker Desktop) offers portable, one-click deployment with professional-grade model serving, but is runtime-only and cannot be used for training. This analysis helps users understand the trade-offs between control vs. simplicity, performance vs. resource requirements, and flexibility vs. optimization, enabling them to choose the most appropriate approach for their specific use case, hardware capabilities, and deployment requirements." id="pros-cons">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#fff3e0;align=center;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="520" y="800" width="260" height="160" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="legend-title" value="Legend" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="674" y="350" width="100" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend-a2" value="A2: LoRA Training" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e1f5fe;strokeColor=#01579b;fontSize=9;" parent="1" vertex="1">
                    <mxGeometry x="674" y="380" width="80" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend-b1" value="B1: GGUF LoRA" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#f3e5f5;strokeColor=#4a148c;fontSize=9;" parent="1" vertex="1">
                    <mxGeometry x="774" y="380" width="80" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend-c" value="C: Merged Model" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e8f5e8;strokeColor=#1b5e20;fontSize=9;" parent="1" vertex="1">
                    <mxGeometry x="874" y="380" width="80" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend-d" value="D: Docker Deploy" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fff3e0;strokeColor=#e65100;fontSize=9;" parent="1" vertex="1">
                    <mxGeometry x="974" y="380" width="80" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="comparison-title" value="Fine-Tuning → GGUF → Docker/Ollama Comparison (Mac M1 32 GB Edition)" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=16;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="70" y="1020" width="500" height="30" as="geometry"/>
                </mxCell>
                <object label="&lt;div&gt;&lt;br&gt;&lt;/div&gt;📊 **Comprehensive Comparison Overview**&lt;br&gt;&lt;br&gt;This detailed comparison covers 20 key aspects across 4 fine-tuning approaches:&lt;br&gt;&lt;br&gt;• **A2 (LoRA Training)**: Native Mac training with PyTorch + Metal&lt;br&gt;• **B1 (GGUF LoRA)**: Lightweight CPU-based training with llama.cpp&lt;br&gt;• **C (Merged Model)**: Production-ready merged and quantized models&lt;br&gt;• **D (Docker Deploy)**: Containerized deployment via Docker Desktop&lt;br&gt;&lt;br&gt;**Key Metrics Compared**: Memory usage, training speed, inference performance, ease of use, Docker integration, and workflow compatibility. Each approach is evaluated across technical specifications, deployment options, and practical use cases for Mac M1 32GB systems." tooltip="This comprehensive comparison overview provides a detailed analysis of the four fine-tuning approaches (A2, B1, C, D) across 20 key technical and practical aspects. The comparison evaluates each approach based on goal and use case, Mac M1 32GB support, base format, training backend, quantization capabilities, memory requirements, speed performance, dataset format, prompt templates, packing support, output artifacts, conversion tools, deployment options, Docker integration, continual retraining capabilities, interchangeability, ease of use, advantages, disadvantages, and best use cases. A2 (LoRA Training) represents native Mac training with PyTorch + Metal, offering full control but requiring significant memory. B1 (GGUF LoRA) provides lightweight CPU-based training with llama.cpp, ideal for resource-constrained environments. C (Merged Model) creates production-ready merged and quantized models for deployment. D (Docker Deploy) offers containerized deployment via Docker Desktop with professional-grade model serving. The comparison helps users understand the trade-offs, compatibility, and practical considerations for each approach, enabling informed decision-making based on their specific requirements, hardware capabilities, and deployment goals." id="comparison-summary">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#f5f5f5;align=left;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="65" y="1061" width="1000" height="180" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="comparison-table" value="&lt;div style=&quot;&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;table style=&quot;width: 100%; border-collapse: collapse; border: 2px solid rgb(51, 51, 51); font-size: 9px;&quot;&gt;&lt;tbody&gt;&lt;tr style=&quot;background-color: rgb(240, 240, 240);&quot;&gt;&lt;th style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); width: 15%;&quot;&gt;Aspect&lt;/th&gt;&lt;th style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); width: 21%;&quot;&gt;A2 — LoRA (fp16/bf16 on Metal)&lt;/th&gt;&lt;th style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); width: 21%;&quot;&gt;B1 — llama.cpp GGUF LoRA (no PyTorch)&lt;/th&gt;&lt;th style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); width: 21%;&quot;&gt;C — Merged Full Model Conversion&lt;/th&gt;&lt;th style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); width: 21%;&quot;&gt;D — Final Artifact in Docker Desktop&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Goal / Use Case&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Fine-tune 7B model locally on Mac (Metal)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Train LoRA adapter directly on GGUF base (CPU/GPU)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Merge LoRA → single HF → GGUF for deployment&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Package the merged/quantized GGUF as a Docker Model artifact&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Mac M1 32 GB Support&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ Full (Metal mps)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;⚙️ Possible (CPU, slower)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ Yes (run merge &amp;amp; convert on CPU)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ Yes (run and manage through Docker Desktop Models)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Base Format&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Hugging Face fp16/bf16&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;GGUF quantized&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;HF merged → GGUF&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;GGUF (OCI packaged model)&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Training Backend&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;PyTorch + PEFT + Metal (mps)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;llama.cpp finetune (C++)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;PyTorch merge → llama.cpp convert&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Docker Model Runner (OCI model runtime)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Quantization at Train Time&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;None (fp16/bf16)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Q4 / Q8&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;None (quantize after merge)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Already quantized&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Typical Memory Need&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;~28 GB unified&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;6 – 12 GB RAM&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;8 – 12 GB RAM&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Negligible (run-only)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Speed on M1&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;🐢 2–3 h per epoch&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;🐌 CPU-bound&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;⚡ Merge &amp;amp; convert ≈ 15–25 min (7 B)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;🚀 Instant launch (run model only)&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Dataset Format&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;JSONL {&quot;instruction&quot;,&quot;input&quot;,&quot;output&quot;}&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;same structure (as A2)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;same structure (as A2)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;none (uses final GGUF)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Prompt Template&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;&amp;lt;|system|&amp;gt;… / ChatML&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;same as A2&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;same as A2&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;same prompts at runtime&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Packing Support&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;manual token packing&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;N/A&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Output Artifact&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;adapter_model.safetensors + config&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;adapter.gguf&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;merged.Q4_K_M.gguf&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Docker Model artifact (OCI or tar)&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Conversion Tool → Ollama/Docker&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;convert_lora_to_gguf.py&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;none (creates GGUF directly)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;convert-hf-to-gguf.py → quantize&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Docker Desktop &quot;Import Model&quot; or docker models import&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Ollama Deployment&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;FROM base + ADAPTER ./my_adapter.gguf&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;same pattern&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;FROM merged.Q4_K_M.gguf&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Runs outside Ollama (using Docker Model Runner API / Compose)&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Docker Desktop Integration&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ (run Ollama in Docker Desktop)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ (bare llama.cpp container)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ (supply GGUF to Docker Desktop)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;⭐ Native (Docker Desktop Models tab / Compose models: block)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Continual Retraining&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ resume LoRA&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ repeat finetune&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;⚙️ re-merge each round&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;❌ (no training inside Docker Desktop)&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Interchange A2 ⇄ B1 ⇄ C ⇄ D&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ Convert adapter → GGUF to move to B1/C/D&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ Use adapter in Ollama or merge (C)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ Export to GGUF → Docker (D)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;✅ Consume merged artifact from C&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Ease of Use on Mac&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;⭐⭐⭐⭐&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;⭐⭐&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;⭐⭐⭐&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;⭐⭐⭐⭐ (1-click run in Docker Desktop)&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Advantages&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Works on Mac natively; full control&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Lightweight; no Python deps&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;One final GGUF; fast Ollama load&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Portable; runnable via Docker Desktop UI or Compose&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold; background-color: rgb(249, 249, 249);&quot;&gt;Disadvantages&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Slow training; no bnb optimizations&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Minimal training features&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Needs conversion each update&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Not trainable; only runtime&lt;/td&gt;&lt;/tr&gt;&lt;tr style=&quot;background-color: rgb(249, 249, 249);&quot;&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51); font-weight: bold;&quot;&gt;Best For&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Mac fine-tuning loop (A2 → C)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Quick CPU tests (B1 ↔ C)&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Creating release builds for Ollama or Docker&lt;/td&gt;&lt;td style=&quot;padding: 6px; border: 1px solid rgb(51, 51, 51);&quot;&gt;Distributing and running final models via Docker Desktop&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;" style="text;html=1;strokeColor=#666666;fillColor=#f5f5f5;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=8;" parent="1" vertex="1">
                    <mxGeometry y="1260" width="1160" height="640" as="geometry"/>
                </mxCell>
                <mxCell id="paths-title" value="Workflow Paths Summary" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="80" y="1920" width="180" height="30" as="geometry"/>
                </mxCell>
                <object label="🔄 **Recommended Paths:**&lt;br&gt;&lt;br&gt;• **A2 → C → D**: Full Mac training to Docker deployment&lt;br&gt;• **B1 → C → D**: CPU-based training to Docker deployment&lt;br&gt;• **A2 → B1**: Convert LoRA to GGUF for testing&lt;br&gt;• **C → D**: Direct production model deployment&lt;br&gt;&lt;br&gt;⚡ **Quick Start**: A2 (train) → C (merge) → D (deploy)&lt;br&gt;🧪 **Testing**: A2 → B1 (test GGUF LoRA) → C → D&lt;br&gt;🚀 **Production**: C (merged model) → D (Docker)" tooltip="This recommended paths section provides strategic guidance for navigating the fine-tuning workflow based on different use cases, resource constraints, and deployment requirements. The paths are designed to help users choose the most appropriate route from training to production deployment. A2 → C → D represents the full Mac training to Docker deployment path, ideal for users with sufficient Mac M1 resources who want complete control over the training process and need production deployment. B1 → C → D offers a CPU-based training alternative for resource-constrained environments, using llama.cpp for lightweight training before merging and deploying. A2 → B1 provides a testing path for converting PyTorch LoRA to GGUF format for quick validation before committing to full production. C → D enables direct production model deployment for users who already have merged models. The Quick Start path (A2 → C → D) is recommended for most users wanting the full experience. The Testing path (A2 → B1 → C → D) allows validation of GGUF LoRA before production. The Production path (C → D) is ideal for users with existing merged models ready for deployment. These paths ensure users can adapt their workflow based on changing requirements while maintaining compatibility and avoiding the need to restart from scratch." id="paths-summary">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#e8f5e8;align=center;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="75" y="1960" width="400" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="decision-title" value="Key Decision Factors" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="700" y="1920" width="200" height="30" as="geometry"/>
                </mxCell>
                <object label="&lt;div&gt;&lt;span style=&quot;background-color: transparent;&quot;&gt;🎯 **Choose A2 if:**&lt;/span&gt;&lt;/div&gt;• You have Mac M1 with 32GB+ RAM&lt;br&gt;• Need full control over training process&lt;br&gt;• Want to use PyTorch + PEFT + Metal&lt;br&gt;&lt;br&gt;🎯 **Choose B1 if:**&lt;br&gt;• Limited memory (6-12GB available)&lt;br&gt;• Want lightweight, no Python dependencies&lt;br&gt;• CPU-only environment&lt;br&gt;&lt;br&gt;🎯 **Choose C if:**&lt;br&gt;• Need production-ready merged model&lt;br&gt;• Want single GGUF file for deployment&lt;br&gt;• Ready for final quantization&lt;br&gt;&lt;br&gt;🎯 **Choose D if:**&lt;br&gt;• Need instant deployment&lt;br&gt;• Want Docker Desktop integration&lt;br&gt;• Production runtime environment" tooltip="This decision factors section provides clear guidance for choosing the most appropriate fine-tuning approach based on specific requirements, hardware capabilities, and deployment goals. The decision matrix helps users evaluate their situation and select the optimal path. Choose A2 (PyTorch LoRA) if you have sufficient Mac M1 resources (32GB+ RAM) and need full control over the training process with PyTorch + PEFT + Metal backend. This approach is ideal for developers who want complete control, need to use established PyTorch workflows, and have the hardware resources to support intensive training. Choose B1 (llama.cpp GGUF LoRA) if you have limited memory (6-12GB available), want a lightweight solution without Python dependencies, or are working in a CPU-only environment. This approach is perfect for resource-constrained scenarios or when you need to avoid Python ecosystem complexity. Choose C (Merged Model) if you need production-ready merged models, want a single GGUF file for deployment, or are ready for final quantization. This approach creates optimized, self-contained models for production use. Choose D (Docker Desktop) if you need instant deployment, want Docker Desktop integration, or require a production runtime environment. This approach provides professional-grade model serving with minimal setup. The decision factors consider hardware resources, control requirements, deployment needs, and operational constraints to guide users toward the most suitable approach for their specific use case." id="decision-factors">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#fff3e0;align=center;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="700" y="1950" width="400" height="250" as="geometry"/>
                    </mxCell>
                </object>
            </root>
        </mxGraphModel>
    </diagram>
</mxfile>