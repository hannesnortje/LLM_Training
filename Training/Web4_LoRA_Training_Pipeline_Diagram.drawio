<mxfile host="65bd71144e">
    <diagram name="Web4 LoRA Training Pipeline" id="web4-lora-pipeline">
        <mxGraphModel dx="1516" dy="711" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="1169" pageHeight="827" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="title" value="🧠 Web4 LoRA / QLoRA Training Pipeline (Mac M1)" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=18;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="50" y="20" width="600" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="overview-title" value="🧭 Progressive Training Validation (4 Stages)" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="50" y="80" width="400" height="30" as="geometry"/>
                </mxCell>
                <object label="1️⃣ Dataset Sanity Check&#xa;Goal: Confirm data integrity&#xa;Duration: Minutes | Risk: Low" tooltip="Stage 1 focuses on validating the dataset before any training begins. This critical first step ensures all JSONL files are properly formatted with required fields (task_type, instruction, input, output), tokenization works correctly with Qwen/DeepSeek tokenizers, and the data loads without parsing errors. The expectation is 25k-40k samples with 15-30M tokens total. This stage prevents training failures due to data issues and ensures the foundation is solid before proceeding to actual training phases." id="stage1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e3f2fd;strokeColor=#1976d2;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="50" y="120" width="200" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <object label="2️⃣ Dry Run / Overfit Test&#xa;Goal: Validate training loop&#xa;Duration: &lt; 1h | Risk: Low" tooltip="Stage 2 performs a dry run with 100 samples to validate the training loop and MPS memory behavior on Mac M1. This stage uses a small subset across all buckets, runs 1 epoch with batch size 1 and sequence length 512, and verifies that loss decreases from 3-5 to less than 1 within a few hundred steps. It confirms no NaNs or CUDA/MPS errors and ensures GPU memory stays stable under 12GB on M1 32GB systems. The outcome confirms the pipeline works by demonstrating the model can overfit the tiny dataset." id="stage2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#f3e5f5;strokeColor=#7b1fa2;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="280" y="120" width="200" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <object label="3️⃣ Mini Fine-Tune&#xa;Goal: Verify learning structure&#xa;Duration: 3-4h | Risk: Medium" tooltip="Stage 3 uses 2k-3k samples to verify the model learns tool-calling and coding style patterns. This stage uses balanced samples across Tool, Style, and Guardrail buckets, trains for 1 epoch with batch size 1 and gradient accumulation 8-12, and observes stable loss curves in the 1.0-1.5 range. It generates 10 tool prompts to verify valid JSON output and 10 style prompts to match Web4 code style. The evaluation should achieve Tool ≥80%, Style ≥85%, and Guardrail ≥95% accuracy, confirming the model internalizes the structure and schema." id="stage3">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e8f5e8;strokeColor=#388e3c;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="510" y="120" width="200" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <object label="4️⃣ Full LoRA Training&#xa;Goal: Production adapter&#xa;Duration: 12-20h | Risk: Medium-High" tooltip="Stage 4 trains on the full dataset (25k-40k samples) to create a production-quality LoRA adapter. This stage includes all buckets with LoRA parameters r=8, alpha=16, dropout=0.05, batch size 1, gradient accumulation 12-16, and sequence length 2048. It runs for 1-2 epochs with loss monitoring to ensure it plateaus between 0.6-1.0. The adapter is saved to outputs/&lt;run_name&gt;/lora with the expectation of a stable, clean adapter with no divergence. This is the main training phase that creates the production model." id="stage4">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fff3e0;strokeColor=#f57c00;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="740" y="120" width="200" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow1" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#1976d2;" parent="1" source="stage1" target="stage2" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="250" y="150" as="sourcePoint"/>
                        <mxPoint x="280" y="150" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow2" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#7b1fa2;" parent="1" source="stage2" target="stage3" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="480" y="150" as="sourcePoint"/>
                        <mxPoint x="510" y="150" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow3" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#388e3c;" parent="1" source="stage3" target="stage4" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="710" y="150" as="sourcePoint"/>
                        <mxPoint x="740" y="150" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="post-training-title" value="📊 Post-Training Pipeline" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="50" y="220" width="300" height="30" as="geometry"/>
                </mxCell>
                <object label="5️⃣ Evaluation&#xa;Goal: Quantify performance&#xa;Tool ≥95%, Style ≥90%, Guardrail ≥98%" tooltip="Stage 5 evaluates the trained model on hold-out evaluation sets to quantify performance. This stage runs evaluate.py on all data/eval/*.jsonl files, checking Tool Eval for JSON validity ≥95% and Correct Tool ≥85%, Style Eval for 100% lint pass and AST match ≥90%, and Guardrail Eval for refusal accuracy ≥98%. The mixed evaluation should achieve ≥90% overall accuracy. Results are logged to eval_log.md or CSV. The expectation is high accuracy with no format regressions, confirming the model meets production quality standards." id="evaluation">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fce4ec;strokeColor=#c2185b;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="50" y="260" width="200" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <object label="6️⃣ Merge + Quantize&#xa;Goal: Deployable artifact&#xa;GGUF or adapter format" tooltip="Stage 6 creates deployable artifacts by merging the LoRA adapter with the base model and converting to optimized formats. This stage merges LoRA weights into the Hugging Face model using merge_and_unload, converts to GGUF format via llama.cpp scripts, and quantizes to Q4_K_M or Q5_K_M for optimal performance. The process verifies that evaluation results remain identical or within ±1% after quantization. The expectation is a deployable GGUF file that maintains model quality while reducing file size and improving inference speed for production deployment." id="merge-quantize">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#f1f8e9;strokeColor=#689f38;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="280" y="260" width="200" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <object label="7️⃣ Deployment&#xa;Goal: Production usage&#xa;Ollama or Docker Desktop" tooltip="Stage 7 deploys the trained model for production usage through Ollama or Docker Desktop Models. This stage creates a Modelfile referencing the GGUF artifact, uses &#39;ollama create web4-coder -f Modelfile&#39; to register the model, and tests &#39;ollama run web4-coder&#39; to verify proper structured responses. Optionally, the GGUF can be imported into Docker Desktop&#39;s Models tab for containerized deployment. The expectation is inference behavior identical to evaluation runs, confirming the model works correctly in production environments with the same quality and performance as during training and evaluation phases." id="deployment">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e0f2f1;strokeColor=#00695c;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="510" y="260" width="200" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <object label="8️⃣ Iteration &amp; Improvement&#xa;Goal: Continuous enhancement&#xa;Troubleshooting guide" tooltip="Stage 8 provides continuous improvement and troubleshooting guidance for ongoing model refinement. This stage addresses common issues like loss stagnation (increase epochs or lower learning rate), prose output instead of JSON (add Tool-Neg examples), naming drift (add more Style-Core examples), refusal inconsistency (add Guardrail samples), and evaluation score plateaus (run extra mini fine-tune with smaller learning rate). This iterative approach ensures the model can be continuously improved based on real-world performance feedback and specific use case requirements." id="iteration">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fff8e1;strokeColor=#ff8f00;fontSize=12;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="740" y="260" width="200" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow4" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#c2185b;" parent="1" source="evaluation" target="merge-quantize" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="250" y="290" as="sourcePoint"/>
                        <mxPoint x="280" y="290" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow5" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#689f38;" parent="1" source="merge-quantize" target="deployment" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="480" y="290" as="sourcePoint"/>
                        <mxPoint x="510" y="290" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow6" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#00695c;" parent="1" source="deployment" target="iteration" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="710" y="290" as="sourcePoint"/>
                        <mxPoint x="740" y="290" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="success-title" value="✅ Success Indicators" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="50" y="360" width="200" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="success-table" value="&lt;table border=&quot;1&quot; cellpadding=&quot;4&quot; style=&quot;border-collapse: collapse; width: 100%;&quot;&gt;&lt;tr style=&quot;background-color: #f5f5f5;&quot;&gt;&lt;th&gt;Stage&lt;/th&gt;&lt;th&gt;Purpose&lt;/th&gt;&lt;th&gt;Success Indicator&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Sanity Check&lt;/td&gt;&lt;td&gt;Data valid, tokenizable&lt;/td&gt;&lt;td&gt;No JSON errors&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Dry Run&lt;/td&gt;&lt;td&gt;Loop functional&lt;/td&gt;&lt;td&gt;Loss drops quickly&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Mini Fine-Tune&lt;/td&gt;&lt;td&gt;Learning structure&lt;/td&gt;&lt;td&gt;JSON/code quality visible&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Full Training&lt;/td&gt;&lt;td&gt;Production adapter&lt;/td&gt;&lt;td&gt;Low loss, stable metrics&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Eval&lt;/td&gt;&lt;td&gt;Verify accuracy&lt;/td&gt;&lt;td&gt;≥90–95% scores&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Merge &amp; Quantize&lt;/td&gt;&lt;td&gt;Artifact build&lt;/td&gt;&lt;td&gt;GGUF validated&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Deploy&lt;/td&gt;&lt;td&gt;Real usage&lt;/td&gt;&lt;td&gt;Ollama responds correctly&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;" style="text;html=1;strokeColor=#666666;fillColor=#f5f5f5;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                    <mxGeometry x="50" y="400" width="370" height="200" as="geometry"/>
                </mxCell>
                <mxCell id="params-title" value="🔧 Key Training Parameters" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="500" y="360" width="250" height="30" as="geometry"/>
                </mxCell>
                <object label="&lt;b&gt;LoRA Configuration:&lt;/b&gt;&lt;br&gt;• r=8, alpha=16, dropout=0.05&lt;br&gt;&lt;br&gt;&lt;b&gt;Training Settings:&lt;/b&gt;&lt;br&gt;• Batch size: 1&lt;br&gt;• Gradient accumulation: 12-16&lt;br&gt;• Sequence length: 2048&lt;br&gt;• Epochs: 1-2&lt;br&gt;&lt;br&gt;&lt;b&gt;Memory Requirements:&lt;/b&gt;&lt;br&gt;• Mac M1 32GB recommended&lt;br&gt;• GPU memory &amp;lt; 12GB&lt;br&gt;• Stable MPS behavior&lt;br&gt;&lt;br&gt;&lt;b&gt;Expected Outcomes:&lt;/b&gt;&lt;br&gt;• Loss plateau: 0.6-1.0&lt;br&gt;• Tool accuracy: ≥95%&lt;br&gt;• Style accuracy: ≥90%&lt;br&gt;• Guardrail accuracy: ≥98%" tooltip="This comprehensive training parameters section details the optimal configuration for Web4 LoRA fine-tuning on Mac M1 systems. LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that freezes the original model weights and trains only small adapter matrices, dramatically reducing memory requirements and training time. The r parameter (rank=8) controls the dimensionality of the adapter matrices - higher values increase model capacity but require more memory and training time. The alpha parameter (16) is a scaling factor that controls how much the adapter affects the original model; it&#39;s typically set to 2x the rank value for optimal performance. Dropout (0.05) prevents overfitting by randomly setting 5% of adapter weights to zero during training. Training settings include batch size 1 (due to memory constraints), gradient accumulation 12-16 (simulates larger batch sizes by accumulating gradients), sequence length 2048 (maximum tokens per sample), and 1-2 epochs (complete passes through the dataset). Memory requirements specify Mac M1 32GB as recommended, with GPU memory staying under 12GB and stable MPS (Metal Performance Shaders) behavior. Expected outcomes include loss plateauing between 0.6-1.0, Tool accuracy ≥95% (JSON validity and correct tool selection), Style accuracy ≥90% (code quality and formatting), and Guardrail accuracy ≥98% (refusal behavior for inappropriate requests). These parameters are optimized for Web4 coding tasks and Mac M1 hardware constraints." id="params-content">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#e8f5e8;align=center;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="500" y="400" width="300" height="260" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="troubleshoot-title" value="🔧 Troubleshooting Guide" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="50" y="690" width="200" height="30" as="geometry"/>
                </mxCell>
                <object label="&lt;table border=&quot;1&quot; cellpadding=&quot;4&quot; style=&quot;border-collapse: collapse; width: 100%;&quot;&gt;&lt;tr style=&quot;background-color: #f5f5f5;&quot;&gt;&lt;th&gt;Issue&lt;/th&gt;&lt;th&gt;Action&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Loss stagnates&lt;/td&gt;&lt;td&gt;Increase epochs or lower LR&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Emits prose for tool JSON&lt;/td&gt;&lt;td&gt;Add Tool-Neg examples&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Naming drift&lt;/td&gt;&lt;td&gt;Add more Style-Core examples&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Refusal inconsistency&lt;/td&gt;&lt;td&gt;Add Guardrail samples&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Eval score plateaus&lt;/td&gt;&lt;td&gt;Run extra mini fine-tune with smaller LR&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;" tooltip="This troubleshooting guide provides systematic solutions for common issues encountered during Web4 LoRA training and evaluation. Loss stagnation occurs when the training loss stops decreasing, indicating the model has reached a local minimum or the learning rate is too high. Solutions include increasing epochs (more training time) or lowering the learning rate (smaller parameter updates) to help the model escape local minima and continue learning. When the model emits prose instead of valid JSON for tool calls, it indicates insufficient negative examples in the training data. Adding Tool-Neg examples (incorrect tool usage patterns) teaches the model what NOT to do, improving JSON format compliance. Naming drift occurs when the model generates inconsistent variable names, function names, or coding patterns that don&#39;t match the Web4 style. Adding more Style-Core examples reinforces the desired coding conventions and naming patterns. Refusal inconsistency happens when the model inconsistently refuses inappropriate requests or accepts requests it should refuse. Adding more Guardrail samples (examples of proper refusal behavior) strengthens the model&#39;s safety mechanisms and ethical boundaries. When evaluation scores plateau despite continued training, it suggests the model has learned all it can from the current data. Running an extra mini fine-tune with a smaller learning rate allows for more precise parameter adjustments and can help the model learn subtle patterns it might have missed. This iterative approach ensures continuous improvement and addresses specific weaknesses in the model&#39;s performance." id="troubleshoot-table">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#fff3e0;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="50" y="730" width="360" height="150" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="dataset-title" value="📊 Dataset Requirements" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=14;fontStyle=1;" parent="1" vertex="1">
                    <mxGeometry x="500" y="690" width="200" height="30" as="geometry"/>
                </mxCell>
                <object label="&lt;div&gt;&lt;b style=&quot;background-color: transparent;&quot;&gt;Format Requirements:&lt;/b&gt;&lt;/div&gt;• Valid JSONL (1 object per line)&lt;br&gt;• Fields: task_type, instruction, input, output&lt;br&gt;• Average tokens &amp;lt; 2048 per record&lt;br&gt;&lt;br&gt;&lt;b&gt;Size Expectations:&lt;/b&gt;&lt;br&gt;• 25k-40k samples total&lt;br&gt;• 15-30M tokens&lt;br&gt;• Balanced across buckets&lt;br&gt;&lt;br&gt;&lt;b&gt;Quality Checks:&lt;/b&gt;&lt;br&gt;• No parsing errors&lt;br&gt;• Tokenizer compatibility&lt;br&gt;• Format consistency&lt;br&gt;• Random sample validation" tooltip="This dataset requirements section outlines the essential specifications for Web4 LoRA training data preparation and validation. JSONL (JSON Lines) format is a text format where each line contains a single JSON object, making it ideal for streaming large datasets and ensuring one training example per line. The required fields include task_type (categorizing the training example like &#39;tool_call&#39;, &#39;style_core&#39;, &#39;guardrail&#39;), instruction (the user&#39;s request or prompt), input (context or additional information), and output (the expected model response). The 2048 token limit per record ensures compatibility with the model&#39;s maximum sequence length and prevents memory overflow during training. Size expectations of 25k-40k samples with 15-30M tokens provide sufficient data diversity for effective learning while remaining manageable for Mac M1 memory constraints. Balanced distribution across buckets (Tool, Style, Guardrail) ensures the model learns all required capabilities without bias toward any single task type. Quality checks include parsing validation to catch malformed JSON, tokenizer compatibility testing with Qwen/DeepSeek tokenizers to ensure proper encoding/decoding, format consistency verification across all samples, and random sample validation to spot-check data quality and identify potential issues before training begins. These requirements ensure the dataset is properly formatted, appropriately sized, and high-quality for successful Web4 LoRA fine-tuning." id="dataset-content">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#e3f2fd;align=center;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" parent="1" vertex="1">
                        <mxGeometry x="500" y="730" width="300" height="210" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="timing-title" value="⏱️ Timing &amp; Verification Parameters" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=16;fontStyle=1;" vertex="1" parent="1">
                    <mxGeometry x="50" y="980" width="400" height="40" as="geometry"/>
                </mxCell>
                <object label="&lt;table border=&quot;1&quot; cellpadding=&quot;4&quot; style=&quot;border-collapse: collapse; width: 100%;&quot;&gt;&lt;tr style=&quot;background-color: #f5f5f5;&quot;&gt;&lt;th&gt;Step&lt;/th&gt;&lt;th&gt;Expected Scale&lt;/th&gt;&lt;th&gt;Done Signals&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1️⃣ Dataset Sanity Check&lt;/td&gt;&lt;td&gt;Very short&lt;/td&gt;&lt;td&gt;No parse errors; tokenization succeeds&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2️⃣ Dry Run (100 samples)&lt;/td&gt;&lt;td&gt;Short&lt;/td&gt;&lt;td&gt;Loss drops fast; no MPS errors&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3️⃣ Mini Fine-Tune (2-3k)&lt;/td&gt;&lt;td&gt;Medium&lt;/td&gt;&lt;td&gt;Loss stabilizes; correct JSON &amp; style&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4️⃣ Full LoRA Training (25-40k)&lt;/td&gt;&lt;td&gt;Long&lt;/td&gt;&lt;td&gt;Loss plateaus; adapter saved&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5️⃣ Evaluation&lt;/td&gt;&lt;td&gt;Short&lt;/td&gt;&lt;td&gt;Tool ≥95%, Style ≥90%, Guardrail ≥98%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6️⃣ Merge + Quantize&lt;/td&gt;&lt;td&gt;Short-Medium&lt;/td&gt;&lt;td&gt;GGUF file created successfully&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7️⃣ Deployment&lt;/td&gt;&lt;td&gt;Very short&lt;/td&gt;&lt;td&gt;Model responds correctly&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;8️⃣ Iteration Cycle&lt;/td&gt;&lt;td&gt;Short-Medium&lt;/td&gt;&lt;td&gt;Eval metrics improve&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;" tooltip="This timing and verification parameters table provides comprehensive guidance for measuring and validating each step of the Web4 LoRA training pipeline. Expected Scale refers to the typical duration for each step: Very short (minutes), Short (under 1 hour), Medium (3-4 hours), Long (12-20 hours), and Short-Medium (1-3 hours). Done Signals indicate the specific criteria that must be met before proceeding to the next step. Dataset Sanity Check (Very short) validates JSONL format integrity, tokenization compatibility, and data loading without errors. Dry Run (Short) tests training loop functionality with 100 samples, verifying loss decreases and MPS (Metal Performance Shaders) stability. Mini Fine-Tune (Medium) uses 2-3k samples to verify learning patterns and JSON/code quality. Full LoRA Training (Long) trains on 25-40k samples until loss plateaus and adapter is saved. Evaluation (Short) runs comprehensive tests achieving Tool ≥95%, Style ≥90%, Guardrail ≥98% accuracy. Merge + Quantize (Short-Medium) creates deployable GGUF files. Deployment (Very short) tests model responsiveness. Iteration Cycle (Short-Medium) improves metrics through continuous refinement. Use &#39;/usr/bin/time -f &quot;wall=%E cpu=%P mem=%MKB&quot; &lt;command&gt;&#39; to measure timing reproducibly. These parameters ensure systematic progress tracking and quality validation throughout the entire training lifecycle." id="timing-table">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#f5f5f5;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=9;" vertex="1" parent="1">
                        <mxGeometry x="50" y="1030" width="400" height="200" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="eval-title" value="📊 Evaluation Parameters &amp; Targets" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=16;fontStyle=1;" vertex="1" parent="1">
                    <mxGeometry x="500" y="980" width="400" height="40" as="geometry"/>
                </mxCell>
                <object label="&lt;div&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/div&gt;&lt;b&gt;A) Tool-Use Evaluation:&lt;/b&gt;&lt;br&gt;• JSON Validity: ≥99%&lt;br&gt;• Schema Match: ≥97%&lt;br&gt;• Tool Selection: ≥90-92%&lt;br&gt;• Argument Correctness: ≥92-95%&lt;br&gt;• Chaining Consistency: ≥95%" tooltip="Tool-Use Evaluation measures the model&#39;s ability to generate valid, structured tool-calling responses for Web4 applications. JSON Validity (≥99%) ensures the model outputs syntactically correct JSON that can be parsed without errors, preventing runtime failures in tool-calling systems. Schema Match (≥97%) validates that the generated JSON contains the correct keys and data types according to the expected tool schema, ensuring compatibility with downstream systems. Tool Selection (≥90-92%) measures the model&#39;s accuracy in choosing the appropriate tool(s) for a given task, which is critical for correct workflow execution. Argument Correctness (≥92-95%) evaluates whether the model provides the right parameters and values for each tool call, ensuring functional tool execution. Chaining Consistency (≥95%) tests the model&#39;s ability to reference previous tool results using &lt;RESULTS:...&gt; syntax in multi-step workflows, enabling complex task automation. These metrics collectively ensure the model can reliably generate tool calls that execute correctly in production Web4 environments, supporting automated workflows and task completion." id="tool-eval">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#e8f5e8;align=left;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" vertex="1" parent="1">
                        <mxGeometry x="500" y="1030" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;div&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/div&gt;&lt;b&gt;B) Style Evaluation:&lt;/b&gt;&lt;br&gt;• Lint Pass: 100%&lt;br&gt;• AST Structure: ≥90%&lt;br&gt;• Naming Conventions: ≥95%&lt;br&gt;• Template Conformance: ≥90%&lt;br&gt;• Forbidden Patterns: 0%" tooltip="Style Evaluation measures the model&#39;s adherence to Web4 coding standards and framework conventions. Lint Pass (100%) ensures all generated code passes ESLint and Prettier formatting rules, maintaining consistent code style and preventing syntax errors. AST Structure (≥90%) validates that the generated code has the correct Abstract Syntax Tree structure, including proper exports, imports, class definitions, and component hierarchy that matches Web4 patterns. Naming Conventions (≥95%) ensures the model uses correct PascalCase for components, ALLCAPS for constants, and camelCase for variables according to Web4 standards. Template Conformance (≥90%) measures how well the generated code matches expected Web4 templates and patterns, ensuring consistency with established frameworks. Forbidden Patterns (0%) ensures the model never uses banned APIs, deprecated methods, or anti-patterns that could cause issues in production. These metrics collectively ensure the model generates code that follows Web4 best practices, maintains consistency with team standards, and integrates seamlessly with existing codebases." id="style-eval">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#e3f2fd;align=left;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" vertex="1" parent="1">
                        <mxGeometry x="720" y="1030" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;div&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/div&gt;&lt;b&gt;C) Guardrail Evaluation:&lt;/b&gt;&lt;br&gt;• Refusal Accuracy: ≥98%&lt;br&gt;• False Acceptance: ≤1%&lt;br&gt;• Benign Compliance: ≥98%&lt;br&gt;&lt;br&gt;&lt;b&gt;D) Mixed Overall:&lt;/b&gt;&lt;br&gt;• Weighted Score: ≥90%&lt;br&gt;• Stability: ≈100%" tooltip="Guardrail Evaluation measures the model&#39;s safety and ethical compliance capabilities for Web4 applications. Refusal Accuracy (≥98%) ensures the model properly refuses inappropriate, harmful, or restricted requests by responding with &lt;REFUSAL&gt; tags, maintaining ethical boundaries and preventing misuse. False Acceptance (≤1%) measures the model&#39;s ability to avoid accepting requests it should refuse, preventing the model from being tricked into generating harmful content or violating safety guidelines. Benign Compliance (≥98%) ensures the model doesn&#39;t over-refuse legitimate requests, maintaining usability while preserving safety. Mixed Overall evaluation combines all evaluation categories with weighted scoring: Tool capabilities (45% weight), Style adherence (45% weight), and Guardrail compliance (10% weight) to achieve a Weighted Score ≥90%. Stability (≈100%) measures the model&#39;s consistency in generating non-empty, non-crashing responses across all test cases. These metrics collectively ensure the model operates safely, ethically, and reliably in production Web4 environments while maintaining high performance across all evaluation dimensions." id="guardrail-eval">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#fff3e0;align=left;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" vertex="1" parent="1">
                        <mxGeometry x="500" y="1150" width="200" height="130" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;div&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/div&gt;&lt;b&gt;🧭 Practical Measurement:&lt;/b&gt;&lt;br&gt;&lt;br&gt;Use: &lt;code&gt;/usr/bin/time -f &quot;wall=%E cpu=%P mem=%MKB&quot; &lt;/code&gt;&lt;br&gt;&lt;br&gt;to measure wall time and memory for every step reproducibly." tooltip="This practical measurement section provides the essential command-line tool for reproducible timing and resource measurement across all Web4 LoRA training steps. The /usr/bin/time command is a standard Unix utility that measures program execution time and resource usage. The -f flag specifies the output format, where wall=%E displays wall clock time (total elapsed time), cpu=%P shows CPU percentage used, and mem=%MKB displays peak memory usage in kilobytes. This command ensures reproducible measurements by providing consistent timing data across different runs and systems. Wall time measures the total elapsed time from start to finish, including I/O waits and system overhead. CPU percentage indicates how much of the available CPU was utilized during execution. Memory measurement tracks peak RAM usage, crucial for Mac M1 systems where memory constraints affect training performance. This tool is essential for validating timing estimates, identifying performance bottlenecks, and ensuring consistent resource usage across different training phases. Use this command to wrap every training step for systematic performance tracking and optimization." id="measurement-tip">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#f1f8e9;align=left;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" vertex="1" parent="1">
                        <mxGeometry x="720" y="1150" width="200" height="130" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;div&gt;&lt;b&gt;&lt;br&gt;&lt;/b&gt;&lt;/div&gt;&lt;b&gt;✅ Deployment Acceptance Criteria:&lt;/b&gt;&lt;br&gt;&lt;br&gt;• Tool JSON validity ≥99%, Schema ≥97%&lt;br&gt;• Style lint passes 100%, AST ≥90%&lt;br&gt;• Guardrail refusals ≥98%, false acceptance ≤1%&lt;br&gt;• Weighted overall score ≥90%&lt;br&gt;&lt;br&gt;&lt;b&gt;📘 Summary:&lt;/b&gt;&lt;br&gt;Use Timing &amp;amp; Verification to know &lt;i&gt;when&lt;/i&gt; each phase is complete. Use Evaluation Parameters to know &lt;i&gt;how good&lt;/i&gt; each model is and &lt;i&gt;what to fix&lt;/i&gt; next." tooltip="This deployment acceptance criteria section defines the final quality gate that must be passed before a Web4 LoRA model can be deployed to production. These criteria ensure the model meets all performance, safety, and reliability standards required for real-world usage. Tool JSON validity ≥99% ensures the model generates syntactically correct JSON that can be parsed without errors, preventing runtime failures in tool-calling systems. Schema match ≥97% validates that the generated JSON contains correct keys and data types according to expected tool schemas, ensuring compatibility with downstream systems. Style lint passes 100% ensures all generated code passes ESLint and Prettier formatting rules, maintaining consistent code quality and preventing syntax errors. AST structure ≥90% validates that generated code has correct Abstract Syntax Tree structure matching Web4 patterns. Guardrail refusals ≥98% ensures the model properly refuses inappropriate requests with &lt;REFUSAL&gt; tags, maintaining ethical boundaries. False acceptance ≤1% prevents the model from accepting requests it should refuse, maintaining safety standards. The weighted overall score ≥90% combines all evaluation categories (Tool 45%, Style 45%, Guardrail 10%) to ensure balanced performance across all capabilities. These criteria collectively ensure the model operates safely, reliably, and effectively in production Web4 environments while maintaining high performance standards across all evaluation dimensions." id="acceptance-criteria">
                    <mxCell style="text;html=1;strokeColor=#666666;fillColor=#fce4ec;align=left;verticalAlign=top;whiteSpace=wrap;rounded=1;fontSize=10;" vertex="1" parent="1">
                        <mxGeometry x="50" y="1250" width="400" height="150" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="eval-process-title" value="🧪 Evaluation Process Flow" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=16;fontStyle=1;" vertex="1" parent="1">
                    <mxGeometry x="50" y="1450" width="230" height="40" as="geometry"/>
                </mxCell>
                <object label="1️⃣ During Training&#xa;Monitor train_loss &amp; validation_loss&#xa;Evaluate mini-sets (100-200 items)&#xa;Stop early if loss &gt;1.6 or JSON errors" tooltip="During Training represents the continuous monitoring phase that ensures the model is learning effectively throughout the training process. Train_loss monitoring tracks the model&#39;s learning progress by measuring how well it predicts the training data, with decreasing loss indicating successful learning. Validation_loss monitoring uses a separate dataset to detect overfitting, where the model memorizes training data but fails to generalize to new examples. Mini-set evaluation (100-200 items) provides mid-epoch performance checks to ensure the model is learning expected patterns without waiting for full epoch completion. Early stopping at loss &gt;1.6 prevents wasted computational resources when the model shows signs of poor learning or divergence. JSON error detection identifies when the model starts generating malformed output, indicating potential training instability or data quality issues. This proactive monitoring approach prevents wasted training time, identifies learning issues early, and ensures the model maintains quality standards throughout the training process. The combination of loss monitoring, validation checks, and output quality assessment provides comprehensive insight into training progress and model health." id="eval-during">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e3f2fd;strokeColor=#1976d2;fontSize=12;fontStyle=1;" vertex="1" parent="1">
                        <mxGeometry x="50" y="1500" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="2️⃣ After Training&#xa;Run evaluate.py on all buckets&#xa;Calculate per-metric success ratios&#xa;Record results in eval_log.md" tooltip="After Training represents the comprehensive evaluation phase that assesses model performance across all Web4 capabilities following training completion. Run evaluate.py systematically across all buckets in data/eval/ to test Tool, Style, and Guardrail capabilities using standardized test cases. Calculate per-metric success ratios for each evaluation category (Tool JSON validity, Style lint compliance, Guardrail refusal accuracy) to understand where the model excels and where it needs improvement. Record all results in eval_log.md with timestamps, base model version, dataset version, and training parameters for tracking progress and enabling comparison between different training runs. This evaluation provides the baseline performance metrics that determine if the model meets deployment standards and identifies specific areas for improvement. The systematic approach ensures consistent evaluation across all model capabilities and provides actionable insights for model refinement. Success ratios help prioritize which capabilities need additional training data or parameter adjustments. The logged results enable tracking of model improvement over time and comparison between different training approaches or datasets." id="eval-after">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#f3e5f5;strokeColor=#7b1fa2;fontSize=12;fontStyle=1;" vertex="1" parent="1">
                        <mxGeometry x="280" y="1500" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="3️⃣ After Merge &amp; Quantize&#xa;Re-run evaluation&#xa;Compare post-quantization scores&#xa;Expect ≤2% drop, re-quantize if larger" tooltip="After merging LoRA weights and quantizing the model, re-evaluation is crucial to ensure quantization doesn&#39;t significantly degrade performance. Re-run the full evaluation suite to compare post-quantization scores with the original trained model. Expect at most a 2% performance drop due to quantization; if the drop is larger, re-quantize with higher precision (e.g., Q5_K_M instead of Q4_K_M) to maintain model quality. This step ensures the final deployed model maintains the performance characteristics of the trained model while benefiting from the reduced file size and improved inference speed of quantization." id="eval-quantize">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#e8f5e8;strokeColor=#388e3c;fontSize=12;fontStyle=1;" vertex="1" parent="1">
                        <mxGeometry x="510" y="1500" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="4️⃣ For Acceptance (Ship Gate)&#xa;Tool JSON ≥99%, Schema ≥97%&#xa;Style lint 100%, AST ≥90%&#xa;Guardrail ≥98%, overall ≥90%" tooltip="The acceptance phase serves as the final quality gate before deployment, ensuring the model meets all production standards. Tool capabilities must achieve JSON validity ≥99%, Schema match ≥97%, and Tool selection ≥90% to ensure reliable tool-calling functionality. Style evaluation requires 100% lint pass, AST structure ≥90%, and naming conventions ≥95% to maintain code quality standards. Guardrail evaluation demands refusal accuracy ≥98% and false acceptance ≤1% to ensure safety and ethical compliance. The weighted overall score must be ≥90% to pass the acceptance criteria. Only models meeting all these benchmarks are approved for production deployment." id="eval-acceptance">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fff3e0;strokeColor=#f57c00;fontSize=12;fontStyle=1;" vertex="1" parent="1">
                        <mxGeometry x="740" y="1500" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="5️⃣ Continuous Improvement&#xa;Version eval sets (eval_v1.1/)&#xa;Refresh samples on schema updates&#xa;Run periodic regression tests" tooltip="Continuous improvement ensures the model remains effective as requirements evolve and new data becomes available. Keep each evaluation set versioned (e.g., eval_v1.1/) to track changes and maintain consistency across model iterations. When schema or style rules are updated, refresh evaluation samples to match the new requirements and ensure the model is tested against current standards. Run periodic regression evaluations to ensure new LoRA adapters don&#39;t degrade older behaviors or introduce regressions in previously working capabilities. This iterative approach maintains model quality over time and enables continuous enhancement based on real-world performance feedback." id="eval-continuous">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#fce4ec;strokeColor=#c2185b;fontSize=12;fontStyle=1;" vertex="1" parent="1">
                        <mxGeometry x="750" y="1670" width="190" height="80" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="eval-arrow1" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=2;strokeColor=#1976d2;" edge="1" parent="1" source="eval-during" target="eval-after">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="250" y="1540" as="sourcePoint"/>
                        <mxPoint x="280" y="1540" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="eval-arrow2" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=2;strokeColor=#7b1fa2;" edge="1" parent="1" source="eval-after" target="eval-quantize">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="480" y="1540" as="sourcePoint"/>
                        <mxPoint x="510" y="1540" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="eval-arrow3" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=2;strokeColor=#388e3c;" edge="1" parent="1" source="eval-quantize" target="eval-acceptance">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="710" y="1540" as="sourcePoint"/>
                        <mxPoint x="740" y="1540" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="eval-arrow4" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=2;strokeColor=#f57c00;entryX=0.464;entryY=-0.006;entryDx=0;entryDy=0;entryPerimeter=0;" edge="1" parent="1" source="eval-acceptance" target="eval-continuous">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="740" y="1580" as="sourcePoint"/>
                        <mxPoint x="50" y="1600" as="targetPoint"/>
                        <Array as="points"/>
                    </mxGeometry>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
</mxfile>