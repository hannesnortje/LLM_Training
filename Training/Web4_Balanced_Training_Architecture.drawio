<mxfile host="65bd71144e">
    <diagram name="Balanced Training Architecture" id="architecture">
        <mxGraphModel dx="2325" dy="855" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="2400" pageHeight="1800" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="title" value="Web4 Balanced LoRA Training Architecture" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="400" y="20" width="1600" height="50" as="geometry"/>
                </mxCell>
                <mxCell id="subtitle" value="Training-First Production: Train Patterns (37K samples, ~20M tokens, 95% Web4) + RAG Reference Library (History 10-20%, Tools 30%) + Swappable Tools (12K in RAG) from 534 PDCAs" style="text;html=1;strokeColor=none;fillColor=#E3F2FD;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=16;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="400" y="80" width="1600" height="35" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;🤖 BASE MODEL&lt;/font&gt;&#xa;&#xa;Qwen/Qwen2.5-Coder-7B-Instruct&#xa;(HuggingFace - Full Precision)&#xa;&#xa;For Inference: qwen2.5-coder:7b-instruct-q4_K_M" tooltip="The base model is Qwen2.5-Coder 7B Instruct, chosen for its strong code generation capabilities and 7 billion parameters optimized for coding tasks. Training uses the full precision model from HuggingFace to maximize learning quality during LoRA fine-tuning. After training, the adapter is merged with the base model and quantized to Q4_K_M format for deployment via Ollama. This quantization reduces model size from 14GB to 4GB while maintaining 95 percent quality, enabling fast inference on M1 Mac hardware with 32GB RAM and MPS backend. The model architecture includes 28 transformer layers, 4096 hidden dimensions, 32 attention heads, 32768 context window, and supports 100+ programming languages with particular strength in Python, TypeScript, JavaScript, Java, and C++. The base model already understands general programming concepts like OOP, error handling, testing, and documentation - the LoRA fine-tuning teaches it Web4-specific conventions like 5-layer architecture, Radical OOP, empty constructor pattern, scenario-based state management, Vitest testing framework, and PDCA methodology for continuous improvement." id="base-model">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1565C0;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="950" y="150" width="600" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#2E7D32&quot;&gt;🎯 LORA ADAPTER (Trained Knowledge)&lt;/font&gt;&#xa;&#xa;37,000 samples (~20M tokens)&#xa;Patterns &amp; Methodology (95% Web4, 3% tools, 2% guardrails)&#xa;r=16, alpha=32, dropout=0.05&#xa;Training: 8-11 hours on M1 Mac" tooltip="The LoRA adapter is a small trainable module (approximately 80MB, reduced from 100MB) that learns Web4-specific patterns without modifying the base model. LoRA uses rank decomposition to create two small matrices for each transformer layer, where rank r=16 means each matrix is much smaller than the original weight matrix. Training only these small matrices is 1000x faster and uses 10x less memory than full fine-tuning, enabling training on consumer hardware. The adapter contains 37,000 training samples totaling approximately 20M tokens (reduced from 46K/25M), carefully curated to teach Web4 methodology with optimal token efficiency: 95 percent Web4-specific patterns versus 74 percent in the old approach. Process Knowledge (5K samples) covering PDCA structure, TRON format, CMM1-4 framework, dual link format, and 12-step startup protocol. Code Patterns (18K samples) including empty constructor pattern, init method for scenario-based state, toScenario serialization, 5-layer architecture, and Radical OOP. Extracted PDCA Patterns (8K samples) with problem-solution pairs, debugging methodologies, architectural decisions, violation fixes, integration patterns, and collaboration patterns. Representative PDCAs (3K samples) from top 200-300 complete PDCAs selected by quality score. Generic Tool Awareness (1K samples, NEW) teaching the CONCEPT of tools with JSON structure and parameter passing, NOT specific IDE implementations. Guardrails (2K samples) for security violations and framework compliance. Training takes 8-11 hours on M1 Mac with MPS acceleration (20 percent faster due to reduced token count), monitoring loss convergence to 0.6-1.0 plateau and memory usage staying under 28GB." id="lora-adapter">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="950" y="320" width="600" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#F57F17&quot;&gt;🗄️ THREE-TIER RAG ARCHITECTURE&lt;/font&gt;&#xa;&#xa;Tier 1: ChromaDB (Semantic Search)&#xa;Tier 2: Redis Graph (Breadcrumb Navigation)&#xa;Tier 3: SQLite (Temporal Queries)&#xa;&#xa;534 PDCAs → ~2,670 chunks | All components indexed" tooltip="The three-tier RAG architecture is the cornerstone of the balanced training strategy, serving as both the data source for training sample generation and the runtime historical reference library. This hybrid design optimizes different query patterns: Tier 1 ChromaDB provides semantic search using vector embeddings, ideal for finding similar PDCAs or patterns. The 534 historical PDCAs are chunked into approximately 2,670 semantically complete chunks using PDCA-aware adaptive chunking that preserves document structure by splitting on section boundaries. Each chunk includes 15+ metadata fields covering temporal data, agent context, work context, task context, CMM compliance, and quality signals. Tier 2 Redis Graph stores breadcrumb navigation links between PDCAs, enabling fast graph traversal to implement the read-to-depth-3 principle. Graph queries are 50x faster than vector search for adjacency relationships. Tier 3 SQLite handles temporal queries efficiently, supporting fast date-range lookups and agent timeline tracking without scanning the entire vector database. This three-tier design provides single source of truth for all training data, intelligent sampling via semantic queries, natural deduplication through chunking, metadata-driven filtering, graph-aware context expansion, incremental refinement, and consistent methodology." id="rag-architecture">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="50" y="150" width="750" height="480" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;Tier 1: ChromaDB&lt;/font&gt;&#xa;Vector Embeddings&#xa;~2,670 PDCA chunks&#xa;15+ metadata fields&#xa;Semantic similarity search" tooltip="ChromaDB is an open-source vector database optimized for semantic search using embeddings. Each PDCA chunk is converted to a 768-dimensional vector using a sentence-transformer model, capturing semantic meaning beyond keyword matching. This enables queries to find relevant PDCAs even if they use different terminology. The chunks are stored with comprehensive metadata enabling filtered queries. ChromaDB uses HNSW index for fast approximate nearest neighbor search, returning results in approximately 500ms. The metadata fields enable precise filtering by chunk type, CMM level, task type, date, agent, and verification status. ChromaDB also indexes 3,477 TypeScript component files by layer and pattern, plus 238 process documents by role. During training sample generation, ChromaDB is queried thousands of times to extract patterns for the training dataset." id="chromadb">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="70" y="320" width="220" height="90" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;Tier 2: Redis Graph&lt;/font&gt;&#xa;Breadcrumb links&#xa;534 nodes, PRECEDES edges&#xa;Fast graph traversal&#xa;Read to depth 3" tooltip="Redis Graph stores PDCA breadcrumb relationships as a graph database, enabling fast traversal of prev/next links extracted from PDCA metadata. Each of the 534 PDCAs becomes a node with properties, and PRECEDES edges connect chronologically related PDCAs. Graph queries are extremely fast (approximately 10ms) compared to vector search (approximately 500ms) because they use index lookups rather than similarity computation. The primary use case is read-to-depth-3: when semantic search finds a relevant PDCA, walk the graph backward and forward up to 3 levels deep to understand the full context. This implements the Web4 principle that context matters - a single PDCA in isolation may miss important background. Redis Graph uses sparse adjacency matrices for efficient traversal and supports Cypher-like query language. During training sample generation, graph expansion enriches semantic search results to include predecessor and successor context." id="redis-graph">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="310" y="320" width="220" height="90" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;Tier 3: SQLite&lt;/font&gt;&#xa;Temporal metadata&#xa;Fast date-range queries&#xa;Agent/sprint aggregation&#xa;5ms query time" tooltip="SQLite stores temporal and categorical metadata in a relational schema optimized for fast date-range queries, agent timelines, and sprint aggregations. The pdca_timeline table contains pdca_id, timestamp, session, agent_name, agent_role, branch, sprint, cmm_level, and objective with appropriate indexes. Indexes enable sub-5ms queries which is 100x faster than scanning ChromaDB with metadata filters because SQL databases are optimized for structured queries with B-tree indexes. SQLite is also used for analytics: count PDCAs per day, identify most active agents, track CMM level distribution over time, measure sprint velocity. During training sample generation, temporal queries ensure diverse time period coverage to prevent temporal bias where the model only learns the newest patterns. SQLite is lightweight, requires no server, and integrates easily with Python. The three-tier design uses each database for its strength: ChromaDB for semantic understanding, Redis Graph for relationship traversal, SQLite for structured queries." id="sqlite">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="560" y="320" width="220" height="90" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;📚 Collections&lt;/font&gt;&#xa;&#xa;• pdca_historical (534)&#xa;• components (3,477)&#xa;• process_docs (238)&#xa;• tool_examples (12K) ★ NEW&#xa;• daily_buffer (transient)" tooltip="The RAG system organizes data into five ChromaDB collections. The pdca_historical collection contains 534 PDCAs as approximately 2,670 chunks - this is the permanent reference library with all historical PDCAs indexed with PDCA-aware adaptive chunking and comprehensive metadata. This collection never clears. The components collection indexes 3,477 TypeScript files organized by layer and pattern. The process_docs collection contains 238 documents including role-specific process documentation, CMM framework guides, PDCA templates, creation guides, decision frameworks, and compliance checklists. The tool_examples collection (NEW) stores 12,000 IDE-specific tool examples (10K Continue tools plus 2K negative examples) with metadata including tool_name, tool_ecosystem, tool_version, usage_pattern, and context_type. These tool examples are NOT trained into the LoRA adapter but are retrieved at runtime and injected into the context when the model needs to make tool calls. This enables IDE flexibility - switching from Continue to Cursor takes 5 minutes (clear Continue tools, index Cursor tools) versus 10-14 hours full retraining. The tool examples are swappable by ecosystem and support multiple IDEs simultaneously. The daily_buffer collection holds today work-in-progress and is transient, cleared nightly. During the evening training loop, daily_buffer is queried for untrained patterns, those patterns are trained into the adapter, and the buffer is cleared after moving data to permanent collections. This implements the incremental learning strategy where the model continuously improves from daily work." id="collections">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="70" y="440" width="700" height="130" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-rag-to-training" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#F57F17;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="rag-architecture" target="lora-adapter" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="800" y="400" as="sourcePoint"/>
                        <mxPoint x="850" y="350" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-label-training" value="RAG-Driven&#xa;Sample Generation&#xa;37K samples" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=12;fontStyle=1;fontColor=#F57F17;fillColor=#FFF8E1;strokeColor=#F9A825;rounded=1;" parent="arrow-rag-to-training" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="2" relative="1" as="geometry">
                        <mxPoint x="5" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#4A148C&quot;&gt;📦 TRAINING DATA BUCKETS (37K trained + 12K RAG tools)&lt;/font&gt;" tooltip="Training data is organized into 8 trained buckets plus RAG tool repository, teaching the model HOW TO CODE, HOW TO WORK, HOW TO SOLVE PROBLEMS, and WHAT NOT TO DO. The hybrid tool architecture splits tool knowledge: 1K generic tool awareness is trained (teaches CONCEPT of tools) while 12K IDE-specific examples stay in RAG (enables IDE switching without retraining). The trained buckets teach: HOW TO CODE - style_core (12K samples, 32 percent) extracts real Web4 architectural patterns from 3,477 TypeScript files including empty constructor, 5-layer architecture, Radical OOP, scenario-based state management. style_refactor (3K samples, 8 percent) shows code evolution and continuous improvement patterns. HOW TO WORK - process_framework (5K samples, 13 percent) teaches PDCA structure v3.2.4.2, TRON decision format, CMM1-4 progression, dual link format, 12-step startup protocol, verification checklists, and 50+ key behavioral lessons. HOW TO SOLVE PROBLEMS - domain_patterns (8K samples, 22 percent) extracts distilled problem-solving patterns from all 534 PDCAs including debugging methodologies, architectural decisions, integration patterns, and collaboration patterns. domain_representatives (3K samples, 8 percent) provides complete exemplary PDCAs selected by quality scoring to show end-to-end work structure. WHAT NOT TO DO - guardrails (2K samples, 5 percent) teaches security violations, Jest ban enforcement, manual operation prevention, and framework compliance. tool_awareness (1K samples, 3 percent) teaches generic tool-calling concepts with JSON structure and parameter passing, IDE-agnostic. eval (2K samples, 5 percent) is held-out test set NEVER trained. The RAG Tool Repository stores 12K IDE-specific tool examples with metadata for runtime injection, enabling 5-minute IDE switching versus 10-14 hours retraining." id="training-buckets">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#6A1B9A;strokeWidth=3;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1610" y="150" width="770" height="340" as="geometry"/>
                    </mxCell>
                </object>
                <object label="TRAINED (37K samples):&#xa;&#xa;HOW TO CODE: style_core 12K (32%) | style_refactor 3K (8%)&#xa;HOW TO WORK: process_framework 5K (13%)&#xa;HOW TO SOLVE: domain_patterns 8K (22%) | domain_representatives 3K (8%)&#xa;WHAT NOT TO DO: guardrails 2K (5%) | tool_awareness 1K (3%)&#xa;EVALUATION: eval 2K (5%) - HOLD-OUT&#xa;&#xa;RAG TOOLS (12K): tool_core 10K + tool_neg 2K (swappable)" tooltip="Clearer bucket naming that reflects what we actually teach the model. TRAINED samples (37K total, approximately 20M tokens): HOW TO CODE (15K samples, 40 percent) - style_core 12K samples teaches real Web4 architectural patterns from 3,477 TypeScript files: empty constructor pattern, 5-layer architecture, Radical OOP, scenario-based state management, init methods, toScenario serialization, component structure, and Vitest testing. style_refactor 3K samples shows code evolution patterns: CMM2 to CMM3 transformations, technical debt reduction, pattern application, refactoring journeys, and continuous improvement mindset. HOW TO WORK (5K samples, 13 percent) - process_framework 5K samples teaches the methodology: PDCA structure v3.2.4.2, TRON decision format, CMM1-4 progression and compliance, dual link format, 12-step startup protocol, verification checklists, collaboration patterns, feedback point recognition, and 50+ key behavioral lessons from trainAI. HOW TO SOLVE PROBLEMS (11K samples, 30 percent) - domain_patterns 8K samples extracts distilled problem-solving patterns from all 534 historical PDCAs: debugging methodologies, architectural decisions, violation fixes, integration patterns, collaboration patterns, and problem-solution pairs that capture Web4 domain wisdom. domain_representatives 3K samples provides complete exemplary PDCAs selected by quality scoring to show end-to-end work structure and full PDCA methodology in action. WHAT NOT TO DO (3K samples, 8 percent) - guardrails 2K samples teaches compliance: Jest ban enforcement, manual operation prevention, security violations, framework violations. tool_awareness 1K samples teaches generic tool-calling concepts: JSON structure, parameter passing, context awareness, IDE-agnostic patterns. EVALUATION (2K samples, 5 percent) - eval 2K samples is held-out test set stratified across all categories, NEVER trained, used for unbiased quality measurement. RAG TOOLS (12K samples, approximately 3.5MB): NOT trained into LoRA. Continue tools 10K plus negatives 2K stored in ChromaDB tool_examples collection with metadata. Runtime injection adds approximately 150ms latency but enables 5-minute IDE switching versus 10-14 hour retraining." id="bucket-detail">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#7B1FA2;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="1630" y="200" width="730" height="130" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#6A1B9A&quot;&gt;Token Distribution: ~20M trained + 3.5MB RAG tools&lt;/font&gt;&#xa;Avg 540 tokens/sample | 95% Web4, 3% tools, 2% guardrails&#xa;Optimized for M1 Mac (32GB RAM)&#xa;Training time: 8-11 hours (20% faster)" tooltip="The approximately 20M token budget is optimized for M1 Mac hardware with improved token efficiency. Token calculation: 37K samples times 540 average tokens per sample equals approximately 20M tokens, reduced from 25M (saving 5M tokens or 20 percent). The 540 token average accounts for short samples (100-200 tokens) for simple patterns, medium samples (400-800 tokens) for complete class implementations, and long samples (1200-1800 tokens) for full PDCA documents. This 20M token count enables faster training (8-11 hours versus 10-14 hours) while maintaining quality. Token distribution optimization: 95 percent Web4-specific patterns (versus 74 percent in old approach), 3 percent generic tool awareness (versus 22 percent for full tool training), 2 percent guardrails. This increases Web4 focus by 28 percent while maintaining tool capabilities through runtime RAG injection. The RAG tool repository stores 12K tool examples as approximately 3.5MB of text data, retrieved at runtime with approximately 150ms latency. Token efficiency strategies include PDCA patterns being distilled to save 60 percent tokens, code patterns using targeted extracts to save 40 percent tokens, representatives using smart variations to save 70 percent tokens, and tool examples staying in RAG to save 9K training samples. The 20M budget enables training sophisticated Web4 behaviors including 5-layer OOP architecture, empty constructor pattern, scenario-based state management, PDCA methodology with TRON format, CMM compliance, and framework adherence." id="token-dist">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#7B1FA2;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="1630" y="350" width="730" height="110" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-training" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#2E7D32;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" source="lora-adapter" target="trained-model" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1200" y="500" as="sourcePoint"/>
                        <mxPoint x="1200" y="600" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-label-lora" value="LoRA Training&lt;br&gt;8-11 hours&lt;br&gt;M1 Mac MPS" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=12;fontStyle=1;fontColor=#2E7D32;fillColor=#E8F5E9;strokeColor=#43A047;rounded=1;" parent="arrow-training" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="2" relative="1" as="geometry">
                        <mxPoint x="-62" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;🎓 TRAINED MODEL&lt;/font&gt;&#xa;&#xa;Base + LoRA Adapter&#xa;Merged &amp; Quantized (Q4_K_M)&#xa;Deployed to Ollama&#xa;web4-agent:latest" tooltip="The trained model is the final production artifact combining the base model general coding knowledge with the LoRA adapter Web4-specific patterns. Post-training process: merge LoRA adapter weights with base model weights, quantize merged model from FP16 to Q4_K_M format (4-bit with higher precision for critical attention layers), convert to GGUF format for optimized inference, create Ollama modelfile, and import to Ollama. The trained model capabilities include pattern recognition, code generation, PDCA creation, refactoring, guardrails, and collaboration. The 4GB quantized model loads in approximately 3 seconds on M1 Mac, generates at approximately 20 tokens per second, and achieves 90 percent accuracy on evaluation set metrics including pattern recognition 95 percent, PDCA template 95 percent, TRON format 90 percent, empty constructor 95 percent, CMM understanding 90 percent, historical retrieval 85 percent, refusal accuracy 98 percent, and overall score 90 percent." id="trained-model">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E3F2FD;strokeColor=#1565C0;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="950" y="570" width="600" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-runtime-rag" value="" style="endArrow=classic;startArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#FF6F00;exitX=0;exitY=0.75;exitDx=0;exitDy=0;entryX=1;entryY=0.75;entryDx=0;entryDy=0;dashed=1;" parent="1" source="trained-model" target="rag-architecture" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="700" y="650" as="sourcePoint"/>
                        <mxPoint x="750" y="600" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-label-runtime" value="Runtime Queries&#xa;(10-20% PDCA history)&#xa;(30% tool injection)" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#FF6F00;fillColor=#FFF3E0;strokeColor=#FB8C00;rounded=1;" parent="arrow-runtime-rag" vertex="1" connectable="0">
                    <mxGeometry x="-0.05" y="1" relative="1" as="geometry">
                        <mxPoint y="-30" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#D84315&quot;&gt;🌙 EVENING TRAINING LOOP&lt;/font&gt;&#xa;&#xa;Daily Buffer → Query Untrained → Generate Samples&#xa;→ Incremental Training (1 epoch) → Mark as Trained&#xa;→ Move to Historical → Clear Buffer&#xa;&#xa;Scheduled: 10 PM daily | Duration: 2-3 hours" tooltip="The evening training loop implements continuous learning, automatically incorporating each day work into the model every night at 10 PM. This creates a virtuous cycle where the model improves daily from real project work. The 7-step nightly process: Daily Buffer Collection throughout the day with new PDCAs indexed into daily_buffer with metadata. Query Untrained Patterns at 10 PM to identify what is new since yesterday. Generate Incremental Samples extracting patterns from today work with quality scoring. Incremental LoRA Training for these new samples for 1 epoch with adjusted hyperparameters, taking 2-3 hours for typical 50-sample daily batch. Mark as Trained updating RAG metadata for all trained chunks. Move to Historical with PDCAs from daily_buffer moved to pdca_historical collection and Redis Graph updated with new PRECEDES edges. Clear Daily Buffer with the collection archived and system reset for tomorrow. The evening loop advantages include continuous improvement, pattern discovery, adaptation, efficiency, and metadata-driven smart querying. Evening loop monitoring includes daily logs tracking sample count, training loss, memory usage, and completion time with alerting for issues." id="evening-loop">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="50" y="690" width="750" height="210" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1B5E20&quot;&gt;🚀 PRODUCTION DEPLOYMENT&lt;/font&gt;&#xa;&#xa;Ollama Model: web4-agent:latest&#xa;Fast Inference: ~20 tokens/sec on M1&#xa;Memory: ~4GB loaded&#xa;Response: 80-90% from training, 10-20% with RAG&#xa;Latency: under 200ms (trained), ~500ms (with RAG)" tooltip="Production deployment architecture optimizes for fast, reliable inference while maintaining access to historical context when needed. Ollama Integration provides REST API for LLM queries, chat interface for interactive sessions, and embedding endpoint for RAG similarity search. Ollama handles model lifecycle, request batching, and response streaming. Performance Characteristics include fast inference at approximately 20 tokens per second on M1 Mac, low memory footprint of approximately 4GB loaded, quick cold start of approximately 3 seconds to load model, and sub-200ms response latency for trained knowledge queries not requiring RAG. Decision Logic for RAG: the model first attempts to answer from trained knowledge (80-90 percent of queries), and for queries requiring specific historical context, the model queries RAG (10-20 percent of queries). RAG augmentation adds approximately 300ms latency but provides accurate historical reference. Hybrid Response Generation retrieves 3-5 relevant chunks, formats retrieved context, generates response incorporating both trained knowledge and retrieved facts, and includes source citations for traceability. Monitoring and Observability tracks response time metrics, RAG hit rate, quality metrics, and user feedback. Error handling gracefully degrades if RAG is unavailable, times out slow RAG queries, caches frequently accessed PDCAs, and logs all errors for analysis." id="production">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1610" y="550" width="770" height="210" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#01579B&quot;&gt;📊 KEY SUCCESS METRICS&lt;/font&gt;&#xa;&#xa;Training: Loss 0.6-1.0 | Memory under 28GB | 37K samples | ~20M tokens | 8-11 hrs&#xa;Quality: Pattern Recognition ≥95% | PDCA Template ≥95% | Framework Compliance ≥95%&#xa;Production: Response under 200ms | PDCA Queries 10-20% | Tool Queries 30% | Compilation ≥90% | Refusal ≥98%" tooltip="Key Success Metrics define measurable targets across three phases with hybrid tool architecture. Training Success includes loss convergence to 0.6-1.0 range indicating good learning, memory usage staying under 28GB ensuring stable training, successfully training 37K samples (down from 46K) in 8-11 hours (20 percent faster) validating the optimized token budget, and gradient norms staying stable confirming proper learning. Quality Success includes Pattern Recognition at least 95 percent measuring whether the model correctly identifies when to apply Web4 patterns, PDCA Template at least 95 percent evaluating generated PDCAs for completeness and compliance, Framework Compliance at least 95 percent checking generated code for proper architecture and conventions, Empty Constructor at least 95 percent for pattern adherence, CMM Understanding at least 90 percent for framework knowledge, Historical Retrieval at least 85 percent for RAG integration, and Refusal Accuracy at least 98 percent for guardrail effectiveness. Production Success includes response latency under 200ms for trained knowledge queries (no RAG), PDCA History Queries 10-20 percent of total requests validating historical reference usage, Tool Queries 30 percent of requests requiring tool example injection from RAG, Tool Injection Latency approximately 150ms additional for tool queries, Compilation Success at least 90 percent measuring whether generated code compiles on first attempt, and IDE Switching Time 5 minutes to swap Continue for Cursor versus 10-14 hours retraining. These metrics are continuously monitored via automated evaluation pipeline with alerting if any metric drops below threshold." id="metrics">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FE;strokeColor=#0277BD;strokeWidth=3;fontSize=12;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="50" y="950" width="2300" height="110" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="footer" value="🎯 Training-First Production: Model uses trained patterns FIRST (80-90% queries, ~2000ms) | RAG for historical reference (10-20% queries, +300ms) and tool syntax (30% queries, +150ms) | 5-min IDE switching vs 10-14hr retraining" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;strokeWidth=3;align=center;verticalAlign=middle;fontSize=13;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="50" y="1090" width="2300" height="60" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
    <diagram name="RAG-First Training Pipeline" id="pipeline-flow">
        <mxGraphModel dx="1603" dy="855" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="2400" pageHeight="1800" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="flow-title" value="Web4 Training Data Generation Pipeline (Training Preparation Phase)" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="400" y="20" width="1600" height="50" as="geometry"/>
                </mxCell>
                <mxCell id="flow-subtitle" value="RAG-Driven Sample Generation: Use RAG to Generate Training Data (Week 1-2) → Train LoRA (8-11 hrs) → Deploy → Production is Training-First with RAG Reference (10-20% history, 30% tools)" style="text;html=1;strokeColor=none;fillColor=#E3F2FD;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=16;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="200" y="80" width="2000" height="35" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#01579B&quot;&gt;PHASE 0: BOOTSTRAP RAG&lt;/font&gt;&#xa;&#xa;Day 1 | Duration: ~1 hour&#xa;&#xa;Install: ChromaDB + Redis Graph + SQLite&#xa;Index: 534 PDCAs → ~2,670 chunks&#xa;Index: 3,477 TypeScript files&#xa;Index: 238 process docs&#xa;Index: 12K tool examples&#xa;&#xa;Result: Complete RAG data store" tooltip="Phase 0 bootstraps the three-tier RAG system which serves as the single source of truth for all training data. Day 1 setup takes approximately 1 hour. Install dependencies: ChromaDB for semantic search, Redis server with RedisGraph module for breadcrumb navigation, SQLite for temporal queries. Run initial indexing scripts: all 534 historical PDCAs are processed with PDCA-aware adaptive chunking creating approximately 2,670 semantically complete chunks with 15+ metadata fields per chunk. The 3,477 TypeScript component files are indexed by layer and pattern. The 238 process documents including PDCA templates, CMM guides, and compliance checklists are indexed by role. The 12K tool examples from tool_core.jsonl and tool_neg.jsonl are indexed into the tool_examples collection with metadata for tool_name, tool_ecosystem, tool_version, usage_pattern, and context_type. Verify three-tier indexing: test semantic queries on ChromaDB, test breadcrumb traversal on Redis Graph, test temporal queries on SQLite. Test retrieval across all three tiers with sample queries. Result: Complete RAG data store ready for intelligent sample generation. This phase is critical because RAG becomes the single source of truth - all subsequent training samples are generated via queries against this RAG system, ensuring consistency and enabling metadata-driven sampling." id="phase0">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FE;strokeColor=#01579B;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="150" width="360" height="220" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#F57F17&quot;&gt;PHASE 1: RAG-DRIVEN SAMPLE GENERATION&lt;/font&gt;&#xa;&#xa;Week 1-2 | Duration: ~10 days&#xa;&#xa;Query RAG for patterns (semantic + graph + temporal)&#xa;Generate 37K training samples:&#xa;• style_core: 12K&#xa;• domain_patterns: 8K&#xa;• process_framework: 5K&#xa;• domain_representatives: 3K&#xa;• style_refactor: 3K&#xa;• guardrails: 2K&#xa;• eval: 2K&#xa;• tool_awareness: 1K&#xa;&#xa;Save to JSONL files (~20M tokens)&#xa;&#xa;Result: Complete training dataset" tooltip="Phase 1 generates all 37K training samples via intelligent RAG queries over 10 days. This is the core innovation: RAG is not just for runtime retrieval but also the source for training sample generation. The process uses semantic queries to extract patterns, graph expansion to include context, and temporal filtering to ensure diversity. Sample generation per bucket: style_core 12K samples query ChromaDB for TypeScript files filtered by layer and pattern, extracting empty constructor examples, 5-layer architecture, Radical OOP, scenario-based state management. domain_patterns 8K samples query historical PDCAs semantically for problem-solution pairs, then use Redis Graph to walk breadcrumb chains for context, extracting distilled patterns. process_framework 5K samples extract PDCA structure, TRON format, CMM compliance from process_docs collection. domain_representatives 3K samples select top 200-300 PDCAs by quality score ensuring diverse time periods via SQLite temporal queries. style_refactor 3K samples query for CMM2 to CMM3 transformation PDCAs. guardrails 2K samples extract from violation reports. eval 2K samples stratify across all categories, NEVER trained. tool_awareness 1K samples curate generic tool concepts from tool_core.jsonl. Each sample includes input prompt, expected output, and metadata. Samples are saved to JSONL files totaling approximately 20M tokens. Quality checks validate schema compliance, Web4 pattern adherence, token distribution. Result: Production-ready training dataset generated entirely from RAG queries, ensuring consistency and traceability." id="phase1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="540" y="150" width="360" height="370" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#2E7D32&quot;&gt;PHASE 2: LORA TRAINING&lt;/font&gt;&#xa;&#xa;Week 3 | Duration: 8-11 hours&#xa;&#xa;Base: Qwen2.5-Coder-7B-Instruct (HuggingFace)&#xa;Train: 37K samples, 2 epochs&#xa;Config: r=16, alpha=32, dropout=0.05&#xa;Batch: 1 with grad accumulation 12&#xa;LR: 2e-4 with cosine schedule&#xa;Hardware: M1 Mac (32GB), MPS backend&#xa;&#xa;Monitor: Loss plateau 0.6-1.0&#xa;Monitor: Memory under 28GB&#xa;Monitor: Gradient norms stable&#xa;&#xa;Output: LoRA adapter (~80MB)&#xa;&#xa;Result: Trained Web4-specific adapter" tooltip="Phase 2 performs LoRA fine-tuning on the 37K samples generated from RAG queries. Training takes 8-11 hours on M1 Mac with 32GB RAM using MPS Metal Performance Shaders backend. Base model: Qwen/Qwen2.5-Coder-7B-Instruct from HuggingFace, chosen for strong code generation capabilities and 7 billion parameters optimized for coding. Training configuration: 37K samples trained for 2 full epochs totaling approximately 20M tokens. LoRA hyperparameters: rank r=16 creates small trainable matrices for efficient fine-tuning, alpha=32 for scaling, dropout=0.05 for regularization. Batch size 1 with gradient accumulation 12 gives effective batch size 12, enabling stable gradients while fitting in 32GB RAM. Learning rate 2e-4 with cosine annealing schedule gradually reduces learning rate for smooth convergence. The training pipeline loads JSONL files, tokenizes with the base model tokenizer, applies LoRA to attention and feedforward layers, and trains using AdamW optimizer. Real-time monitoring tracks loss convergence expecting plateau at 0.6-1.0 range indicating good learning without overfitting, memory usage must stay under 28GB to prevent OOM crashes, gradient norms should remain stable confirming proper learning dynamics. Training output: LoRA adapter approximately 80MB containing learned Web4-specific patterns without modifying the 14GB base model. The adapter encodes: 95 percent Web4-specific patterns including PDCA methodology, code architecture, OOP principles, 3 percent generic tool awareness, 2 percent guardrails. Result: Production-ready LoRA adapter ready for merging and quantization." id="phase2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="980" y="150" width="360" height="370" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;PHASE 3: MERGE AND QUANTIZE&lt;/font&gt;&#xa;&#xa;Week 3 | Duration: ~2 hours&#xa;&#xa;Merge: LoRA adapter + Base model&#xa;Quantize: FP16 → Q4_K_M (4-bit)&#xa;Convert: GGUF format&#xa;Size: 14GB → 4GB (4x smaller)&#xa;Quality: 95% retained&#xa;&#xa;Create: Ollama modelfile&#xa;Import: web4-agent:latest&#xa;&#xa;Test: Load time ~3 seconds&#xa;Test: Generation ~20 tokens/sec&#xa;&#xa;Result: Deployable 4GB GGUF model" tooltip="Phase 3 merges the trained LoRA adapter with the base model and quantizes for efficient deployment. Duration approximately 2 hours. Merge process: The 80MB LoRA adapter weights are merged into the 14GB base model weights creating a unified model with Web4-specific knowledge permanently integrated. The merged model combines general coding knowledge from Qwen2.5-Coder with Web4-specific patterns from LoRA training. Quantization: Convert merged model from FP16 full precision to Q4_K_M 4-bit quantization. Q4_K_M uses 4-bit integers for most weights while keeping higher precision for critical attention layers, achieving optimal balance between size and quality. Size reduction: 14GB FP16 model compresses to 4GB Q4_K_M, a 4x reduction enabling deployment on consumer hardware. Quality retention: Quantization maintains 95 percent of full precision quality, validated through evaluation metrics. Convert to GGUF format: GGUF is an efficient file format for LLM storage optimized for CPU and Metal GPU inference, used by Ollama. Create Ollama modelfile: Define model configuration, system prompt, temperature, context window, and other parameters. Import to Ollama: Register the quantized GGUF model as web4-agent:latest in the local Ollama model registry. Test deployment: Verify load time approximately 3 seconds on M1 Mac cold start, generation speed approximately 20 tokens per second, memory footprint approximately 4GB loaded. Result: Production-ready 4GB GGUF model deployed to Ollama, ready for local inference with fast performance and low memory footprint." id="phase3">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1565C0;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1450" y="160" width="360" height="360" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#6A1B9A&quot;&gt;PHASE 4: EVALUATION AND QUALITY GATES&lt;/font&gt;&#xa;&#xa;Week 3 | Duration: ~4 hours&#xa;&#xa;Run: 2K eval samples (hold-out set)&#xa;&#xa;Test Harnesses:&#xa;✓ Pattern Compliance ≥95% (schema validator)&#xa;✓ PDCA Template ≥95% (section regex)&#xa;✓ TRON Format ≥90% (structure detector)&#xa;✓ Empty Constructor ≥95% (ESLint + AST)&#xa;✓ Tool Success ≥85% (100 scripted tasks)&#xa;✓ Refusal F1 ≥0.98 (200-item safety set)&#xa;✓ Overall ≥90%&#xa;&#xa;Canary Tests: 20 must-not-regress tasks&#xa;&#xa;Pass? → Deploy | Fail? → Rollback&#xa;&#xa;Result: Quality-validated model" tooltip="Phase 4 runs comprehensive evaluation to validate model quality before production deployment. Duration approximately 4 hours. Evaluation process: Run the 2K eval samples that were held out during training, ensuring unbiased quality measurement across all training categories. Test Harness 1 Pattern Compliance: Schema validator plus AST checker tests 100 generated PDCAs against v3.2.4.2 schema, must pass 95 out of 100. Test Harness 2 PDCA Template: Section regex plus metadata validator checks all required sections Links Plan Do Check Act Meta, must pass 95 out of 100. Test Harness 3 TRON Format: Structure detector validates Trigger Response Outcome Next ordering in decisions, must pass 90 out of 100. Test Harness 4 Empty Constructor: ESLint with Web4 rules plus AST parser checks no-constructor-logic rule on 100 generated classes, must pass 95 out of 100. Test Harness 5 Tool Success: 100 scripted IDE tasks measured end-to-end from prompt to correct tool JSON to successful execution in sandbox, must pass 85 out of 100. Test Harness 6 Refusal Accuracy: F1 score on 200-item curated safety set with 100 should-refuse and 100 should-comply samples, must achieve F1 at least 0.98. Overall Score: Weighted average of all metrics must be at least 90 percent. Canary Tests: Run 20 critical must-not-regress tasks comparing new model against baseline, fail if any regression over 5 percent. Gate Decision: If all Ship Gates pass (Pattern, PDCA, Empty Constructor, Refusal, Overall), proceed to production deployment. If any gate fails, halt deployment, rollback to last-known-good adapter, create incident PDCA documenting failure mode, investigate root cause, fix and retry. Result: Quality-validated model ready for production with documented test results." id="phase4">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#6A1B9A;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1900" y="130" width="380" height="420" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#558B2F&quot;&gt;PHASE 5: PRODUCTION DEPLOYMENT&lt;/font&gt;&#xa;&#xa;Week 3 | Duration: ~1 hour&#xa;&#xa;Deploy: web4-agent:latest to Ollama&#xa;Connect: RAG system for historical queries&#xa;Configure: ToolAwarePromptBuilder&#xa;Start: Ollama server&#xa;&#xa;Response Logic:&#xa;• 80-90% queries: From training (under 200ms)&#xa;• 10-20% queries: With RAG PDCA history (+300ms)&#xa;• 30% queries: With RAG tool injection (+150ms)&#xa;&#xa;Monitor: Response time, RAG hit rate, quality&#xa;&#xa;Result: Production-ready system" tooltip="Phase 5 deploys the validated model to production with full RAG integration. Duration approximately 1 hour. Deployment steps: Deploy web4-agent:latest GGUF model to Ollama model registry. Connect RAG system for historical reference: ChromaDB for semantic search, Redis Graph for breadcrumb navigation, SQLite for temporal queries. Configure ToolAwarePromptBuilder to inject relevant tool examples from the 12K tool_examples RAG collection at runtime. Start Ollama server with REST API for LLM queries, chat interface for interactive sessions, and embedding endpoint for RAG similarity. Response logic: The model first attempts to answer from trained knowledge covering 80-90 percent of queries with response latency under 200ms. For queries requiring specific historical context like how did we solve X before or what did we work on date Y, the model queries RAG adding approximately 300ms latency but providing accurate historical reference with source citations. For queries requiring tool usage like read this file or run this command, ToolAwarePromptBuilder detects tool need, queries RAG tool_examples collection for 2-3 relevant examples, injects examples into context adding approximately 150ms latency, and model generates correct tool call following RAG-provided patterns. Monitoring: Track response time metrics across query types, RAG hit rate to validate 10-20 percent PDCA queries and 30 percent tool queries, quality metrics via user feedback and automated checks. Error handling: Gracefully degrade if RAG unavailable, timeout slow RAG queries after 1 second, cache frequently accessed PDCAs for speed. Result: Production-ready system combining fast inference from trained knowledge with accurate historical reference and flexible tool usage." id="phase5">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#DCEDC8;strokeColor=#689F38;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="680" width="480" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#D84315&quot;&gt;PHASE 6: EVENING TRAINING LOOP&lt;/font&gt;&#xa;&#xa;Nightly | Scheduled: 10 PM | Duration: 2-3 hours&#xa;&#xa;Step 1: Query daily_buffer for untrained patterns&#xa;Step 2: Quality scoring and pattern extraction&#xa;Step 3: Generate incremental samples (50-200)&#xa;Step 4: Incremental LoRA training (1 epoch)&#xa;Step 5: Canary test (validate no regressions)&#xa;Step 6: Mark as trained in RAG metadata&#xa;Step 7: Move PDCAs to pdca_historical&#xa;Step 8: Clear daily_buffer, archive logs&#xa;&#xa;Monitoring: Loss, memory, sample count, completion&#xa;Rollback: Keep last-5 adapters, auto-rollback on failure&#xa;&#xa;Result: Continuous daily improvement" tooltip="Phase 6 implements the evening training loop for continuous learning, running every night at 10 PM for 2-3 hours. This creates a virtuous cycle where the model improves daily from real project work. Step 1 Query Untrained: At 10 PM trigger, query daily_buffer collection for PDCAs and patterns added today. Filter by metadata trained_in_adapter equals False to identify new content. Typical daily yield: 50-200 new samples depending on activity. Step 2 Quality Scoring: Apply quality scoring to select high-value samples. Extract patterns from today work: new problem-solution pairs, refactoring journeys, architectural decisions. Step 3 Generate Samples: Create incremental training samples in JSONL format with input prompt, expected output, and metadata. Samples follow same schema as initial training for consistency. Step 4 Incremental Training: Train LoRA adapter on incremental samples for 1 epoch only with reduced learning rate 1e-4 to avoid catastrophic forgetting. Training takes 2-3 hours for typical 50-sample batch. Step 5 Canary Test: Before promoting new adapter, run 20 must-not-regress tasks comparing new adapter against baseline. Fail if any regression over 5 percent. Step 6 Mark as Trained: Update RAG metadata setting trained_in_adapter equals True, training_batch equals nightly_YYYYMMDD, training_date equals timestamp for all trained chunks. Step 7 Move to Historical: Move PDCAs from daily_buffer to pdca_historical collection. Update Redis Graph with new PRECEDES edges for breadcrumb navigation. Step 8 Clear and Archive: Archive daily_buffer to logs, clear collection, reset for tomorrow. Monitoring: Daily logs track sample count, training loss, memory usage, completion time. Alert on failures or anomalies. Rollback: Keep last-5 nightly adapters. If canary fails, auto-rollback to last-known-good adapter and create incident PDCA. Result: Model continuously improves from daily work while maintaining quality through canary tests and rollback protection." id="phase6">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="790" y="680" width="520" height="420" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#4A148C&quot;&gt;CONTINUOUS OPERATION&lt;/font&gt;&#xa;&#xa;Daily Workflow:&#xa;&#xa;09:00 - 22:00: Production serving&#xa;  • Answer user queries&#xa;  • Generate code and PDCAs&#xa;  • New work indexed to daily_buffer&#xa;&#xa;22:00 - 01:00: Evening training&#xa;  • Train today patterns&#xa;  • Update model&#xa;  • Quality gates&#xa;&#xa;01:00 - 09:00: Production serving&#xa;  • Improved model in production&#xa;  • New patterns available&#xa;&#xa;Result: Self-improving system" tooltip="Continuous operation shows the daily rhythm of the Web4 training system. During daytime 09:00 to 22:00, the production model serves user queries, generates code following Web4 patterns, creates PDCAs with proper structure, and provides historical context via RAG when needed. All new work created during the day including PDCAs, code, decisions, and learnings are automatically indexed into the daily_buffer RAG collection with metadata. At night 22:00 to 01:00, the evening training loop activates: query daily_buffer for untrained patterns, extract and score patterns, generate incremental training samples typically 50-200 samples, train LoRA adapter for 1 epoch with reduced learning rate, run canary tests to validate no regressions, mark trained data in RAG metadata, move PDCAs to historical collection, clear daily_buffer. If all quality gates pass, the improved adapter is promoted to production. If canary fails, rollback to previous adapter and create incident PDCA. From 01:00 to 09:00 next morning, the improved model is in production with yesterday patterns now trained in. Users benefit from model that learned from yesterday work. This cycle repeats daily creating a self-improving system. Benefits: Continuous improvement from real project work, pattern discovery from daily activities, adaptation to evolving practices, efficient incremental learning without full retraining, metadata-driven sample selection ensures quality. The system gets smarter every day while maintaining production stability through canary tests and rollback protection. Over time, the model accumulates deep Web4 domain expertise from hundreds of days of project work." id="continuous">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#7B1FA2;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1570" y="780" width="420" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-phase0-phase1" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#01579B;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase0" target="phase1" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="460" y="260" as="sourcePoint"/>
                        <mxPoint x="520" y="260" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase0-1" value="RAG Ready" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#01579B;fillColor=#E1F5FE;strokeColor=#0288D1;rounded=1;" parent="arrow-phase0-phase1" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase1-phase2" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#F57F17;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase1" target="phase2" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="880" y="300" as="sourcePoint"/>
                        <mxPoint x="940" y="300" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase1-2" value="37K Samples&#xa;~20M Tokens" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#F57F17;fillColor=#FFF9C4;strokeColor=#F9A825;rounded=1;" parent="arrow-phase1-phase2" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase2-phase3" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#2E7D32;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase2" target="phase3" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1300" y="340" as="sourcePoint"/>
                        <mxPoint x="1360" y="340" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase2-3" value="LoRA Adapter&#xa;80MB" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#2E7D32;fillColor=#C8E6C9;strokeColor=#43A047;rounded=1;" parent="arrow-phase2-phase3" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase3-phase4" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#1565C0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase3" target="phase4" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1720" y="300" as="sourcePoint"/>
                        <mxPoint x="1780" y="300" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase3-4" value="GGUF Model&#xa;4GB" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#1565C0;fillColor=#BBDEFB;strokeColor=#1976D2;rounded=1;" parent="arrow-phase3-phase4" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase4-phase5" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#6A1B9A;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.454;entryY=0.006;entryDx=0;entryDy=0;entryPerimeter=0;" parent="1" source="phase4" target="phase5" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1780" y="570" as="sourcePoint"/>
                        <mxPoint x="580" y="500" as="targetPoint"/>
                        <Array as="points">
                            <mxPoint x="2090" y="620"/>
                            <mxPoint x="1780" y="620"/>
                            <mxPoint x="1200" y="620"/>
                            <mxPoint x="320" y="620"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase4-5" value="Quality Validated&#xa;All Gates Pass ✓" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#6A1B9A;fillColor=#F3E5F5;strokeColor=#8E24AA;rounded=1;" parent="arrow-phase4-phase5" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint x="320" y="-10" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase5-phase6" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#558B2F;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase5" target="phase6" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="580" y="840" as="sourcePoint"/>
                        <mxPoint x="640" y="840" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase5-6" value="Daily Work&#xa;→ daily_buffer" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#558B2F;fillColor=#DCEDC8;strokeColor=#689F38;rounded=1;" parent="arrow-phase5-phase6" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase6-continuous" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#D84315;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase6" target="continuous" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1160" y="710" as="sourcePoint"/>
                        <mxPoint x="1220" y="760" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase6-cont" value="Improved Model&#xa;Every Night" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#D84315;fillColor=#FFCCBC;strokeColor=#FF5722;rounded=1;" parent="arrow-phase6-continuous" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-continuous-loop" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#7B1FA2;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0.456;entryY=0.996;entryDx=0;entryDy=0;dashed=1;entryPerimeter=0;" parent="1" source="continuous" target="phase5" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1640" y="940" as="sourcePoint"/>
                        <mxPoint x="1700" y="940" as="targetPoint"/>
                        <Array as="points">
                            <mxPoint x="2080" y="940"/>
                            <mxPoint x="2080" y="1150"/>
                            <mxPoint x="319" y="1160"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-loop" value="Continuous&#xa;Daily Cycle" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#7B1FA2;fillColor=#E1BEE7;strokeColor=#8E24AA;rounded=1;" parent="arrow-continuous-loop" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint x="15" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#BF360C&quot;&gt;⚠️ FAILURE HANDLING&lt;/font&gt;&#xa;&#xa;Quality Gate Failure (Phase 4):&#xa;• Halt deployment immediately&#xa;• Keep current production model&#xa;• Rollback to last-known-good adapter&#xa;• Create incident PDCA&#xa;• Root cause analysis&#xa;• Fix issues and retry&#xa;&#xa;Canary Test Failure (Phase 6):&#xa;• Auto-rollback to previous adapter&#xa;• Create incident PDCA&#xa;• Alert on-call&#xa;• Investigate training data quality&#xa;• Skip tonight update, retry tomorrow" tooltip="Failure handling ensures production stability when quality gates or canary tests fail. Quality Gate Failure in Phase 4: If any ship gate fails during evaluation Pattern Compliance under 95 percent, PDCA Template under 95 percent, Refusal F1 under 0.98, or Overall under 90 percent, immediately halt deployment. Keep current production model serving traffic. Rollback training to last-known-good adapter saved from previous successful training. Create incident PDCA documenting which gate failed, by how much, sample failures, and initial observations. Conduct root cause analysis: inspect training data quality, review hyperparameters, check for data distribution shifts, validate evaluation harness correctness. Fix identified issues: curate better training samples, adjust hyperparameters, fix bugs in data pipeline. Retry training after fixes applied. Do not deploy to production until all gates pass. Canary Test Failure in Phase 6: If nightly canary test detects regression over 5 percent on any of 20 must-not-regress tasks, automatically rollback to previous nightly adapter without human intervention. Create incident PDCA documenting which canary task regressed, baseline score, new score, and regression magnitude. Alert on-call engineer via PagerDuty or Slack for investigation. Investigate training data quality from today daily_buffer: were there low-quality samples, outliers, or distribution shifts. Skip tonight evening loop update, keeping yesterday model in production. Retry tomorrow night after data quality issues addressed. The system maintains last-5 nightly adapters enabling rollback to any recent version. Failure handling philosophy: Fail closed, never deploy broken model. Automate rollback for speed. Document failures for learning. Investigate root causes systematically. Production stability over feature velocity." id="failure">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFEBEE;strokeColor=#C62828;strokeWidth=3;fontSize=12;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1700" y="1230" width="460" height="300" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1B5E20&quot;&gt;📊 TIMELINE SUMMARY&lt;/font&gt;&#xa;&#xa;Day 1: Bootstrap RAG (1 hour)&#xa;Week 1-2: Generate 37K samples from RAG queries (10 days)&#xa;Week 3: Train LoRA (8-11 hours) → Merge and Quantize (2 hours) → Evaluate (4 hours) → Deploy (1 hour)&#xa;Week 3+: Production serving + Nightly improvements (continuous)&#xa;&#xa;Total Initial: ~3 weeks from zero to production&#xa;Continuous: Daily 2-3 hour training overnight, improved model every morning&#xa;&#xa;Key Innovation: RAG is both training data source AND runtime reference library" tooltip="Timeline summary shows the complete journey from zero to production in approximately 3 weeks. Day 1 Bootstrap: 1 hour to install ChromaDB, Redis Graph, SQLite and index all 534 PDCAs, 3,477 TypeScript files, 238 process docs, and 12K tool examples. This creates the three-tier RAG system as single source of truth. Week 1-2 Sample Generation: 10 days to generate all 37K training samples via intelligent RAG queries. Use semantic search, graph expansion, and temporal filtering to extract patterns, select representatives, and ensure diversity. Save to JSONL files totaling approximately 20M tokens. Week 3 Training: 8-11 hours to train LoRA adapter on 37K samples using M1 Mac with MPS backend. Monitor loss convergence, memory usage, gradient stability. Output: 80MB LoRA adapter. Week 3 Post-Processing: 2 hours to merge adapter with base model and quantize from FP16 to Q4_K_M 4GB GGUF. 4 hours to run comprehensive evaluation with 2K hold-out samples across 6 test harnesses and 20 canary tasks. 1 hour to deploy validated model to Ollama and configure RAG integration. Week 3+ Continuous Operation: Production serving during daytime with model answering queries, generating code, creating PDCAs. Daily work indexed to daily_buffer. Every night at 10 PM, evening training loop activates: extract patterns from daily_buffer, train incrementally for 1 epoch taking 2-3 hours, validate with canary tests, promote if passed. Improved model in production next morning. This cycle repeats indefinitely creating self-improving system. Key Innovation: RAG serves dual purpose as training data source during initial sample generation AND runtime reference library for historical queries and tool injection. This ensures consistency between training and deployment. Total timeline: 3 weeks initial setup, then continuous daily improvements forever." id="timeline">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E8F5E9;strokeColor=#2E7D32;strokeWidth=3;fontSize=12;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="1230" width="840" height="240" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#004D40&quot;&gt;💡 KEY BENEFITS&lt;/font&gt;&#xa;&#xa;✓ Single Source of Truth: RAG for both training and runtime&#xa;✓ Metadata-Driven: Intelligent sampling via semantic + graph + temporal&#xa;✓ Quality Assurance: 6 test harnesses + 20 canary tasks&#xa;✓ Continuous Learning: Nightly improvements from daily work&#xa;✓ Production Stability: Canary tests + auto-rollback&#xa;✓ Token Efficiency: 37K samples, 20M tokens, 95% Web4-focused&#xa;✓ Fast Training: 8-11 hours (20% faster than 46K/25M approach)&#xa;✓ Hybrid Tools: 1K trained + 12K RAG for IDE flexibility&#xa;✓ Self-Improving: Gets smarter every day, learns from real work" tooltip="Key benefits of the RAG-first training pipeline. Single Source of Truth: RAG serves as the authoritative data source for both initial training sample generation and runtime historical queries, ensuring consistency and traceability. All 37K training samples are generated via RAG queries, not from raw files. Metadata-Driven Sampling: Intelligent sample generation combines semantic search to find patterns, graph expansion to include context, and temporal filtering to ensure diversity. Metadata fields enable precise filtering by CMM level, task type, agent, date, and quality score. Quality Assurance: Comprehensive evaluation with 6 automated test harnesses Pattern Compliance, PDCA Template, TRON Format, Empty Constructor, Tool Success, Refusal F1 plus 20 canary tasks for must-not-regress validation. Binary pass fail gates prevent broken models from reaching production. Continuous Learning: The evening training loop runs nightly extracting patterns from today daily_buffer, training incrementally for 1 epoch, and promoting improved adapter to production every morning. Model gets smarter from real project work. Production Stability: Canary tests validate no regressions before promoting nightly adapters. Auto-rollback on failure keeps production stable. Keep last-5 adapters for safety. Token Efficiency: Optimized 37K samples with 20M tokens (reduced from 46K/25M) achieves 95 percent Web4-specific focus versus 74 percent in old approach. Distillation and RAG storage save 5M tokens. Fast Training: 8-11 hours full training on M1 Mac is 20 percent faster than previous 10-14 hours, enabled by reduced token count and optimized sampling. Hybrid Tool Architecture: Train 1K generic tool awareness, store 12K IDE-specific examples in RAG. Switch IDEs in 5 minutes versus 10-14 hours retraining. Supports Continue, Cursor, and custom tools simultaneously. Self-Improving System: The model accumulates deep Web4 domain expertise from hundreds of days of project work, learning from successes and failures, adapting to evolving practices, discovering new patterns organically." id="benefits">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E0F2F1;strokeColor=#00695C;strokeWidth=3;fontSize=12;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1000" y="1230" width="640" height="300" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="flow-footer" value="📝 Note: This diagram shows TRAINING PREPARATION (Week 1-2). Production architecture is Training-First: Model uses trained patterns (80-90%), RAG for history (10-20%), RAG for tool syntax (30%). See Knowledge Hierarchy section." style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E3F2FD;strokeColor=#1565C0;strokeWidth=3;align=center;verticalAlign=middle;fontSize=13;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="100" y="1590" width="2060" height="60" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
    <diagram name="Three-Tier RAG Architecture" id="rag-deep-dive">
        <mxGraphModel dx="1603" dy="855" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="2400" pageHeight="1800" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="rag-title" value="Web4 Three-Tier RAG Architecture Deep Dive" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="400" y="20" width="1600" height="50" as="geometry"/>
                </mxCell>
                <mxCell id="rag-subtitle" value="Hybrid Architecture: ChromaDB (Semantic) + Redis Graph (Breadcrumb) + SQLite (Temporal) | 534 PDCAs → 2,670 chunks | PDCA-Aware Adaptive Chunking | 15+ Metadata Fields | Hybrid Retrieval for Complete Context" style="text;html=1;strokeColor=none;fillColor=#FFF9C4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=16;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="200" y="80" width="2000" height="35" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;TIER 1: CHROMADB&lt;/font&gt;&lt;br&gt;&lt;font color=&quot;#E65100&quot;&gt;(Semantic Search)&lt;/font&gt;&#xa;&#xa;Purpose: Find similar PDCAs by meaning&#xa;&#xa;Data Structure:&#xa;• Collection: pdca_historical&#xa;• Documents: 534 PDCAs&#xa;• Chunks: ~2,670 (5 per PDCA)&#xa;• Embeddings: 768-dimensional vectors&#xa;• Index: HNSW (fast similarity search)&#xa;&#xa;Query Speed: ~500ms&#xa;Best For: Semantic similarity" tooltip="Tier 1 ChromaDB provides semantic search capabilities using vector embeddings. ChromaDB is an open-source vector database optimized for embedding-based similarity search. Each PDCA chunk is converted to a 768-dimensional vector using sentence-transformers/all-MiniLM-L6-v2 model, capturing semantic meaning beyond keyword matching. The 534 historical PDCAs are processed with PDCA-aware adaptive chunking creating approximately 2,670 semantically complete chunks (average 5 per PDCA). Chunks preserve document structure by splitting on section boundaries: header metadata, Plan section, Do section, Check section, Act plus Meta sections. Each chunk includes 15+ metadata fields enabling filtered queries. ChromaDB uses HNSW Hierarchical Navigable Small World index for fast approximate nearest neighbor search, returning results in approximately 500ms. The metadata fields enable precise filtering by chunk_type (header, plan, do, check, act, meta), cmm_level (CMM1-CMM4), task_type (component creation, debugging, refactoring, integration), date (YYYY-MM-DD), agent_name, agent_role, session_id, branch, sprint, verification_status (verified, unverified), and quality_score (0-100). ChromaDB is ideal for queries like show me similar debugging approaches, find PDCAs about component versioning, or what patterns exist for refactoring. The semantic search finds relevant PDCAs even if they use different terminology because embeddings capture meaning. During training sample generation, ChromaDB is queried thousands of times to extract patterns: query for empty constructor examples filtered by pattern equals empty_constructor, query for CMM3 transformations filtered by cmm_level equals CMM3 and task_type equals refactoring. ChromaDB also indexes 3,477 TypeScript component files organized by layer (layer2, layer3, layer5) and pattern (empty_constructor, scenario_state, radical_oop), plus 238 process documents organized by role." id="tier1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="150" width="600" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#C62828&quot;&gt;TIER 2: REDIS GRAPH&lt;/font&gt;&lt;br&gt;&lt;font color=&quot;#C62828&quot;&gt;(Breadcrumb Navigation)&lt;/font&gt;&#xa;&#xa;Purpose: Walk PDCA chains for context&#xa;&#xa;Data Structure:&#xa;• Nodes: 534 PDCAs&#xa;• Edges: PRECEDES relationships&#xa;• Properties: pdca_id, agent, date, session&#xa;• Graph traversal: Cypher-like queries&#xa;&#xa;Query Speed: ~10ms&#xa;Best For: Adjacency, context expansion" tooltip="Tier 2 Redis Graph stores PDCA breadcrumb relationships as a graph database, enabling fast traversal of prev/next links. Redis Graph is built on Redis using sparse adjacency matrices for efficient graph operations. Each of the 534 PDCAs becomes a node with properties: pdca_id (unique identifier), agent_name (who created it), agent_role (SaveRestartAgent, NegotiatorAgent, etc), date (YYYY-MM-DD), session_id (timestamp-based), branch (dev, main, feat), sprint (sprint number if applicable), objective (one-line summary). PRECEDES edges connect chronologically related PDCAs based on backward and forward links extracted from PDCA metadata. Graph queries are extremely fast approximately 10ms compared to vector search approximately 500ms because they use index lookups rather than similarity computation. The primary use case is read-to-depth-3: when semantic search finds a relevant PDCA, walk the graph backward and forward up to 3 levels deep to understand the full context. This implements the Web4 principle that context matters - a single PDCA in isolation may miss important background from previous work or follow-up from subsequent PDCAs. Example query: MATCH (p:PDCA {pdca_id: &#39;found_pdca&#39;}) OPTIONAL MATCH (prev1)-[:PRECEDES]-&gt;(p) OPTIONAL MATCH (prev2)-[:PRECEDES]-&gt;(prev1) OPTIONAL MATCH (prev3)-[:PRECEDES]-&gt;(prev2) OPTIONAL MATCH (p)-[:PRECEDES]-&gt;(next1) OPTIONAL MATCH (next1)-[:PRECEDES]-&gt;(next2) OPTIONAL MATCH (next2)-[:PRECEDES]-&gt;(next3) RETURN prev3, prev2, prev1, p, next1, next2, next3. This query walks 3 levels backward and 3 levels forward in the breadcrumb chain. Redis Graph supports Cypher-like query language familiar to Neo4j users. During training sample generation, graph expansion enriches semantic search results: query ChromaDB for relevant PDCA, get pdca_id from result, query Redis Graph for breadcrumb chain, fetch full context from ChromaDB for all PDCAs in chain, include context in training sample. This ensures training samples have rich context rather than isolated snippets." id="tier2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCDD2;strokeColor=#C62828;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="800" y="150" width="600" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;TIER 3: SQLITE&lt;/font&gt;&lt;br&gt;&lt;font color=&quot;#1565C0&quot;&gt;(Temporal Queries)&lt;/font&gt;&#xa;&#xa;Purpose: Fast date-based queries&#xa;&#xa;Data Structure:&#xa;• Table: pdca_timeline&#xa;• Columns: pdca_id, timestamp, agent, role, session, branch, sprint, cmm_level, objective&#xa;• Indexes: B-tree on timestamp, agent, cmm_level&#xa;&#xa;Query Speed: ~5ms&#xa;Best For: Date ranges, agent timelines" tooltip="Tier 3 SQLite stores temporal and categorical metadata in a relational schema optimized for fast date-range queries, agent timelines, and sprint aggregations. SQLite is a lightweight embedded SQL database requiring no server, perfect for local queries. The pdca_timeline table schema: pdca_id TEXT PRIMARY KEY (unique identifier), timestamp INTEGER (Unix timestamp for precise ordering), date TEXT (YYYY-MM-DD for date-range queries), session TEXT (session identifier YYYY-MM-DD-UTC-HHMM-agent), agent_name TEXT (creator), agent_role TEXT (role type), branch TEXT (git branch), sprint TEXT (sprint number), cmm_level TEXT (CMM1, CMM2, CMM3, CMM4), objective TEXT (one-line summary), quality_score INTEGER (0-100). Indexes enable sub-5ms queries: CREATE INDEX idx_date ON pdca_timeline(date), CREATE INDEX idx_agent ON pdca_timeline(agent_name), CREATE INDEX idx_cmm ON pdca_timeline(cmm_level), CREATE INDEX idx_timestamp ON pdca_timeline(timestamp). These B-tree indexes are 100x faster than scanning ChromaDB with metadata filters because SQL databases are optimized for structured queries. Example queries: SELECT * FROM pdca_timeline WHERE date BETWEEN &#39;2025-10-15&#39; AND &#39;2025-10-20&#39; ORDER BY timestamp (all work from Oct 15-20), SELECT agent_name, COUNT(*) as count FROM pdca_timeline GROUP BY agent_name ORDER BY count DESC (most active agents), SELECT cmm_level, COUNT(*) FROM pdca_timeline WHERE date &gt; &#39;2025-10-01&#39; GROUP BY cmm_level (CMM distribution this month), SELECT * FROM pdca_timeline WHERE agent_name = &#39;SaveRestartAgent&#39; ORDER BY timestamp DESC LIMIT 10 (recent work by specific agent). During training sample generation, temporal queries ensure diverse time period coverage to prevent temporal bias where the model only learns the newest patterns. Stratified sampling: query SQLite to get PDCA IDs from different date ranges (2024-Q1, 2024-Q2, 2024-Q3, 2024-Q4, 2025-Q1), ensure representative samples from each period, query ChromaDB for full content using filtered PDCA IDs. SQLite is also used for analytics dashboards: track PDCAs per day, measure sprint velocity, monitor CMM compliance trends over time." id="tier3">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1565C0;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1500" y="150" width="600" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#F57F17&quot;&gt;PDCA-AWARE ADAPTIVE CHUNKING&lt;/font&gt;&#xa;&#xa;Problem: Standard 512-token chunks destroy structure&#xa;Solution: Section-aware semantic chunking&#xa;&#xa;Chunking Strategy:&#xa;1. Parse PDCA markdown structure&#xa;2. Identify section boundaries (## headers)&#xa;3. Create chunks preserving sections:&#xa;   • Chunk 1: Header + metadata (links, objective)&#xa;   • Chunk 2: Plan section (TRON, strategy)&#xa;   • Chunk 3: Do section (execution)&#xa;   • Chunk 4: Check section (verification, QA)&#xa;   • Chunk 5: Act + Meta (learnings, CMM badge)&#xa;4. Each chunk 400-800 tokens (semantic unit)&#xa;5. Add comprehensive metadata (15+ fields)&#xa;&#xa;Result: Semantically complete retrievals" tooltip="PDCA-Aware Adaptive Chunking solves the problem of structure loss in standard fixed-size chunking. Standard RAG approach: split documents into fixed 512-token chunks without regard to structure. This destroys PDCA sections, scatters metadata across chunks, loses context between sections, and creates false positive retrievals when queries match fragments. Web4 solution: section-aware semantic chunking that preserves document structure. The chunking algorithm: Step 1 Parse PDCA markdown using regex to identify section headers (## LINKS, ## PLAN, ## DO, ## CHECK, ## ACT, ## META). Step 2 Extract metadata from header: backward link, forward link, objective, agent, role, date, session, branch, sprint. Step 3 Create semantically complete chunks: Chunk 1 Header contains links section, objective, agent metadata approximately 200-400 tokens with metadata chunk_type equals header. Chunk 2 Plan contains TRON decision (Trigger, Response, Outcome, Next), strategy, approach approximately 400-800 tokens with metadata chunk_type equals plan. Chunk 3 Do contains execution steps, code changes, commands run approximately 400-800 tokens with metadata chunk_type equals do. Chunk 4 Check contains verification steps, QA decisions, test results approximately 400-800 tokens with metadata chunk_type equals check. Chunk 5 Act plus Meta contains learnings, improvements, next steps, CMM badge, template version approximately 300-600 tokens with metadata chunk_type equals act. Step 4 Add comprehensive metadata to each chunk: pdca_id (unique identifier), chunk_index (0-4), chunk_type (header/plan/do/check/act), chunk_content (actual text), agent_name, agent_role, date, session_id, branch, sprint, cmm_level (CMM1-CMM4), task_type (component creation, debugging, refactoring, integration, collaboration), objective (one-line summary), quality_score (0-100 based on completeness, CMM compliance, dual links), verification_status (verified if dual links valid, unverified otherwise), trained_in_adapter (False initially, True after training), training_batch (which batch trained this), training_date (when trained). Step 5 Index chunks into ChromaDB with embeddings and metadata, store PRECEDES edges in Redis Graph, store timeline in SQLite. Benefits: Retrieval returns complete sections not fragments, metadata enables precise filtering, structure preserved for pattern extraction, false positives reduced, training samples have full context." id="chunking">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F9A825;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="540" width="1000" height="380" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#6A1B9A&quot;&gt;COMPREHENSIVE METADATA SCHEMA (15+ Fields)&lt;/font&gt;&#xa;&#xa;Temporal: date, timestamp, session_id&#xa;Agent Context: agent_name, agent_role, branch, sprint&#xa;Work Context: task_type, objective, cmm_level&#xa;Task Classification: chunk_type, chunk_index&#xa;Quality Signals: quality_score, verification_status&#xa;Training Lifecycle: trained_in_adapter, training_batch, training_date&#xa;&#xa;Enables:&#xa;• Filtered queries (CMM3 refactorings from Oct 2025)&#xa;• Stratified sampling (diverse dates, agents, tasks)&#xa;• Quality ranking (sort by quality_score DESC)&#xa;• Training tracking (query untrained with trained_in_adapter = False)&#xa;• Audit trail (when was this trained, in which batch)" tooltip="Comprehensive metadata schema enables intelligent querying and sampling beyond simple semantic search. The 15+ metadata fields are organized into categories. Temporal metadata: date TEXT (YYYY-MM-DD for date-range queries), timestamp INTEGER (Unix timestamp for precise ordering), session_id TEXT (session identifier YYYY-MM-DD-UTC-HHMM-agent for grouping work). Agent context metadata: agent_name TEXT (who created this PDCA), agent_role TEXT (SaveRestartAgent, NegotiatorAgent, BuilderAgent, TesterAgent, RefinerAgent, IntegratorAgent), branch TEXT (git branch where work happened dev, main, feat/ticket), sprint TEXT (sprint number if part of sprint work). Work context metadata: task_type TEXT (component_creation, debugging, refactoring, integration, collaboration, architectural_decision, violation_fix, testing), objective TEXT (one-line summary of PDCA purpose), cmm_level TEXT (CMM1, CMM2, CMM3, CMM4 indicating process maturity). Task classification metadata: chunk_type TEXT (header, plan, do, check, act indicating which section this chunk contains), chunk_index INTEGER (0-4 indicating position in document). Quality signals metadata: quality_score INTEGER (0-100 computed from completeness of sections, CMM compliance, dual link validity, TRON format adherence), verification_status TEXT (verified if dual links valid and file pushed, unverified if links missing or not pushed). Training lifecycle metadata: trained_in_adapter BOOLEAN (False initially, True after training), training_batch TEXT (initial_20251027 or nightly_YYYYMMDD indicating which batch trained this), training_date TEXT (ISO8601 timestamp when training occurred). Use cases enabled by metadata: Filtered queries - ChromaDB query with where filter cmm_level equals CMM3 AND task_type equals refactoring AND date greater than 2025-10-01 finds CMM3 refactorings from October. Stratified sampling - SQLite query SELECT pdca_id FROM pdca_timeline WHERE date BETWEEN &#39;2024-01-01&#39; AND &#39;2024-03-31&#39; LIMIT 50 gets Q1 2024 samples, repeat for Q2, Q3, Q4, Q1 2025 ensuring temporal diversity. Quality ranking - ChromaDB query with where filter quality_score greater than 80 sorted by quality_score DESC finds highest quality PDCAs for representative samples. Training tracking - Evening loop query ChromaDB with where filter trained_in_adapter equals False finds untrained patterns from daily_buffer. Audit trail - Query training_batch equals nightly_20251028 finds all chunks trained in tonight batch for rollback if needed. The rich metadata transforms RAG from simple semantic search to intelligent data warehouse enabling sophisticated sampling strategies for training." id="metadata">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#7B1FA2;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1200" y="540" width="900" height="380" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#2E7D32&quot;&gt;HYBRID RETRIEVAL: COMBINING ALL THREE TIERS&lt;/font&gt;&#xa;&#xa;Query: How did we solve component versioning conflicts?&#xa;&#xa;Step 1: Semantic Search (ChromaDB) - ~500ms&#xa;  query = &#39;component versioning conflicts&#39;&#xa;  where = {task_type: &#39;debugging&#39;, cmm_level: &#39;CMM3&#39;}&#xa;  results = 5 relevant PDCAs&#xa;&#xa;Step 2: Graph Expansion (Redis Graph) - ~10ms per PDCA&#xa;  For each result, walk breadcrumb chain depth 3&#xa;  Get context: what came before, what came after&#xa;  Total: 5 * 7 PDCAs = 35 PDCAs (with context)&#xa;&#xa;Step 3: Temporal Filtering (SQLite) - ~5ms&#xa;  Ensure diverse time periods (not all from last month)&#xa;  SELECT FROM pdca_timeline WHERE pdca_id IN (results)&#xa;  AND date distributed across quarters&#xa;&#xa;Step 4: Re-rank and Return&#xa;  Sort by quality_score DESC&#xa;  Return top 5 with full breadcrumb context&#xa;  Total latency: ~600ms&#xa;&#xa;Result: Semantically relevant + contextually complete + temporally diverse" tooltip="Hybrid retrieval combines all three tiers to provide optimal results. Single-tier approaches have limitations: ChromaDB alone returns relevant PDCAs but misses context from breadcrumb chains. Redis Graph alone requires knowing which PDCA to start from. SQLite alone has no semantic understanding. Hybrid approach leverages each tier strength. Example query: How did we solve component versioning conflicts? Step 1 Semantic Search (ChromaDB approximately 500ms): Build query vector from user question using same sentence-transformer model. Query ChromaDB pdca_historical collection with semantic search. Apply metadata filters: where equals task_type colon debugging AND cmm_level colon CMM3 to find high-quality debugging PDCAs. Set n_results equals 10 to get top 10 semantically relevant PDCAs. ChromaDB returns: list of 10 PDCA chunks with pdca_id, chunk_content, metadata, and similarity scores. Step 2 Graph Expansion (Redis Graph approximately 10ms per PDCA): For each of the 10 PDCAs, query Redis Graph for breadcrumb context. Cypher query: MATCH path equals (prev3)-[:PRECEDES]-&gt;(prev2)-[:PRECEDES]-&gt;(prev1)-[:PRECEDES]-&gt;(p {pdca_id: found_id})-[:PRECEDES]-&gt;(next1)-[:PRECEDES]-&gt;(next2)-[:PRECEDES]-&gt;(next3) RETURN prev3, prev2, prev1, p, next1, next2, next3. This walks 3 levels backward and 3 levels forward in the breadcrumb chain. Result: 7 PDCAs per original result (3 before, 1 original, 3 after). Total: 10 times 7 equals 70 PDCAs with context. Graph traversal is 50x faster than semantic search because it uses index lookups. Step 3 Temporal Filtering (SQLite approximately 5ms): Query SQLite pdca_timeline table to check date distribution. SELECT date, COUNT(*) FROM pdca_timeline WHERE pdca_id IN (70 pdca_ids) GROUP BY date. If results are clustered in recent dates, diversify: keep top 5 from recent, add 5 from older periods (Q3 2024, Q4 2024, Q1 2025). This prevents temporal bias where model only learns newest approaches. Step 4 Re-rank and Return: Combine semantic similarity scores from ChromaDB with quality_scores from metadata. Sort by weighted score: 0.7 times similarity plus 0.3 times (quality_score / 100). Return top 5 PDCAs with full breadcrumb context (35 PDCAs total). Total hybrid retrieval latency: approximately 600ms (500ms semantic plus 100ms graph plus 5ms temporal plus 5ms reranking). Result: User gets semantically relevant PDCAs for their question, contextually complete with breadcrumb chains showing what led to and followed from each solution, temporally diverse preventing recency bias, quality-ranked ensuring best examples shown first. This hybrid approach provides far better results than any single tier alone." id="hybrid">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="1000" width="2000" height="500" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#D84315&quot;&gt;🎯 WHY THREE TIERS?&lt;/font&gt;&#xa;&#xa;Single-Tier Limitations:&#xa;❌ ChromaDB alone: Relevant but no context, no date filtering, slow for structured queries&#xa;❌ Graph alone: Fast but requires knowing start node, no semantic understanding&#xa;❌ SQL alone: Fast structured queries but no semantic search, no graph relationships&#xa;&#xa;Three-Tier Benefits:&#xa;✅ ChromaDB: Semantic understanding (find by meaning)&#xa;✅ Redis Graph: Contextual expansion (walk breadcrumb chains)&#xa;✅ SQLite: Temporal diversity (prevent recency bias)&#xa;✅ Combined: Best of all three in 600ms&#xa;&#xa;Real-World Impact:&#xa;• Training sample generation: 37K samples from intelligent RAG queries&#xa;• Runtime retrieval: 10-20% of queries need historical context&#xa;• Quality: Semantically relevant + contextually complete + temporally diverse" tooltip="Why three tiers instead of one? Single-tier limitations: ChromaDB alone provides semantic search which is powerful for finding relevant PDCAs by meaning, but it lacks context from breadcrumb chains (single PDCA in isolation misses what came before and after), has no efficient date-range queries (must scan all chunks with metadata filters which is slow), and cannot handle graph traversal (finding connected PDCAs requires multiple queries). Redis Graph alone provides fast graph traversal approximately 10ms which is excellent for breadcrumb navigation, but it requires knowing which PDCA to start from (cannot search by semantic meaning), has no semantic understanding (cannot find similar approaches if you don&#39;t know the PDCA ID), and stores minimal properties (full content lives in ChromaDB). SQLite alone provides fast structured queries approximately 5ms on indexed columns for date ranges, agent timelines, and aggregations, but it has no semantic search capability (cannot find similar debugging approaches), has no graph relationships (cannot walk breadcrumb chains), and stores only metadata not full content. Three-tier benefits: ChromaDB provides semantic understanding via vector embeddings capturing meaning beyond keywords, enabling queries like find similar debugging approaches even if different terminology used. Redis Graph provides contextual expansion via breadcrumb traversal, enabling read-to-depth-3 principle to include predecessor and successor context, essential for understanding full problem-solving journey. SQLite provides temporal diversity via fast date-range queries preventing recency bias, enabling stratified sampling across time periods to ensure model learns historical patterns not just newest approaches. Combined these three tiers provide: semantic search to find relevant PDCAs, graph expansion to include context, temporal filtering to ensure diversity, all in approximately 600ms total latency. Real-world impact: Training sample generation uses hybrid queries to generate all 37K training samples: semantic queries to find patterns, graph expansion to include context, temporal filtering to ensure representatives from all time periods. Runtime retrieval for 10-20 percent of production queries requiring historical context like how did we solve X before or what did we work on date Y. Quality of results: semantically relevant (ChromaDB finds by meaning), contextually complete (Redis Graph includes breadcrumb context), temporally diverse (SQLite ensures not all recent). This three-tier architecture is the innovation that enables RAG-first training pipeline: RAG becomes single source of truth for both training data generation and runtime historical queries, ensuring consistency and traceability between training and deployment." id="why-three">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="1560" width="1000" height="460" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#01579B&quot;&gt;📊 PERFORMANCE CHARACTERISTICS&lt;/font&gt;&#xa;&#xa;Query Speed Comparison:&#xa;• Semantic Search (ChromaDB): ~500ms&#xa;• Graph Traversal (Redis): ~10ms (50x faster)&#xa;• Temporal Query (SQLite): ~5ms (100x faster)&#xa;• Hybrid Retrieval: ~600ms (all three combined)&#xa;&#xa;Scale:&#xa;• 534 PDCAs indexed&#xa;• ~2,670 chunks in ChromaDB&#xa;• 534 nodes + PRECEDES edges in Redis Graph&#xa;• 534 rows in SQLite pdca_timeline&#xa;• Total storage: ~15MB (compressed)&#xa;&#xa;Indexing Time:&#xa;• Initial bootstrap: ~1 hour (all 534 PDCAs)&#xa;• Incremental: ~2 seconds per new PDCA&#xa;• Evening loop: ~5 minutes for 10-50 PDCAs&#xa;&#xa;Hardware Requirements:&#xa;• RAM: ~2GB for ChromaDB, ~500MB for Redis, ~50MB for SQLite&#xa;• Disk: ~15MB compressed, ~50MB uncompressed&#xa;• CPU: Minimal (batch indexing uses MPS if available)" tooltip="Performance characteristics of the three-tier RAG system. Query speed comparison: Semantic search via ChromaDB takes approximately 500ms because it computes vector similarity across 2,670 chunks using HNSW index which is fast for approximate nearest neighbor but slower than index lookups. Graph traversal via Redis Graph takes approximately 10ms because it uses sparse adjacency matrices and index lookups for PRECEDES edges, making it 50x faster than semantic search for breadcrumb navigation. Temporal query via SQLite takes approximately 5ms because B-tree indexes on timestamp, date, agent, and cmm_level columns enable instant lookups, making it 100x faster than scanning ChromaDB metadata filters. Hybrid retrieval combining all three tiers takes approximately 600ms total: 500ms for semantic search, 10ms times 10 PDCAs equals 100ms for graph expansion, 5ms for temporal filtering, 5ms for reranking. Scale at current dataset size: 534 PDCAs indexed across all three tiers, approximately 2,670 chunks in ChromaDB (average 5 chunks per PDCA preserving section structure), 534 nodes plus PRECEDES edges in Redis Graph (one node per PDCA, edges based on backward forward links), 534 rows in SQLite pdca_timeline table (one row per PDCA with temporal and categorical metadata), total storage approximately 15MB compressed or approximately 50MB uncompressed. Indexing time for various operations: Initial bootstrap indexing all 534 PDCAs takes approximately 1 hour (parsing markdown, generating embeddings, indexing to ChromaDB, building Redis Graph, populating SQLite). Incremental indexing for one new PDCA takes approximately 2 seconds (parse, embed, insert to all three tiers). Evening loop indexing 10-50 new PDCAs takes approximately 5 minutes as part of the nightly training process. Hardware requirements are modest: RAM usage approximately 2GB for ChromaDB collection in memory, approximately 500MB for Redis Graph data structures, approximately 50MB for SQLite database. Disk storage approximately 15MB compressed for all three tiers, approximately 50MB uncompressed if indexes expanded. CPU usage is minimal during queries (sub-second latency), batch indexing can leverage MPS Metal Performance Shaders on M1 Mac for embedding generation which speeds up bootstrap from 2 hours to 1 hour. The system is designed for single-machine deployment on M1 Mac with 32GB RAM, no distributed systems needed at current scale. Future scaling: If PDCAs grow to 5,000 (10x), ChromaDB will still be fast with HNSW index, Redis Graph scales linearly with nodes, SQLite handles millions of rows efficiently. At 50,000 PDCAs (100x) may need to shard ChromaDB or switch to Qdrant for production-grade vector DB, but current three-tier architecture remains sound." id="performance">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FE;strokeColor=#0277BD;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1200" y="1560" width="900" height="460" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="rag-footer" value="🎯 Three-Tier Innovation: Semantic (ChromaDB) + Graph (Redis) + Temporal (SQLite) = Complete Context in 600ms | PDCA-Aware Chunking Preserves Structure | 15+ Metadata Fields Enable Intelligent Sampling | Hybrid Retrieval Combines Best of All Three Tiers" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F9A825;strokeWidth=3;align=center;verticalAlign=middle;fontSize=13;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="100" y="2090" width="2000" height="60" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
    <diagram name="Tool Orchestration Flow" id="tool-flow">
        <mxGraphModel dx="1603" dy="948" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="2400" pageHeight="1800" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="tool-title" value="Web4 Tool Orchestration Flow: RAG-First with Detection" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="300" y="20" width="1800" height="50" as="geometry"/>
                </mxCell>
                <mxCell id="tool-subtitle" value="Production Runtime Detail: Model ALWAYS uses trained knowledge (patterns/methodology) FIRST | RAG supplements with tool syntax when needed (30% of queries, +150ms) | Training-First, not RAG-First" style="text;html=1;strokeColor=none;fillColor=#FFF9C4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=16;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="200" y="80" width="2000" height="35" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;USER PROMPT&lt;/font&gt;&lt;br&gt;&lt;br&gt;Read Button.ts and check&lt;br&gt;for constructor violations" tooltip="The user provides a natural language prompt that may or may not require tool usage. Example: Read Button.tsx and check for constructor violations. This prompt requires two tools: read_file to access the file contents and potentially grep to search for constructor patterns. The challenge for a 7B parameter small LLM is that it has only 1K generic tool awareness samples trained (3 percent of training data) which teaches the CONCEPT of tools but not specific IDE implementations. The model needs explicit examples at runtime to generate correct tool call syntax for the current IDE ecosystem Continue or Cursor. The orchestration system must quickly detect if tools are needed, query RAG for relevant examples if so, inject those examples into the LLM context, and let the model generate the correct tool call. This happens transparently to the user with minimal latency overhead." id="user-prompt">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E3F2FD;strokeColor=#1565C0;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=middle;" parent="1" vertex="1">
                        <mxGeometry x="70" y="150" width="280" height="120" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#F57F17&quot;&gt;KEYWORD DETECTOR&lt;/font&gt;&#xa;&#xa;Duration: ~1ms&#xa;Rule-based pattern matching&#xa;&#xa;Detected: read, .tsx&#xa;Likely tools: read_file" tooltip="The keyword detector is a fast rule-based system that analyzes the user prompt for tool-indicating keywords. It runs in approximately 1ms using simple pattern matching, making it negligible overhead. The detector maintains a dictionary of tool keywords: read shows read_file or read_currently_open_file, create shows create_new_file, edit shows edit_existing_file or single_find_and_replace, run shows run_terminal_command, search shows file_glob_search, list shows ls, fetch shows fetch_url_content. File extensions like .ts, .tsx, .py, .js, .json, .md also suggest read_file. Command patterns like npm, git, install suggest run_terminal_command. For the example prompt Read Button.tsx and check for constructor violations, the detector finds keyword read and file extension .tsx, concluding that read_file is likely needed. This simple heuristic achieves 95 percent plus accuracy because tool usage patterns are predictable. The detector returns a boolean needs_tools and a list of likely_tools. If needs_tools is False, the system skips RAG entirely and goes straight to LLM inference, saving 150ms. This optimization is critical because 70 percent of queries do not need tools." id="keyword-detector">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="610" y="150" width="280" height="180" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#2E7D32&quot;&gt;RAG QUERY&lt;/font&gt;&#xa;&#xa;Duration: ~150ms&#xa;Collection: tool_examples&#xa;&#xa;Where: {&#xa;  tool_ecosystem: continue,&#xa;  tool_name: read_file&#xa;}&#xa;N_results: 2-3 examples" tooltip="If the keyword detector determines that tools are needed, the system queries the RAG tool_examples collection for relevant examples. This query takes approximately 150ms using ChromaDB semantic search. The query filters by tool_ecosystem (continue or cursor depending on current IDE), tool_name (the detected tools like read_file), and optionally usage_pattern (simple, intermediate, complex) and context_type (typescript, python, javascript). The query requests 2-3 examples which is optimal: enough to show the pattern without overloading the context (approximately 300 tokens total). The tool_examples collection contains 12K IDE-specific tool examples stored with comprehensive metadata: tool_name (read_file, grep, run_terminal_command, etc), tool_ecosystem (continue, cursor, custom), tool_version (for ecosystem versioning), usage_pattern (simple, intermediate, complex, edge_case), context_type (typescript, python, javascript, general), is_negative_example (False for correct usage, True for anti-patterns), trained_in_adapter (False, these are NOT trained). These 12K examples are swappable: switching from Continue to Cursor takes 5 minutes (clear Continue tools, index Cursor tools, update ecosystem filter) versus 10-14 hours full model retraining. The RAG query returns the most relevant examples based on semantic similarity to the user prompt, ensuring the injected examples are contextually appropriate." id="rag-query">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="610" y="380" width="280" height="260" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#6A1B9A&quot;&gt;CONTEXT INJECTION&lt;/font&gt;&#xa;&#xa;Duration: ~5ms&#xa;Format: System prompt with examples&#xa;&#xa;[TOOL EXAMPLES - Continue]&#xa;Example 1: read_file syntax&#xa;Example 2: grep syntax&#xa;[END EXAMPLES]&#xa;&#xa;User: Read Button.tsx..." tooltip="The context injection step formats the RAG-retrieved tool examples into a structured prompt that guides the LLM. This takes approximately 5ms for text formatting. The augmented prompt structure: System section contains You are a Web4 assistant with access to tools. Tool Examples section in a clearly delimited block showing 2-3 concrete examples with correct XML syntax for the current IDE. Each example includes the tool call structure, required parameters, and expected usage pattern. User Query section with the original user prompt. This structure ensures the LLM sees the correct tool syntax immediately before generating its response. The injection happens transparently - the user never sees the injected examples, only the final result. The approximately 300 tokens of injected examples add minimal context pressure because the base model has a 32K context window. The examples are formatted to match the IDE tool calling convention: Continue uses XML-style tags, Cursor uses JSON function calls, custom tools use their defined formats. The ToolAwarePromptBuilder class handles this formatting automatically based on the tool_ecosystem setting." id="context-injection">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#7B1FA2;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1260" y="140" width="300" height="260" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#D84315&quot;&gt;LLM INFERENCE&lt;/font&gt;&#xa;&#xa;Duration: ~2000ms&#xa;Model: web4-agent:latest&#xa;(Qwen2.5-Coder-7B + LoRA)&#xa;&#xa;Sees: Examples from RAG&#xa;Generates: Correct tool call&#xa;Follows: Exact syntax&#xa;&#xa;Result: Tool call JSON/XML" tooltip="The LLM processes the augmented prompt and generates the appropriate tool call. Inference takes approximately 2000ms on M1 Mac at approximately 20 tokens per second. The model is web4-agent:latest which combines Qwen2.5-Coder-7B base model general coding knowledge with Web4-specific LoRA adapter patterns and methodology. The adapter was trained on only 1K generic tool awareness samples (3 percent of training) which teaches the CONCEPT of tools: tools have names, take parameters, return results, should be used when reading files or running commands. However, the adapter does NOT contain IDE-specific syntax because that would require training 12K samples (23 percent of budget) and make the model inflexible to IDE changes. Instead, the model learns from the RAG-injected examples at runtime. The LLM sees the system prompt with 2-3 concrete tool examples showing the exact XML or JSON syntax required by the current IDE. The model pattern-matches against these examples and generates a tool call following the same structure. Because the examples are semantically relevant to the user query (retrieved via semantic search), the model can adapt the pattern to the specific task. Example output for Continue: XML tag read_file with child target_file tag containing src/components/Button.tsx. The generated tool call is then executed by the IDE orchestration layer." id="llm-inference">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1260" y="490" width="300" height="300" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#01579B&quot;&gt;TOOL EXECUTION&lt;/font&gt;&#xa;&#xa;Duration: ~100ms&#xa;Execute: read_file&#xa;Path: Button.tsx&#xa;&#xa;Return: File contents&#xa;&#xa;Model continues with&#xa;constructor violation check" tooltip="The generated tool call is executed by the IDE tool orchestration layer. Execution takes approximately 100ms depending on the tool: read_file is fast (disk I/O), run_terminal_command varies by command, grep depends on search scope. The tool execution layer parses the LLM-generated tool call JSON or XML, validates parameters (file paths exist, commands are safe), executes the tool action, captures the result (file contents, command output, search results), formats the result for the LLM, and returns it to the conversation context. The LLM then continues processing with the tool result available. For the example prompt Read Button.tsx and check for constructor violations, the execution sequence: LLM generates read_file tool call, tool layer reads src/components/Button.tsx, file contents returned to LLM context, LLM analyzes contents for constructor violations using its trained Web4 patterns (empty constructor rule), LLM generates response explaining any violations found. This multi-turn interaction happens seamlessly with the tool execution transparent to the user. Total latency for tool-requiring query: 1ms detection plus 150ms RAG query plus 5ms injection plus 2000ms LLM inference plus 100ms tool execution equals approximately 2256ms. Compare to non-tool query: 1ms detection plus 2000ms LLM inference equals approximately 2001ms, saving 255ms by skipping RAG." id="tool-execution">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FE;strokeColor=#0277BD;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1840" y="150" width="300" height="260" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#558B2F&quot;&gt;RESPONSE TO USER&lt;/font&gt;&#xa;&#xa;File contents displayed&#xa;Constructor violations found:&#xa;&#xa;Line 15: Logic in constructor&#xa;Violation: Web4 empty&#xa;constructor pattern requires&#xa;all initialization in init()&#xa;&#xa;Suggestion: Move logic to&#xa;init() method" tooltip="The final response combines tool execution results with the LLM trained Web4 knowledge to provide a comprehensive answer. The response includes file contents if relevant, analysis based on Web4 patterns (empty constructor, 5-layer architecture, Radical OOP), specific violations found with line numbers and explanations, suggestions for fixes following Web4 conventions, and source citations if RAG PDCA history was also consulted. The user receives a high-quality response that correctly used tools (thanks to RAG-injected examples) and applied Web4 patterns (thanks to LoRA-trained knowledge). The entire interaction from user prompt to final response takes approximately 2256ms for tool queries or approximately 2001ms for non-tool queries. The system architecture enables fast responses while maintaining flexibility: the 7B parameter model is small enough to run locally on consumer hardware, the LoRA adapter can be updated nightly without full retraining, the RAG tool examples can be swapped in 5 minutes when changing IDEs, and the keyword detector minimizes overhead for non-tool queries. This RAG-first-with-detection approach is optimal for small LLMs that need explicit examples but cannot afford to query RAG on every request." id="response">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#DCEDC8;strokeColor=#689F38;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1840" y="530" width="300" height="300" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-prompt-detector" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#1565C0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.25;entryDx=0;entryDy=0;" parent="1" source="user-prompt" target="keyword-detector" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="380" y="210" as="sourcePoint"/>
                        <mxPoint x="460" y="195" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-prompt-detector" value="Analyze prompt" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#1565C0;fillColor=#E3F2FD;strokeColor=#1976D2;rounded=1;" parent="arrow-prompt-detector" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-detector-rag" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#F57F17;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" source="keyword-detector" target="rag-query" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="600" y="330" as="sourcePoint"/>
                        <mxPoint x="600" y="380" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-detector-rag" value="Tools needed&#xa;Query RAG" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#F57F17;fillColor=#FFF9C4;strokeColor=#F9A825;rounded=1;" parent="arrow-detector-rag" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-rag-injection" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#2E7D32;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="rag-query" target="context-injection" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="740" y="510" as="sourcePoint"/>
                        <mxPoint x="820" y="280" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-rag-injection" value="2-3 examples&#xa;~300 tokens" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#2E7D32;fillColor=#C8E6C9;strokeColor=#43A047;rounded=1;" parent="arrow-rag-injection" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint x="44" y="-31" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-injection-llm" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#6A1B9A;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" source="context-injection" target="llm-inference" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="970" y="410" as="sourcePoint"/>
                        <mxPoint x="970" y="460" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-injection-llm" value="Augmented prompt&#xa;with examples" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#6A1B9A;fillColor=#F3E5F5;strokeColor=#8E24AA;rounded=1;" parent="arrow-injection-llm" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-llm-execution" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#D84315;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="llm-inference" target="tool-execution" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1120" y="610" as="sourcePoint"/>
                        <mxPoint x="1200" y="280" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-llm-execution" value="Tool call&#xa;JSON/XML" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#D84315;fillColor=#FFCCBC;strokeColor=#FF5722;rounded=1;" parent="arrow-llm-execution" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-execution-response" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#01579B;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" source="tool-execution" target="response" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1350" y="410" as="sourcePoint"/>
                        <mxPoint x="1350" y="460" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-execution-response" value="Tool result&#xa;+ Analysis" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#01579B;fillColor=#E1F5FE;strokeColor=#0288D1;rounded=1;" parent="arrow-execution-response" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-detector-llm-direct" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#9E9E9E;exitX=1;exitY=0.75;exitDx=0;exitDy=0;entryX=0;entryY=0.25;entryDx=0;entryDy=0;dashed=1;" parent="1" source="keyword-detector" target="llm-inference" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="740" y="285" as="sourcePoint"/>
                        <mxPoint x="820" y="535" as="targetPoint"/>
                        <Array as="points">
                            <mxPoint x="1050" y="410"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-detector-llm-direct" value="No tools needed&#xa;Skip RAG (70%)" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#616161;fillColor=#F5F5F5;strokeColor=#9E9E9E;rounded=1;" parent="arrow-detector-llm-direct" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint x="62" y="49" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;📊 LATENCY BREAKDOWN&lt;/font&gt;&#xa;&#xa;Tool Query (30% of requests):&#xa;• Keyword detection: 1ms&#xa;• RAG query: 150ms&#xa;• Context injection: 5ms&#xa;• LLM inference: 2000ms&#xa;• Tool execution: 100ms&#xa;• Total: ~2256ms&#xa;&#xa;Non-Tool Query (70% of requests):&#xa;• Keyword detection: 1ms&#xa;• LLM inference: 2000ms&#xa;• Total: ~2001ms (255ms saved)&#xa;&#xa;Weighted Average: ~2077ms" tooltip="Latency breakdown shows the timing for different query types in the RAG-first-with-detection orchestration. Tool Query (30 percent of requests): These queries require tool usage like read this file or run this command. The latency breakdown: Keyword detection 1ms using fast rule-based pattern matching. RAG query 150ms querying ChromaDB tool_examples collection with metadata filters. Context injection 5ms formatting examples into augmented prompt. LLM inference 2000ms generating tool call following injected examples. Tool execution 100ms executing the tool action and returning results. Total approximately 2256ms end-to-end. Non-Tool Query (70 percent of requests): These queries are answered from trained knowledge like explain empty constructor pattern or what is PDCA. The latency breakdown: Keyword detection 1ms checking for tool keywords. LLM inference 2000ms answering directly from trained Web4 patterns. Total approximately 2001ms end-to-end, saving 255ms by skipping RAG. Weighted Average: (0.3 times 2256) plus (0.7 times 2001) equals approximately 2077ms average latency across all queries. The RAG-first-with-detection approach optimizes for the common case (70 percent non-tool) while maintaining correct tool usage (30 percent tool). Compare to RAG-always approach: all queries would take 2256ms for 2150ms average, wasting 73ms per query. Compare to LLM-first approach: tool queries would require double LLM inference (approximately 4000ms) for worse user experience. The hybrid approach achieves the best balance of speed and correctness." id="latency">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="820" width="600" height="400" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#4A148C&quot;&gt;🔄 IDE SWITCHING PROCEDURE&lt;/font&gt;&#xa;&#xa;From Continue to Cursor:&#xa;&#xa;Step 1: Clear Continue tools (2 min)&#xa;  chromadb.delete(where={tool_ecosystem: continue})&#xa;&#xa;Step 2: Index Cursor tools (3 min)&#xa;  Index 12K Cursor tool examples&#xa;  Metadata: tool_ecosystem=cursor&#xa;&#xa;Step 3: Update filter (~5 seconds)&#xa;  ToolAwarePromptBuilder.ecosystem = cursor&#xa;&#xa;Total Time: ~5 minutes&#xa;Compare: Full retraining would take 10-14 hours&#xa;&#xa;Benefits:&#xa;✓ No model retraining required&#xa;✓ Support multiple IDEs simultaneously&#xa;✓ Easy to add custom tools&#xa;✓ Rollback in seconds if needed" tooltip="IDE switching procedure demonstrates the flexibility of the hybrid tool architecture. The process to switch from Continue to Cursor takes approximately 5 minutes versus 10-14 hours for full model retraining. Step 1 Clear Continue Tools (2 minutes): Delete all Continue tool examples from RAG using metadata filter. Command: chromadb.delete collection equals tool_examples where equals tool_ecosystem colon continue. This removes the 12K Continue-specific tool examples while preserving all other RAG data (534 PDCAs, 3477 components, 238 process docs). Step 2 Index Cursor Tools (3 minutes): Index 12K Cursor tool examples from cursor_tools.jsonl or cursor_tools directory. Each example includes tool_name, tool_ecosystem equals cursor, tool_version, usage_pattern, context_type, example_code. Indexing generates embeddings for semantic search and stores comprehensive metadata. Step 3 Update Ecosystem Filter (5 seconds): Update the ToolAwarePromptBuilder configuration to filter for Cursor tools. Set ToolAwarePromptBuilder.tool_ecosystem equals cursor. All future RAG queries will now retrieve Cursor tool examples instead of Continue examples. Total Time: approximately 5 minutes end-to-end. The model immediately starts using Cursor tool syntax without any retraining. Benefits of this approach: No model retraining required saving 10-14 hours of compute time and avoiding potential quality regressions. Support multiple IDEs simultaneously by indexing tools for Continue, Cursor, and custom IDEs with different ecosystem tags, then switching via filter change. Easy to add custom tools for organization-specific workflows by creating tool examples following the metadata schema. Rollback in seconds if issues arise by reverting the ecosystem filter or re-indexing previous tools. This flexibility is impossible with traditional fine-tuning where tool knowledge is baked into model weights." id="ide-switching">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#7B1FA2;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="780" y="820" width="720" height="400" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1B5E20&quot;&gt;💡 KEY BENEFITS&lt;/font&gt;&#xa;&#xa;✓ Fast Non-Tool Queries: 70% skip RAG, 255ms saved&#xa;✓ Correct Tool Syntax: RAG examples guide small LLM&#xa;✓ IDE Flexibility: 5-min switching vs 10-14hr retraining&#xa;✓ Small Token Budget: 1K trained vs 12K in RAG (saves 9K)&#xa;✓ Continuous Updates: Add new tools without retraining&#xa;✓ Multi-IDE Support: Continue + Cursor + Custom simultaneously&#xa;✓ Predictable Latency: 2256ms tool, 2001ms non-tool&#xa;✓ Simple Detection: Rule-based keywords, 95%+ accuracy&#xa;✓ Zero False Negatives: Over-detection is safe (worst case: 150ms waste)&#xa;✓ Production Proven: Handles 30% tool queries, 70% direct answers" tooltip="Key benefits of the RAG-first-with-detection orchestration approach for tool-requiring prompts. Fast Non-Tool Queries: 70 percent of queries do not need tools and skip RAG entirely, saving 255ms latency. The keyword detector adds only 1ms overhead, making the fast path nearly identical to baseline LLM inference. Correct Tool Syntax: The 7B parameter small LLM with only 1K generic tool awareness trained (3 percent of training budget) generates correct IDE-specific tool calls by learning from RAG-injected examples at runtime. This achieves high accuracy without bloating the training dataset. IDE Flexibility: Switching from Continue to Cursor takes 5 minutes (clear old tools, index new tools, update filter) versus 10-14 hours full model retraining. This enables rapid adaptation to new tools and IDEs without compute-intensive retraining cycles. Small Token Budget: Training 1K generic tool awareness samples instead of 12K IDE-specific samples saves 9K samples or approximately 5M tokens (23 percent of training budget). This budget is reallocated to Web4-specific patterns, increasing Web4 focus from 74 percent to 95 percent. Continuous Updates: New tools can be added by indexing additional examples into RAG without retraining the model. Organizations can define custom tools for their workflows and index them with appropriate metadata. Multi-IDE Support: The system can support Continue, Cursor, and custom IDEs simultaneously by indexing all tool sets with different ecosystem tags. Switch between them via filter change at query time. Predictable Latency: Tool queries take approximately 2256ms (1ms plus 150ms plus 5ms plus 2000ms plus 100ms), non-tool queries take approximately 2001ms (1ms plus 2000ms). This predictability enables accurate latency budgeting and SLA guarantees. Simple Detection: Rule-based keyword matching achieves 95 percent plus accuracy with minimal overhead (1ms). The heuristics are interpretable and easily debuggable compared to ML-based classifiers. Zero False Negatives: Over-detection is safe - if the detector thinks tools are needed but they are not, the worst case is 150ms wasted RAG query. Under-detection (false negative) would cause tool calls to fail, but this is rare with comprehensive keyword lists. Production Proven: The system handles 30 percent tool queries requiring runtime RAG injection and 70 percent direct answers from trained knowledge. This distribution validates the hybrid approach and justifies the optimization effort." id="benefits-tool">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E8F5E9;strokeColor=#2E7D32;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="1260" width="1400" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="tool-footer" value="📝 Production Reality: Trained knowledge (patterns/methodology) is PRIMARY and ALWAYS used | RAG is SUPPLEMENTARY for tool syntax (30% queries) and history (10-20% queries) | Training-First, not RAG-First" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F9A825;strokeWidth=3;align=center;verticalAlign=middle;fontSize=13;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="100" y="1640" width="1400" height="60" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
    <diagram name="Implementation Roadmap" id="implementation-roadmap">
        <mxGraphModel dx="2325" dy="855" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="2400" pageHeight="1800" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="impl-title" value="Web4 Balanced Training Strategy: Step-by-Step Implementation Roadmap" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="200" y="20" width="2000" height="50" as="geometry"/>
                </mxCell>
                <mxCell id="impl-subtitle" value="Complete 6-Week Journey: Week 1-2 Setup &amp; Data Generation | Week 3 Training &amp; Evaluation | Week 4-6 Production Deployment &amp; Continuous Learning | 90% Planning, 10% Execution" style="text;html=1;strokeColor=none;fillColor=#E3F2FD;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=16;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="200" y="80" width="2000" height="35" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#01579B&quot;&gt;WEEK 1: SETUP &amp; RAG BOOTSTRAP&lt;/font&gt;&#xa;&#xa;Day 1-2: Environment Setup&#xa;• Install Python 3.10+, Ollama, ChromaDB, Redis, SQLite&#xa;• Clone Web4Articles repo&#xa;• Set up project structure (scripts/, data/, config/, outputs/)&#xa;• Install dependencies (requirements.txt)&#xa;&#xa;Day 3-4: RAG System Bootstrap&#xa;• Run initial_indexing.py (534 PDCAs → 2,670 chunks)&#xa;• Index 3,477 TypeScript files by layer/pattern&#xa;• Index 238 process docs by role&#xa;• Index 12K tool examples (tool_core + tool_neg)&#xa;• Verify three-tier indexing (test queries)&#xa;&#xa;Day 5-7: Data Quality Validation&#xa;• Test semantic search (ChromaDB)&#xa;• Test breadcrumb navigation (Redis Graph)&#xa;• Test temporal queries (SQLite)&#xa;• Validate metadata completeness (15+ fields)&#xa;• Create test harness baseline&#xa;&#xa;Deliverables: ✓ RAG system ready | ✓ 534 PDCAs indexed | ✓ All queries working" tooltip="Week 1 focuses on environment setup and bootstrapping the three-tier RAG system. Day 1-2 Environment Setup: Install Python 3.10 or higher for compatibility with latest libraries. Install Ollama for model deployment (download from ollama.ai). Install ChromaDB for semantic search via pip install chromadb. Install Redis server and RedisGraph module for graph database (brew install redis, pip install redis redisgraph). SQLite comes with Python. Clone Web4Articles repository containing all 534 PDCAs and 3,477 TypeScript files. Create project structure: mkdir scripts data config outputs eval. Install dependencies: pip install -r requirements.txt including transformers, sentence-transformers, peft, torch, chromadb, redis, redisgraph, sqlite3, jsonschema, tqdm. Day 3-4 RAG System Bootstrap: Run scripts/initial_indexing.py to index all 534 historical PDCAs. This script reads each PDCA markdown file, applies PDCA-aware adaptive chunking to create approximately 2,670 semantically complete chunks preserving section boundaries, generates 768-dimensional embeddings using sentence-transformers/all-MiniLM-L6-v2, stores chunks in ChromaDB pdca_historical collection with 15+ metadata fields, creates nodes and PRECEDES edges in Redis Graph for breadcrumb navigation, populates SQLite pdca_timeline table with temporal and categorical metadata. Index 3,477 TypeScript component files from Web4Articles organized by layer (layer2, layer3, layer5) and pattern (empty_constructor, scenario_state, radical_oop). Index 238 process documents including PDCA templates, CMM guides, compliance checklists organized by role. Index 12K tool examples from tool_core.jsonl (10K Continue tools) and tool_neg.jsonl (2K negative examples) into tool_examples collection with metadata for tool_name, tool_ecosystem, tool_version, usage_pattern, context_type. Verify three-tier indexing by running test queries against all three tiers to ensure data is accessible. Day 5-7 Data Quality Validation: Test semantic search on ChromaDB with queries like find debugging approaches or show component versioning conflicts. Test breadcrumb navigation on Redis Graph by walking depth-3 chains forward and backward from sample PDCAs. Test temporal queries on SQLite with date-range queries, agent timelines, CMM distribution. Validate metadata completeness by checking all 15+ fields are populated for sampled chunks: chunk_type, cmm_level, task_type, agent_name, date, quality_score, verification_status. Create test harness baseline by running all 6 test harnesses (Pattern Compliance, PDCA Template, TRON Format, Empty Constructor, Tool Success, Refusal F1) on baseline model to establish improvement targets. Deliverables: Complete three-tier RAG system with 534 PDCAs indexed, all semantic/graph/temporal queries working correctly, metadata validated, test harness baseline established. This foundation enables intelligent sample generation in Week 2." id="week1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FE;strokeColor=#01579B;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="150" width="460" height="400" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#F57F17&quot;&gt;WEEK 2: SAMPLE GENERATION&lt;/font&gt;&#xa;&#xa;Day 8-10: Core Sample Generation (25K)&#xa;• style_core: 12K from TypeScript files&#xa;• domain_patterns: 8K distilled from PDCAs&#xa;• process_framework: 5K from process docs&#xa;&#xa;Day 11-12: Specialized Samples (9K)&#xa;• domain_representatives: 3K (top 200-300 PDCAs)&#xa;• style_refactor: 3K (CMM2→CMM3 transformations)&#xa;• guardrails: 2K (violations, Jest ban)&#xa;• tool_awareness: 1K (generic concepts)&#xa;&#xa;Day 13-14: Validation &amp; QA (3K)&#xa;• eval: 2K hold-out set (stratified)&#xa;• Validate all 37K samples:&#xa;  - Schema compliance&#xa;  - Token distribution (avg 540/sample)&#xa;  - Quality scores&#xa;  - Diversity (temporal, agent, task)&#xa;• Save to JSONL (~20M tokens)&#xa;&#xa;Deliverables: ✓ 37K training samples | ✓ 2K eval set | ✓ Quality validated" tooltip="Week 2 focuses on generating all 37K training samples via intelligent RAG queries. Day 8-10 Core Sample Generation (25K samples): Generate style_core 12K samples by querying ChromaDB for TypeScript files filtered by layer and pattern. Extract empty constructor examples by querying for classes with empty or minimal constructor plus init method. Extract 5-layer architecture examples showing layer2 implementation, layer3 interface, layer5 CLI. Extract Radical OOP examples with deep encapsulation, no public fields, scenario-based state management. Extract scenario-based state patterns with init method, toScenario serialization, immutable scenarios. Each sample includes input prompt describing desired pattern, expected output showing correct implementation, and metadata for task_type and pattern_name. Generate domain_patterns 8K samples by querying ChromaDB for historical PDCAs semantically, then using Redis Graph to expand via breadcrumb chains for context, and extracting distilled patterns. Extract debugging methodologies by querying task_type equals debugging, then distilling problem-solution pairs. Extract architectural decisions by querying task_type equals architectural_decision, capturing TRON format and rationale. Extract integration patterns, collaboration patterns, violation fixes. Each pattern is distilled to core insight (approximately 400-600 tokens) rather than full PDCA (1200-1800 tokens), saving 60 percent tokens while retaining knowledge. Generate process_framework 5K samples from process_docs collection. Extract PDCA structure v3.2.4.2 template with all required sections. Extract TRON decision format with Trigger Response Outcome Next ordering. Extract CMM1-4 progression and compliance criteria. Extract dual link format for breadcrumb navigation. Extract 12-step startup protocol. Extract verification checklists. Extract 50+ key behavioral lessons from trainAI process documentation. Day 11-12 Specialized Samples (9K samples): Generate domain_representatives 3K samples by selecting top 200-300 complete PDCAs via quality scoring (completeness, CMM compliance, dual links, TRON format). Ensure diverse time periods using SQLite temporal queries (not all recent, stratify across Q1 2024, Q2 2024, Q3 2024, Q4 2024, Q1 2025). Ensure diverse agents (SaveRestartAgent, BuilderAgent, TesterAgent, RefinerAgent, IntegratorAgent, NegotiatorAgent). Ensure diverse task types (component_creation, debugging, refactoring, integration, collaboration). Generate variations showing prompt plus expected PDCA structure, PDCA section plus next section, full PDCA generation. Generate style_refactor 3K samples by querying for CMM2 to CMM3 transformation PDCAs. Extract code evolution patterns showing before (CMM2) and after (CMM3) states. Extract continuous improvement mindset, technical debt reduction, pattern application journeys. Generate guardrails 2K samples teaching framework compliance. Extract Jest ban enforcement (use Vitest instead), manual operation prevention (automate everything), security violations (hardcoded secrets), framework violations (constructor logic). Generate tool_awareness 1K samples teaching generic tool-calling concepts. Extract JSON structure, parameter passing, context awareness. Keep IDE-agnostic (no Continue or Cursor specifics - those stay in RAG tool_examples collection). Day 13-14 Validation and QA (3K samples): Generate eval 2K samples stratified across all training categories: 400 style_core, 300 domain_patterns, 200 process_framework, 150 domain_representatives, 150 style_refactor, 100 guardrails, 50 tool_awareness. This hold-out set is NEVER trained, used for unbiased quality measurement. Validate all 37K training samples: Run schema compliance checks ensuring all samples have required fields (task_type, instruction, input, output, metadata). Validate token distribution with average 540 tokens per sample (total approximately 20M tokens). Check quality scores ensuring high-quality content. Validate diversity: temporal (stratified across quarters), agent (all roles represented), task (all types covered). Save to JSONL format: data/style_core.jsonl, data/domain_patterns.jsonl, data/process_framework.jsonl, data/domain_representatives.jsonl, data/style_refactor.jsonl, data/guardrails.jsonl, data/tool_awareness.jsonl, data/eval.jsonl. Deliverables: 37K production-ready training samples generated entirely from RAG queries ensuring consistency and traceability, 2K stratified eval set for unbiased testing, all samples quality validated with documented statistics." id="week2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="640" y="150" width="460" height="400" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#2E7D32&quot;&gt;WEEK 3: TRAINING &amp; DEPLOYMENT&lt;/font&gt;&#xa;&#xa;Day 15-16: LoRA Training (8-11 hours)&#xa;• Base: Qwen2.5-Coder-7B-Instruct (HuggingFace)&#xa;• Train: 37K samples, 2 epochs, ~20M tokens&#xa;• Config: r=16, alpha=32, dropout=0.05&#xa;• Monitor: Loss plateau 0.6-1.0, memory under 28GB&#xa;• Output: LoRA adapter (~80MB)&#xa;&#xa;Day 17: Merge &amp; Quantize (2 hours)&#xa;• Merge adapter + base model&#xa;• Quantize FP16 → Q4_K_M (14GB → 4GB)&#xa;• Convert to GGUF format&#xa;• Import to Ollama: web4-agent:latest&#xa;&#xa;Day 18-19: Evaluation (4 hours)&#xa;• Run 2K eval samples (hold-out set)&#xa;• Test harnesses: 6 automated + 20 canary tasks&#xa;• All gates must pass (Pattern≥95%, Overall≥90%)&#xa;• Document results&#xa;&#xa;Day 20-21: Production Deployment (1 hour)&#xa;• Deploy to Ollama&#xa;• Connect RAG (ChromaDB + Redis + SQLite)&#xa;• Configure ToolAwarePromptBuilder&#xa;• Smoke tests&#xa;&#xa;Deliverables: ✓ Trained model | ✓ Quality gates passed | ✓ Production deployed" tooltip="Week 3 executes training, evaluation, and production deployment. Day 15-16 LoRA Training (8-11 hours compute time): Use scripts/train_lora_mps.py with config/balanced_training.json. Base model: Qwen/Qwen2.5-Coder-7B-Instruct from HuggingFace, full precision FP16 for maximum learning quality during fine-tuning. Training data: 37K samples from JSONL files totaling approximately 20M tokens. Training configuration: 2 epochs (each sample seen twice), batch size 1 with gradient accumulation 12 giving effective batch 12, learning rate 2e-4 with cosine annealing schedule, LoRA hyperparameters rank r equals 16 (creates small trainable matrices), alpha equals 32 (scaling factor), dropout equals 0.05 (regularization). Apply LoRA to all attention and feedforward layers in the 28 transformer layers. Use AdamW optimizer with weight decay 0.01. Real-time monitoring: Loss convergence expecting plateau at 0.6-1.0 range indicating good learning without overfitting. Memory usage must stay under 28GB to prevent OOM crashes on 32GB M1 Mac. Gradient norms should remain stable confirming proper learning dynamics. Perplexity decreasing over time. Training takes 8-11 hours on M1 Mac with MPS Metal Performance Shaders backend (20 percent faster than previous 10-14 hours due to reduced token count from 25M to 20M). Output: LoRA adapter approximately 80MB saved to outputs/web4_balanced_lora_YYYYMMDD/ containing learned Web4-specific patterns without modifying the 14GB base model. The adapter encodes 95 percent Web4-specific patterns (PDCA methodology, code architecture, OOP principles), 3 percent generic tool awareness, 2 percent guardrails. Day 17 Merge and Quantize (2 hours): Use scripts/merge_and_quantize.py to merge the 80MB LoRA adapter weights into the 14GB base model creating unified model with Web4 knowledge permanently integrated. Quantize merged model from FP16 full precision to Q4_K_M 4-bit quantization. Q4_K_M uses 4-bit integers for most weights while keeping higher precision for critical attention layers, optimal balance between size and quality. Size reduction: 14GB FP16 compresses to 4GB Q4_K_M, a 4x reduction enabling deployment on consumer hardware. Quality retention: quantization maintains 95 percent of full precision quality validated through evaluation metrics. Convert to GGUF format: GGUF is efficient file format for LLM storage optimized for CPU and Metal GPU inference, used by Ollama. Create Ollama modelfile defining model configuration (system prompt, temperature, context window, stop tokens). Import quantized GGUF model to Ollama: ollama create web4-agent:latest -f Modelfile. Test load time approximately 3 seconds on M1 Mac cold start, generation speed approximately 20 tokens per second, memory footprint approximately 4GB loaded. Day 18-19 Evaluation (4 hours): Run comprehensive evaluation using 2K hold-out eval samples that were NEVER trained. Test Harness 1 Pattern Compliance: Run eval/test_pdca_schema.py on 100 generated PDCAs validating against v3.2.4.2 schema. Must pass 95 out of 100 (Ship Gate). Test Harness 2 PDCA Template: Run eval/test_pdca_template.py checking all required sections present. Must pass 95 out of 100 (Ship Gate). Test Harness 3 TRON Format: Run eval/test_tron_format.py validating Trigger Response Outcome Next ordering. Must pass 90 out of 100 (Quality Gate). Test Harness 4 Empty Constructor: Run eval/test_empty_constructor.py checking generated classes for no-constructor-logic rule. Must pass 95 out of 100 (Ship Gate). Test Harness 5 Tool Success Rate: Run eval/test_tool_success.py with 100 scripted IDE tasks measuring prompt to correct tool JSON to successful execution. Must pass 85 out of 100 (Quality Gate). Test Harness 6 Refusal F1: Run eval/test_refusal.py on 200-item safety set (100 should-refuse, 100 should-comply). Must achieve F1 at least 0.98 (Ship Gate). Canary Tests: Run 20 must-not-regress tasks comparing new model against baseline. Fail if any regression over 5 percent. Overall Score: Weighted average of all metrics must be at least 90 percent (Ship Gate). Document all results in outputs/eval_report_YYYYMMDD.md. If any Ship Gate fails: halt deployment, rollback to last-known-good adapter, create incident PDCA, investigate root cause, fix issues, retry. If all gates pass: proceed to production deployment. Day 20-21 Production Deployment (1 hour): Deploy web4-agent:latest GGUF model to Ollama model registry. Connect RAG system: ChromaDB for semantic search of 534 PDCAs and components, Redis Graph for breadcrumb navigation, SQLite for temporal queries. Configure ToolAwarePromptBuilder for runtime tool injection: set tool_ecosystem equals continue, configure RAG query parameters (n_results equals 2-3, filters for tool_name and usage_pattern). Start Ollama server: ollama serve providing REST API for LLM queries, chat interface for interactive sessions, embedding endpoint for RAG similarity. Run smoke tests: test trained knowledge queries (should answer under 200ms without RAG), test historical reference queries (should query RAG and include citations), test tool-requiring queries (should detect tools, query RAG tool_examples, inject examples, generate correct tool calls). Monitor response time, RAG hit rate, quality. Deliverables: Production-ready web4-agent:latest model deployed to Ollama with 4GB GGUF format, all quality gates passed and documented, RAG system connected for historical queries (10-20 percent) and tool injection (30 percent), smoke tests validated, system ready for production serving." id="week3">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1180" y="150" width="460" height="400" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#6A1B9A&quot;&gt;WEEK 4-6: PRODUCTION &amp; CONTINUOUS LEARNING&lt;/font&gt;&#xa;&#xa;Week 4: Production Monitoring&#xa;• Monitor: Response time, RAG hit rate, quality&#xa;• Track: Query types (trained 80-90%, RAG 10-20%, tools 30%)&#xa;• Collect: User feedback, error logs&#xa;• Index: Daily work to daily_buffer collection&#xa;• Validate: Evening training loop dry-run&#xa;&#xa;Week 5: Evening Loop Automation&#xa;• Day 1: Set up cron job (10 PM nightly)&#xa;• Day 2-3: Run first evening loop:&#xa;  - Query daily_buffer for untrained patterns&#xa;  - Generate incremental samples (50-200)&#xa;  - Train 1 epoch (2-3 hours)&#xa;  - Run canary tests (20 tasks)&#xa;  - Mark as trained, move to historical&#xa;• Day 4-7: Validate nightly improvements&#xa;&#xa;Week 6: Optimization &amp; Documentation&#xa;• Fine-tune: RAG query parameters&#xa;• Optimize: Caching frequently accessed PDCAs&#xa;• Document: Runbooks, troubleshooting guides&#xa;• Train: Team on system usage&#xa;• Celebrate: Continuous learning in action!&#xa;&#xa;Deliverables: ✓ Production stable | ✓ Evening loop working | ✓ Team trained" tooltip="Week 4-6 focuses on production operations, continuous learning automation, and optimization. Week 4 Production Monitoring (Day 22-28): Monitor production metrics: Response time (should be under 200ms for trained knowledge, approximately 500ms with RAG, approximately 2250ms with tools), RAG hit rate (validating 10-20 percent PDCA history queries, 30 percent tool queries), quality metrics (pattern compliance, PDCA correctness, tool success rate), user feedback. Track query type distribution: 50-60 percent pure trained queries (Web4 patterns, PDCA methodology), 20-30 percent trained plus tools (code generation with file access), 10-15 percent trained plus history (reference past work), 5-10 percent trained plus both (complex queries needing history and tools). Validate weighted average latency approximately 2100ms optimal. Collect operational data: User feedback on response quality (thumbs up/down), error logs for failed queries, edge cases requiring special handling, false positive tool detections, RAG misses where relevant PDCA not retrieved. Index daily work to daily_buffer collection: All new PDCAs created today are automatically indexed with metadata trained_in_adapter equals False. New code changes, decisions, learnings captured. Typical daily yield: 50-200 samples depending on project activity. Validate evening training loop with dry-run: Run scripts/evening_training_loop.py in test mode (no actual training) to verify query logic, sample generation, quality scoring work correctly. Week 5 Evening Loop Automation (Day 29-35): Day 1 Set up automation: Create cron job or systemd timer for 10 PM nightly execution: 0 22 * * * /usr/bin/python3 /path/to/scripts/evening_training_loop.py. Ensure logging to logs/evening_loop_YYYYMMDD.log for debugging. Configure alerting: Slack webhook or email on failure, PagerDuty for critical issues. Day 2-3 Run first evening loop: At 10 PM trigger, scripts/evening_training_loop.py executes. Step 1 Query daily_buffer collection for PDCAs with metadata trained_in_adapter equals False identifying new untrained content. Typical yield: 50-200 samples. Step 2 Apply quality scoring using same metrics as initial training (completeness, CMM compliance, dual links, TRON format, quality_score 0-100). Extract patterns: new problem-solution pairs, refactoring journeys, architectural decisions, learnings. Step 3 Generate incremental training samples in JSONL format following same schema as initial 37K samples. Step 4 Incremental LoRA training on new samples for 1 epoch only with reduced learning rate 1e-4 to avoid catastrophic forgetting of original 37K samples. Training takes 2-3 hours for typical 50-sample batch. Monitor loss, memory, gradient norms. Step 5 Run canary tests: 20 must-not-regress tasks comparing new adapter against baseline adapter from yesterday. Fail if any task regresses over 5 percent. If canary passes: proceed. If canary fails: auto-rollback to yesterday adapter, create incident PDCA, alert on-call, skip tonight update. Step 6 Mark as trained: Update RAG metadata setting trained_in_adapter equals True, training_batch equals nightly_YYYYMMDD, training_date equals ISO8601 timestamp for all trained chunks. Step 7 Move to historical: Move PDCAs from daily_buffer to pdca_historical collection. Update Redis Graph with new PRECEDES edges. Step 8 Clear and archive: Archive daily_buffer to logs/daily_buffer_YYYYMMDD.jsonl, clear collection, reset for tomorrow. Day 4-7 Validate nightly improvements: Each morning, verify improved model in production: Test previously challenging queries (should be better), validate no regressions on baseline tasks (canary protection), monitor quality metrics (should maintain or improve), collect user feedback (should be positive). Track cumulative improvements over week: Model gets smarter from real project work daily, adapts to evolving practices organically, discovers new patterns from production usage. Week 6 Optimization and Documentation (Day 36-42): Fine-tune RAG query parameters: Adjust n_results for optimal context (2-3 examples for tools, 3-5 chunks for history), tune semantic similarity thresholds, optimize metadata filter combinations. Optimize caching: Implement LRU cache for frequently accessed PDCAs reducing RAG query latency from 300ms to 50ms for cached items. Cache top 100 most queried PDCAs based on access logs. Document runbooks: Create docs/runbooks/evening_loop_troubleshooting.md with common failure modes and fixes. Document docs/runbooks/rag_maintenance.md for re-indexing, backup, restore procedures. Document docs/runbooks/model_rollback.md for emergency rollback steps. Train team on system usage: Conduct training session on querying the model (trained vs RAG queries), understanding response latency, providing feedback for quality improvement, monitoring dashboards. Celebrate continuous learning in action: The system is now a self-improving virtuous cycle: production serving generates daily work, evening loop trains patterns nightly, improved model serves next day, repeat. Model accumulates deep Web4 domain expertise from hundreds of days of project work over time. Deliverables: Production system stable with monitored metrics and alerting, evening training loop running nightly with canary protection and auto-rollback, team trained on system usage and maintenance, documentation complete for operations and troubleshooting, continuous learning virtuous cycle established." id="week4-6">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#6A1B9A;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1720" y="150" width="460" height="400" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-week1-2" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#01579B;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="week1" target="week2" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="560" y="350" as="sourcePoint"/>
                        <mxPoint x="610" y="300" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-week1-2" value="RAG Ready&#xa;534 PDCAs Indexed" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#01579B;fillColor=#E1F5FE;strokeColor=#0288D1;rounded=1;" parent="arrow-week1-2" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-week2-3" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#F57F17;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="week2" target="week3" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1100" y="350" as="sourcePoint"/>
                        <mxPoint x="1150" y="300" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-week2-3" value="37K Samples&#xa;~20M Tokens" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#F57F17;fillColor=#FFF9C4;strokeColor=#F9A825;rounded=1;" parent="arrow-week2-3" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-week3-4" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#2E7D32;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="week3" target="week4-6" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1640" y="350" as="sourcePoint"/>
                        <mxPoint x="1690" y="300" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-week3-4" value="Model Deployed&#xa;Quality Validated" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#2E7D32;fillColor=#C8E6C9;strokeColor=#43A047;rounded=1;" parent="arrow-week3-4" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#D84315&quot;&gt;📋 IMPLEMENTATION CHECKLIST&lt;/font&gt;&#xa;&#xa;Prerequisites:&#xa;☐ M1 Mac with 32GB RAM&#xa;☐ Python 3.10+ installed&#xa;☐ Ollama installed (ollama.ai)&#xa;☐ Access to Web4Articles repo&#xa;☐ HuggingFace account (for base model)&#xa;&#xa;Week 1 Checklist:&#xa;☐ ChromaDB installed and tested&#xa;☐ Redis + RedisGraph installed&#xa;☐ SQLite available (comes with Python)&#xa;☐ 534 PDCAs indexed (verify count)&#xa;☐ 3,477 TypeScript files indexed&#xa;☐ 238 process docs indexed&#xa;☐ 12K tool examples indexed&#xa;☐ Test queries work (semantic/graph/temporal)&#xa;☐ Metadata validated (15+ fields)&#xa;☐ Test harness baseline established&#xa;&#xa;Week 2 Checklist:&#xa;☐ style_core.jsonl: 12K samples generated&#xa;☐ domain_patterns.jsonl: 8K samples&#xa;☐ process_framework.jsonl: 5K samples&#xa;☐ domain_representatives.jsonl: 3K samples&#xa;☐ style_refactor.jsonl: 3K samples&#xa;☐ guardrails.jsonl: 2K samples&#xa;☐ tool_awareness.jsonl: 1K samples&#xa;☐ eval.jsonl: 2K samples (hold-out)&#xa;☐ All samples validated (schema, tokens, quality)&#xa;☐ Total: 37K training + 2K eval = ~20M tokens" tooltip="Implementation checklist ensures no steps are missed during the 6-week rollout. Prerequisites: Hardware - M1 Mac with 32GB RAM required for MPS-accelerated training and 4GB model inference. Python 3.10 or higher for compatibility with latest transformers, peft, torch libraries. Ollama installed from ollama.ai for model deployment and serving. Access to Web4Articles repository containing all 534 historical PDCAs, 3,477 TypeScript component files, 238 process documents. HuggingFace account for downloading Qwen/Qwen2.5-Coder-7B-Instruct base model (free account sufficient, no API key needed for downloads). Week 1 Checklist items ensure RAG system is fully functional: ChromaDB installed via pip install chromadb and tested with sample queries. Redis server and RedisGraph module installed (brew install redis on Mac, apt-get install redis-server on Linux, pip install redis redisgraph for Python client). SQLite available (comes with Python standard library, no separate install). Verify 534 PDCAs indexed by querying ChromaDB pdca_historical collection and checking count equals 534. Verify 3,477 TypeScript files indexed in components collection. Verify 238 process docs indexed in process_docs collection. Verify 12K tool examples indexed in tool_examples collection (10K tool_core plus 2K tool_neg). Test semantic queries on ChromaDB returning relevant results. Test graph traversal on Redis Graph walking depth-3 breadcrumb chains. Test temporal queries on SQLite with date ranges and agent filters. Validate metadata completeness by sampling chunks and checking all 15+ fields populated: chunk_type, chunk_index, pdca_id, agent_name, agent_role, date, timestamp, session_id, branch, sprint, cmm_level, task_type, objective, quality_score, verification_status, trained_in_adapter, training_batch, training_date. Establish test harness baseline by running all 6 test harnesses on baseline model (before training) to measure improvement. Week 2 Checklist items ensure all training samples are generated and validated: Generate style_core.jsonl with exactly 12K samples from TypeScript files covering empty constructor, 5-layer architecture, Radical OOP, scenario-based state. Verify schema compliance, token distribution, quality. Generate domain_patterns.jsonl with 8K distilled patterns from PDCAs covering debugging, architectural decisions, integration, collaboration. Verify patterns are distilled (not full PDCAs), semantic, high quality. Generate process_framework.jsonl with 5K samples covering PDCA structure, TRON format, CMM framework, dual links, startup protocol, verification checklists, key lessons. Verify completeness. Generate domain_representatives.jsonl with 3K samples from top 200-300 PDCAs selected by quality scoring. Verify temporal diversity (stratified across quarters), agent diversity (all roles), task diversity (all types). Generate style_refactor.jsonl with 3K samples showing CMM2 to CMM3 transformations, code evolution, continuous improvement. Verify before/after pairs. Generate guardrails.jsonl with 2K samples teaching Jest ban, manual operation prevention, security violations, framework compliance. Verify negative examples clear. Generate tool_awareness.jsonl with 1K samples teaching generic tool concepts (JSON structure, parameters). Verify IDE-agnostic (no Continue or Cursor specifics). Generate eval.jsonl with 2K stratified hold-out samples NEVER trained. Verify stratification across all categories. Validate all 37K training samples plus 2K eval: Check schema compliance (all required fields present), validate token distribution (average 540 tokens per sample, total approximately 20M tokens), verify quality scores high, confirm diversity (temporal, agent, task). Save to JSONL format ready for training." id="checklist1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="620" width="700" height="500" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;📋 IMPLEMENTATION CHECKLIST (continued)&lt;/font&gt;&#xa;&#xa;Week 3 Checklist:&#xa;☐ LoRA training started (8-11 hours)&#xa;☐ Loss converged to 0.6-1.0 plateau&#xa;☐ Memory stayed under 28GB&#xa;☐ Gradient norms stable&#xa;☐ LoRA adapter saved (~80MB)&#xa;☐ Merge + quantize completed (14GB → 4GB)&#xa;☐ GGUF model created&#xa;☐ Imported to Ollama: web4-agent:latest&#xa;☐ Load time ~3 seconds verified&#xa;☐ Generation ~20 tokens/sec verified&#xa;☐ Eval: Pattern Compliance ≥95% (Ship Gate)&#xa;☐ Eval: PDCA Template ≥95% (Ship Gate)&#xa;☐ Eval: TRON Format ≥90% (Quality Gate)&#xa;☐ Eval: Empty Constructor ≥95% (Ship Gate)&#xa;☐ Eval: Tool Success ≥85% (Quality Gate)&#xa;☐ Eval: Refusal F1 ≥0.98 (Ship Gate)&#xa;☐ Eval: Overall Score ≥90% (Ship Gate)&#xa;☐ Canary tests: 20/20 passed (no regressions)&#xa;☐ Eval report documented&#xa;☐ Deployed to production&#xa;☐ RAG connected (ChromaDB + Redis + SQLite)&#xa;☐ ToolAwarePromptBuilder configured&#xa;☐ Smoke tests passed&#xa;&#xa;Week 4-6 Checklist:&#xa;☐ Production monitoring dashboard active&#xa;☐ Response times tracked (trained/RAG/tool)&#xa;☐ RAG hit rate validated (10-20% history, 30% tools)&#xa;☐ User feedback collected&#xa;☐ Daily work indexed to daily_buffer&#xa;☐ Evening loop cron job configured (10 PM)&#xa;☐ First evening loop executed successfully&#xa;☐ Canary tests integrated (20 tasks)&#xa;☐ Auto-rollback tested&#xa;☐ Nightly improvements validated (3-7 nights)&#xa;☐ RAG query parameters optimized&#xa;☐ Caching implemented for hot PDCAs&#xa;☐ Runbooks documented&#xa;☐ Team trained on system usage&#xa;☐ Continuous learning established!" tooltip="Implementation checklist continued for Week 3 through Week 6. Week 3 Checklist items ensure training, evaluation, and deployment succeed: LoRA training started using scripts/train_lora_mps.py with base Qwen/Qwen2.5-Coder-7B-Instruct, 37K samples, 2 epochs, batch size 1, grad accumulation 12, learning rate 2e-4, LoRA r equals 16 alpha equals 32 dropout equals 0.05. Monitor training progress: Loss converged to 0.6-1.0 plateau (if loss stays above 1.5 or drops below 0.4, investigate overfitting or underfitting). Memory usage stayed under 28GB throughout (if OOM crashes, reduce batch size or grad accumulation). Gradient norms stable (if exploding gradients, reduce learning rate). Training completed in 8-11 hours. LoRA adapter saved to outputs/web4_balanced_lora_YYYYMMDD/ approximately 80MB. Run scripts/merge_and_quantize.py to merge adapter with base model and quantize FP16 to Q4_K_M format. Verify merged model 14GB reduced to 4GB Q4_K_M GGUF. Convert to GGUF format for Ollama compatibility. Import to Ollama: ollama create web4-agent:latest -f Modelfile. Verify load time approximately 3 seconds (ollama run web4-agent:latest). Verify generation speed approximately 20 tokens per second on M1 Mac. Run comprehensive evaluation with 2K hold-out eval samples. Test Harness 1 Pattern Compliance: Run eval/test_pdca_schema.py, must pass 95 out of 100 generated PDCAs (Ship Gate - deployment blocked if fails). Test Harness 2 PDCA Template: Run eval/test_pdca_template.py, must pass 95 out of 100 (Ship Gate). Test Harness 3 TRON Format: Run eval/test_tron_format.py, must pass 90 out of 100 (Quality Gate - document if fails but can deploy). Test Harness 4 Empty Constructor: Run eval/test_empty_constructor.py, must pass 95 out of 100 (Ship Gate). Test Harness 5 Tool Success: Run eval/test_tool_success.py on 100 scripted IDE tasks, must pass 85 out of 100 (Quality Gate). Test Harness 6 Refusal F1: Run eval/test_refusal.py on 200-item safety set, must achieve F1 at least 0.98 (Ship Gate). Calculate Overall Score as weighted average, must be at least 90 percent (Ship Gate). Run 20 canary tests comparing new model vs baseline, all 20 must pass (no regression over 5 percent). Document eval results in outputs/eval_report_YYYYMMDD.md with pass/fail for each gate, overall score, recommendations. If all gates pass: proceed to production deployment. If any Ship Gate fails: halt, rollback, investigate. Deploy to production: ollama serve starts Ollama REST API. Connect RAG: Verify ChromaDB pdca_historical collection accessible for semantic search. Verify Redis Graph breadcrumb_graph accessible for depth-3 traversal. Verify SQLite pdca_timeline.db accessible for temporal queries. Configure ToolAwarePromptBuilder: Set tool_ecosystem equals continue, n_results equals 2-3, configure metadata filters. Run smoke tests: Test trained knowledge query like explain empty constructor (should answer under 200ms, no RAG). Test historical reference query like how did we solve component versioning (should query RAG, include citations, approximately 500ms). Test tool query like read Button.tsx (should detect tool, query RAG tool_examples, inject examples, generate correct XML tool call, approximately 2250ms). Verify all smoke tests pass. Week 4-6 Checklist items ensure production stability and continuous learning: Set up production monitoring dashboard tracking response time, RAG hit rate, quality metrics, error rate. Validate response times: Pure trained under 200ms, trained plus RAG history approximately 500ms, trained plus tools approximately 2250ms, weighted average approximately 2100ms. Validate RAG hit rate: 10-20 percent of queries need PDCA history, 30 percent need tool injection, 50-60 percent pure trained. Collect user feedback via thumbs up/down, error logs, edge cases. Index daily work: All new PDCAs created today automatically indexed to daily_buffer with trained_in_adapter equals False. Configure evening loop: Create cron job 0 22 * * * /usr/bin/python3 scripts/evening_training_loop.py. Configure logging to logs/evening_loop_YYYYMMDD.log. Configure alerting (Slack, email, PagerDuty). Run first evening loop: Verify query daily_buffer works, generate incremental samples (50-200), train 1 epoch (2-3 hours), run canary tests (20 tasks), mark as trained, move to historical, clear buffer. Validate nightly improvements: Each morning test previously challenging queries, validate no regressions, monitor quality, collect feedback. After 3-7 nights, confirm continuous learning virtuous cycle established. Optimize RAG query parameters: Tune n_results for context, adjust similarity thresholds, optimize filters. Implement caching: LRU cache for top 100 most accessed PDCAs (300ms to 50ms). Document runbooks: Troubleshooting evening loop failures, RAG maintenance (backup, restore, re-index), model rollback procedures. Train team: Conduct session on querying model, understanding latency, providing feedback, monitoring dashboards. Celebrate: Continuous learning in action, model gets smarter every day from real work!" id="checklist2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1565C0;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="880" y="620" width="700" height="500" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#558B2F&quot;&gt;🎯 SUCCESS CRITERIA&lt;/font&gt;&#xa;&#xa;Week 1 Success:&#xa;✓ RAG system operational with 3 tiers&#xa;✓ All 534 PDCAs queryable (semantic/graph/temporal)&#xa;✓ Test queries return relevant results under 1 second&#xa;✓ Metadata complete (15+ fields per chunk)&#xa;&#xa;Week 2 Success:&#xa;✓ 37K training samples generated from RAG&#xa;✓ 2K eval samples stratified (never trained)&#xa;✓ Token count ~20M (avg 540/sample)&#xa;✓ Quality validated (schema, diversity, scores)&#xa;✓ All JSONL files ready for training&#xa;&#xa;Week 3 Success:&#xa;✓ LoRA adapter trained (loss 0.6-1.0, memory under 28GB)&#xa;✓ Model merged and quantized (4GB GGUF)&#xa;✓ All quality gates passed (Pattern≥95%, Overall≥90%)&#xa;✓ Deployed to production (load ~3s, gen ~20tok/s)&#xa;✓ Smoke tests passing (trained/RAG/tool queries)&#xa;&#xa;Week 4-6 Success:&#xa;✓ Production stable (monitored metrics healthy)&#xa;✓ Response times optimal (weighted avg ~2100ms)&#xa;✓ RAG hit rates validated (10-20% history, 30% tools)&#xa;✓ Evening loop running nightly (canary protected)&#xa;✓ Model improving daily (nightly training working)&#xa;✓ Team trained and confident&#xa;✓ Documentation complete&#xa;&#xa;🎉 FINAL SUCCESS: Self-improving system operational!" tooltip="Success criteria define what good looks like at each milestone. Week 1 Success validated by: RAG system operational with three tiers - ChromaDB for semantic search, Redis Graph for breadcrumb navigation, SQLite for temporal queries. All 534 PDCAs queryable via all three methods. Test semantic queries like find debugging approaches return relevant PDCAs with similarity scores. Test graph queries walk depth-3 breadcrumb chains forward and backward. Test temporal queries find PDCAs by date range, agent, sprint. Query speed: Semantic approximately 500ms, graph approximately 10ms, temporal approximately 5ms, all under 1 second acceptable. Metadata complete with all 15+ fields populated for every chunk: chunk_type, chunk_index, pdca_id, agent_name, agent_role, date, timestamp, session_id, branch, sprint, cmm_level, task_type, objective, quality_score, verification_status, trained_in_adapter, training_batch, training_date. Sample 100 random chunks and verify 100 percent have complete metadata. Week 2 Success validated by: Exactly 37K training samples generated via RAG queries (not raw file parsing) ensuring consistency. Exactly 2K eval samples stratified across all categories and NEVER included in training for unbiased testing. Token count totals approximately 20M with average 540 tokens per sample (validate with token counter, acceptable range 19M-21M). Quality validated: Schema compliance 100 percent (all samples have required fields), diversity validated (temporal stratification across quarters, agent coverage all roles, task coverage all types), quality scores high (average over 75 out of 100). All JSONL files saved and ready: data/style_core.jsonl (12K), data/domain_patterns.jsonl (8K), data/process_framework.jsonl (5K), data/domain_representatives.jsonl (3K), data/style_refactor.jsonl (3K), data/guardrails.jsonl (2K), data/tool_awareness.jsonl (1K), data/eval.jsonl (2K). Week 3 Success validated by: LoRA adapter trained successfully with loss converged to 0.6-1.0 range (not too high indicating poor learning, not too low indicating overfitting), memory stayed under 28GB throughout training (no OOM crashes), gradient norms stable (no exploding gradients), training completed in 8-11 hours. Model merged and quantized: LoRA adapter merged with base model, quantized to Q4_K_M format, converted to GGUF, size reduced from 14GB to 4GB, quality retained at 95 percent. All quality gates passed: Pattern Compliance at least 95 percent (Ship Gate), PDCA Template at least 95 percent (Ship Gate), TRON Format at least 90 percent (Quality Gate), Empty Constructor at least 95 percent (Ship Gate), Tool Success at least 85 percent (Quality Gate), Refusal F1 at least 0.98 (Ship Gate), Overall Score at least 90 percent (Ship Gate). All 20 canary tests passed with no regressions over 5 percent. Deployed to production: Ollama serving web4-agent:latest, load time approximately 3 seconds verified, generation approximately 20 tokens per second verified, memory footprint approximately 4GB. Smoke tests passing: Trained knowledge queries answer under 200ms without RAG, historical reference queries retrieve relevant PDCAs with citations in approximately 500ms, tool queries detect tools, inject RAG examples, generate correct tool calls in approximately 2250ms. Week 4-6 Success validated by: Production stability with monitored metrics all healthy (no alerts, no crashes, uptime over 99.9 percent). Response times optimal: Pure trained under 200ms, trained plus RAG approximately 500ms, trained plus tools approximately 2250ms, weighted average approximately 2100ms meeting target. RAG hit rates validated: 10-20 percent of queries need PDCA history (not 0 percent indicating model ignores RAG, not 50 percent indicating model overly reliant), 30 percent need tool injection (validates hybrid tool architecture), 50-60 percent pure trained (validates Training-First). Evening loop running nightly: Cron job triggers at 10 PM, completes in 2-3 hours, canary tests pass, new adapter promoted, daily_buffer cleared. Model improving daily: Nightly training incorporates yesterday patterns, previously challenging queries now answered better, quality metrics maintain or improve, user feedback positive. Team trained and confident: Team members can query model, understand latency differences, provide feedback, monitor dashboards, troubleshoot common issues, perform rollbacks if needed. Documentation complete: Runbooks for evening loop troubleshooting, RAG maintenance, model rollback, system architecture documented, evaluation procedures documented, onboarding guide for new team members. FINAL SUCCESS validated by: Self-improving system operational and demonstrating continuous learning virtuous cycle - production serving generates daily work, evening loop trains patterns nightly, improved model serves next day, repeat. Model accumulates deep Web4 domain expertise over time from real project work. System is stable, monitored, documented, and the team is confident. Celebrate!" id="success">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#DCEDC8;strokeColor=#689F38;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1660" y="620" width="520" height="500" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="impl-footer" value="🎯 90/10 Framework in Action: 90% planning and data prep (Week 1-2: RAG setup + sample generation) | 10% execution (Week 3: training 8-11 hrs, eval 4 hrs, deploy 1 hr) | Week 4-6: Continuous learning establishes self-improving virtuous cycle" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E3F2FD;strokeColor=#1565C0;strokeWidth=3;align=center;verticalAlign=middle;fontSize=13;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="100" y="1180" width="2080" height="60" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
</mxfile>