<mxfile host="65bd71144e">
    <diagram name="Balanced Training Architecture" id="architecture">
        <mxGraphModel dx="2325" dy="855" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="2400" pageHeight="1800" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="title" value="Web4 Balanced LoRA Training Architecture" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="400" y="20" width="1600" height="50" as="geometry"/>
                </mxCell>
                <mxCell id="subtitle" value="Training-First Production: Train Patterns (37K samples, ~20M tokens, 95% Web4) + RAG Reference Library (History 10-20%, Tools 30%) + Swappable Tools (12K in RAG) from 534 PDCAs" style="text;html=1;strokeColor=none;fillColor=#E3F2FD;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=16;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="400" y="80" width="1600" height="35" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;🤖 BASE MODEL&lt;/font&gt;&#xa;&#xa;Qwen/Qwen2.5-Coder-7B-Instruct&#xa;(HuggingFace - Full Precision)&#xa;&#xa;For Inference: qwen2.5-coder:7b-instruct-q4_K_M" tooltip="The base model is Qwen2.5-Coder 7B Instruct, chosen for its strong code generation capabilities and 7 billion parameters optimized for coding tasks. Training uses the full precision model from HuggingFace to maximize learning quality during LoRA fine-tuning. After training, the adapter is merged with the base model and quantized to Q4_K_M format for deployment via Ollama. This quantization reduces model size from 14GB to 4GB while maintaining 95 percent quality, enabling fast inference on M1 Mac hardware with 32GB RAM and MPS backend. The model architecture includes 28 transformer layers, 4096 hidden dimensions, 32 attention heads, 32768 context window, and supports 100+ programming languages with particular strength in Python, TypeScript, JavaScript, Java, and C++. The base model already understands general programming concepts like OOP, error handling, testing, and documentation - the LoRA fine-tuning teaches it Web4-specific conventions like 5-layer architecture, Radical OOP, empty constructor pattern, scenario-based state management, Vitest testing framework, and PDCA methodology for continuous improvement." id="base-model">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1565C0;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="950" y="150" width="600" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#2E7D32&quot;&gt;🎯 LORA ADAPTER (Trained Knowledge)&lt;/font&gt;&#xa;&#xa;37,000 samples (~20M tokens)&#xa;Patterns &amp; Methodology (95% Web4, 3% tools, 2% guardrails)&#xa;r=16, alpha=32, dropout=0.05&#xa;Training: 8-11 hours on M1 Mac" tooltip="The LoRA adapter is a small trainable module (approximately 80MB, reduced from 100MB) that learns Web4-specific patterns without modifying the base model. LoRA uses rank decomposition to create two small matrices for each transformer layer, where rank r=16 means each matrix is much smaller than the original weight matrix. Training only these small matrices is 1000x faster and uses 10x less memory than full fine-tuning, enabling training on consumer hardware. The adapter contains 37,000 training samples totaling approximately 20M tokens (reduced from 46K/25M), carefully curated to teach Web4 methodology with optimal token efficiency: 95 percent Web4-specific patterns versus 74 percent in the old approach. Process Knowledge (5K samples) covering PDCA structure, TRON format, CMM1-4 framework, dual link format, and 12-step startup protocol. Code Patterns (18K samples) including empty constructor pattern, init method for scenario-based state, toScenario serialization, 5-layer architecture, and Radical OOP. Extracted PDCA Patterns (8K samples) with problem-solution pairs, debugging methodologies, architectural decisions, violation fixes, integration patterns, and collaboration patterns. Representative PDCAs (3K samples) from top 200-300 complete PDCAs selected by quality score. Generic Tool Awareness (1K samples, NEW) teaching the CONCEPT of tools with JSON structure and parameter passing, NOT specific IDE implementations. Guardrails (2K samples) for security violations and framework compliance. Training takes 8-11 hours on M1 Mac with MPS acceleration (20 percent faster due to reduced token count), monitoring loss convergence to 0.6-1.0 plateau and memory usage staying under 28GB." id="lora-adapter">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="950" y="320" width="600" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#F57F17&quot;&gt;🗄️ THREE-TIER RAG ARCHITECTURE&lt;/font&gt;&#xa;&#xa;Tier 1: ChromaDB (Semantic Search)&#xa;Tier 2: Redis Graph (Breadcrumb Navigation)&#xa;Tier 3: SQLite (Temporal Queries)&#xa;&#xa;534 PDCAs → ~2,670 chunks | All components indexed" tooltip="The three-tier RAG architecture is the cornerstone of the balanced training strategy, serving as both the data source for training sample generation and the runtime historical reference library. This hybrid design optimizes different query patterns: Tier 1 ChromaDB provides semantic search using vector embeddings, ideal for finding similar PDCAs or patterns. The 534 historical PDCAs are chunked into approximately 2,670 semantically complete chunks using PDCA-aware adaptive chunking that preserves document structure by splitting on section boundaries. Each chunk includes 15+ metadata fields covering temporal data, agent context, work context, task context, CMM compliance, and quality signals. Tier 2 Redis Graph stores breadcrumb navigation links between PDCAs, enabling fast graph traversal to implement the read-to-depth-3 principle. Graph queries are 50x faster than vector search for adjacency relationships. Tier 3 SQLite handles temporal queries efficiently, supporting fast date-range lookups and agent timeline tracking without scanning the entire vector database. This three-tier design provides single source of truth for all training data, intelligent sampling via semantic queries, natural deduplication through chunking, metadata-driven filtering, graph-aware context expansion, incremental refinement, and consistent methodology." id="rag-architecture">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="50" y="150" width="750" height="480" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;Tier 1: ChromaDB&lt;/font&gt;&#xa;Vector Embeddings&#xa;~2,670 PDCA chunks&#xa;15+ metadata fields&#xa;Semantic similarity search" tooltip="ChromaDB is an open-source vector database optimized for semantic search using embeddings. Each PDCA chunk is converted to a 768-dimensional vector using a sentence-transformer model, capturing semantic meaning beyond keyword matching. This enables queries to find relevant PDCAs even if they use different terminology. The chunks are stored with comprehensive metadata enabling filtered queries. ChromaDB uses HNSW index for fast approximate nearest neighbor search, returning results in approximately 500ms. The metadata fields enable precise filtering by chunk type, CMM level, task type, date, agent, and verification status. ChromaDB also indexes 3,477 TypeScript component files by layer and pattern, plus 238 process documents by role. During training sample generation, ChromaDB is queried thousands of times to extract patterns for the training dataset." id="chromadb">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="70" y="320" width="220" height="90" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;Tier 2: Redis Graph&lt;/font&gt;&#xa;Breadcrumb links&#xa;534 nodes, PRECEDES edges&#xa;Fast graph traversal&#xa;Read to depth 3" tooltip="Redis Graph stores PDCA breadcrumb relationships as a graph database, enabling fast traversal of prev/next links extracted from PDCA metadata. Each of the 534 PDCAs becomes a node with properties, and PRECEDES edges connect chronologically related PDCAs. Graph queries are extremely fast (approximately 10ms) compared to vector search (approximately 500ms) because they use index lookups rather than similarity computation. The primary use case is read-to-depth-3: when semantic search finds a relevant PDCA, walk the graph backward and forward up to 3 levels deep to understand the full context. This implements the Web4 principle that context matters - a single PDCA in isolation may miss important background. Redis Graph uses sparse adjacency matrices for efficient traversal and supports Cypher-like query language. During training sample generation, graph expansion enriches semantic search results to include predecessor and successor context." id="redis-graph">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="310" y="320" width="220" height="90" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;Tier 3: SQLite&lt;/font&gt;&#xa;Temporal metadata&#xa;Fast date-range queries&#xa;Agent/sprint aggregation&#xa;5ms query time" tooltip="SQLite stores temporal and categorical metadata in a relational schema optimized for fast date-range queries, agent timelines, and sprint aggregations. The pdca_timeline table contains pdca_id, timestamp, session, agent_name, agent_role, branch, sprint, cmm_level, and objective with appropriate indexes. Indexes enable sub-5ms queries which is 100x faster than scanning ChromaDB with metadata filters because SQL databases are optimized for structured queries with B-tree indexes. SQLite is also used for analytics: count PDCAs per day, identify most active agents, track CMM level distribution over time, measure sprint velocity. During training sample generation, temporal queries ensure diverse time period coverage to prevent temporal bias where the model only learns the newest patterns. SQLite is lightweight, requires no server, and integrates easily with Python. The three-tier design uses each database for its strength: ChromaDB for semantic understanding, Redis Graph for relationship traversal, SQLite for structured queries." id="sqlite">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="560" y="320" width="220" height="90" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;📚 Collections&lt;/font&gt;&#xa;&#xa;• pdca_historical (534)&#xa;• components (3,477)&#xa;• process_docs (238)&#xa;• tool_examples (12K) ★ NEW&#xa;• daily_buffer (transient)" tooltip="The RAG system organizes data into five ChromaDB collections. The pdca_historical collection contains 534 PDCAs as approximately 2,670 chunks - this is the permanent reference library with all historical PDCAs indexed with PDCA-aware adaptive chunking and comprehensive metadata. This collection never clears. The components collection indexes 3,477 TypeScript files organized by layer and pattern. The process_docs collection contains 238 documents including role-specific process documentation, CMM framework guides, PDCA templates, creation guides, decision frameworks, and compliance checklists. The tool_examples collection (NEW) stores 12,000 IDE-specific tool examples (10K Continue tools plus 2K negative examples) with metadata including tool_name, tool_ecosystem, tool_version, usage_pattern, and context_type. These tool examples are NOT trained into the LoRA adapter but are retrieved at runtime and injected into the context when the model needs to make tool calls. This enables IDE flexibility - switching from Continue to Cursor takes 5 minutes (clear Continue tools, index Cursor tools) versus 10-14 hours full retraining. The tool examples are swappable by ecosystem and support multiple IDEs simultaneously. The daily_buffer collection holds today work-in-progress and is transient, cleared nightly. During the evening training loop, daily_buffer is queried for untrained patterns, those patterns are trained into the adapter, and the buffer is cleared after moving data to permanent collections. This implements the incremental learning strategy where the model continuously improves from daily work." id="collections">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="70" y="440" width="700" height="130" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-rag-to-training" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#F57F17;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="rag-architecture" target="lora-adapter" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="800" y="400" as="sourcePoint"/>
                        <mxPoint x="850" y="350" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-label-training" value="RAG-Driven&#xa;Sample Generation&#xa;37K samples" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=12;fontStyle=1;fontColor=#F57F17;fillColor=#FFF8E1;strokeColor=#F9A825;rounded=1;" parent="arrow-rag-to-training" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="2" relative="1" as="geometry">
                        <mxPoint x="5" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#4A148C&quot;&gt;📦 TRAINING DATA BUCKETS (37K trained + 12K RAG tools)&lt;/font&gt;" tooltip="Training data is organized into 8 trained buckets plus RAG tool repository, teaching the model HOW TO CODE, HOW TO WORK, HOW TO SOLVE PROBLEMS, and WHAT NOT TO DO. The hybrid tool architecture splits tool knowledge: 1K generic tool awareness is trained (teaches CONCEPT of tools) while 12K IDE-specific examples stay in RAG (enables IDE switching without retraining). The trained buckets teach: HOW TO CODE - style_core (12K samples, 32 percent) extracts real Web4 architectural patterns from 3,477 TypeScript files including empty constructor, 5-layer architecture, Radical OOP, scenario-based state management. style_refactor (3K samples, 8 percent) shows code evolution and continuous improvement patterns. HOW TO WORK - process_framework (5K samples, 13 percent) teaches PDCA structure v3.2.4.2, TRON decision format, CMM1-4 progression, dual link format, 12-step startup protocol, verification checklists, and 50+ key behavioral lessons. HOW TO SOLVE PROBLEMS - domain_patterns (8K samples, 22 percent) extracts distilled problem-solving patterns from all 534 PDCAs including debugging methodologies, architectural decisions, integration patterns, and collaboration patterns. domain_representatives (3K samples, 8 percent) provides complete exemplary PDCAs selected by quality scoring to show end-to-end work structure. WHAT NOT TO DO - guardrails (2K samples, 5 percent) teaches security violations, Jest ban enforcement, manual operation prevention, and framework compliance. tool_awareness (1K samples, 3 percent) teaches generic tool-calling concepts with JSON structure and parameter passing, IDE-agnostic. eval (2K samples, 5 percent) is held-out test set NEVER trained. The RAG Tool Repository stores 12K IDE-specific tool examples with metadata for runtime injection, enabling 5-minute IDE switching versus 10-14 hours retraining." id="training-buckets">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#6A1B9A;strokeWidth=3;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1610" y="150" width="770" height="340" as="geometry"/>
                    </mxCell>
                </object>
                <object label="TRAINED (37K samples):&#xa;&#xa;HOW TO CODE: style_core 12K (32%) | style_refactor 3K (8%)&#xa;HOW TO WORK: process_framework 5K (13%)&#xa;HOW TO SOLVE: domain_patterns 8K (22%) | domain_representatives 3K (8%)&#xa;WHAT NOT TO DO: guardrails 2K (5%) | tool_awareness 1K (3%)&#xa;EVALUATION: eval 2K (5%) - HOLD-OUT&#xa;&#xa;RAG TOOLS (12K): tool_core 10K + tool_neg 2K (swappable)" tooltip="Clearer bucket naming that reflects what we actually teach the model. TRAINED samples (37K total, approximately 20M tokens): HOW TO CODE (15K samples, 40 percent) - style_core 12K samples teaches real Web4 architectural patterns from 3,477 TypeScript files: empty constructor pattern, 5-layer architecture, Radical OOP, scenario-based state management, init methods, toScenario serialization, component structure, and Vitest testing. style_refactor 3K samples shows code evolution patterns: CMM2 to CMM3 transformations, technical debt reduction, pattern application, refactoring journeys, and continuous improvement mindset. HOW TO WORK (5K samples, 13 percent) - process_framework 5K samples teaches the methodology: PDCA structure v3.2.4.2, TRON decision format, CMM1-4 progression and compliance, dual link format, 12-step startup protocol, verification checklists, collaboration patterns, feedback point recognition, and 50+ key behavioral lessons from trainAI. HOW TO SOLVE PROBLEMS (11K samples, 30 percent) - domain_patterns 8K samples extracts distilled problem-solving patterns from all 534 historical PDCAs: debugging methodologies, architectural decisions, violation fixes, integration patterns, collaboration patterns, and problem-solution pairs that capture Web4 domain wisdom. domain_representatives 3K samples provides complete exemplary PDCAs selected by quality scoring to show end-to-end work structure and full PDCA methodology in action. WHAT NOT TO DO (3K samples, 8 percent) - guardrails 2K samples teaches compliance: Jest ban enforcement, manual operation prevention, security violations, framework violations. tool_awareness 1K samples teaches generic tool-calling concepts: JSON structure, parameter passing, context awareness, IDE-agnostic patterns. EVALUATION (2K samples, 5 percent) - eval 2K samples is held-out test set stratified across all categories, NEVER trained, used for unbiased quality measurement. RAG TOOLS (12K samples, approximately 3.5MB): NOT trained into LoRA. Continue tools 10K plus negatives 2K stored in ChromaDB tool_examples collection with metadata. Runtime injection adds approximately 150ms latency but enables 5-minute IDE switching versus 10-14 hour retraining." id="bucket-detail">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#7B1FA2;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="1630" y="200" width="730" height="130" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#6A1B9A&quot;&gt;Token Distribution: ~20M trained + 3.5MB RAG tools&lt;/font&gt;&#xa;Avg 540 tokens/sample | 95% Web4, 3% tools, 2% guardrails&#xa;Optimized for M1 Mac (32GB RAM)&#xa;Training time: 8-11 hours (20% faster)" tooltip="The approximately 20M token budget is optimized for M1 Mac hardware with improved token efficiency. Token calculation: 37K samples times 540 average tokens per sample equals approximately 20M tokens, reduced from 25M (saving 5M tokens or 20 percent). The 540 token average accounts for short samples (100-200 tokens) for simple patterns, medium samples (400-800 tokens) for complete class implementations, and long samples (1200-1800 tokens) for full PDCA documents. This 20M token count enables faster training (8-11 hours versus 10-14 hours) while maintaining quality. Token distribution optimization: 95 percent Web4-specific patterns (versus 74 percent in old approach), 3 percent generic tool awareness (versus 22 percent for full tool training), 2 percent guardrails. This increases Web4 focus by 28 percent while maintaining tool capabilities through runtime RAG injection. The RAG tool repository stores 12K tool examples as approximately 3.5MB of text data, retrieved at runtime with approximately 150ms latency. Token efficiency strategies include PDCA patterns being distilled to save 60 percent tokens, code patterns using targeted extracts to save 40 percent tokens, representatives using smart variations to save 70 percent tokens, and tool examples staying in RAG to save 9K training samples. The 20M budget enables training sophisticated Web4 behaviors including 5-layer OOP architecture, empty constructor pattern, scenario-based state management, PDCA methodology with TRON format, CMM compliance, and framework adherence." id="token-dist">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#7B1FA2;strokeWidth=2;fontSize=11;fontStyle=0;" parent="1" vertex="1">
                        <mxGeometry x="1630" y="350" width="730" height="110" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-training" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#2E7D32;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" source="lora-adapter" target="trained-model" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1200" y="500" as="sourcePoint"/>
                        <mxPoint x="1200" y="600" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-label-lora" value="LoRA Training&lt;br&gt;8-11 hours&lt;br&gt;M1 Mac MPS" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=12;fontStyle=1;fontColor=#2E7D32;fillColor=#E8F5E9;strokeColor=#43A047;rounded=1;" parent="arrow-training" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="2" relative="1" as="geometry">
                        <mxPoint x="-62" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;🎓 TRAINED MODEL&lt;/font&gt;&#xa;&#xa;Base + LoRA Adapter&#xa;Merged &amp; Quantized (Q4_K_M)&#xa;Deployed to Ollama&#xa;web4-agent:latest" tooltip="The trained model is the final production artifact combining the base model general coding knowledge with the LoRA adapter Web4-specific patterns. Post-training process: merge LoRA adapter weights with base model weights, quantize merged model from FP16 to Q4_K_M format (4-bit with higher precision for critical attention layers), convert to GGUF format for optimized inference, create Ollama modelfile, and import to Ollama. The trained model capabilities include pattern recognition, code generation, PDCA creation, refactoring, guardrails, and collaboration. The 4GB quantized model loads in approximately 3 seconds on M1 Mac, generates at approximately 20 tokens per second, and achieves 90 percent accuracy on evaluation set metrics including pattern recognition 95 percent, PDCA template 95 percent, TRON format 90 percent, empty constructor 95 percent, CMM understanding 90 percent, historical retrieval 85 percent, refusal accuracy 98 percent, and overall score 90 percent." id="trained-model">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E3F2FD;strokeColor=#1565C0;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="950" y="570" width="600" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-runtime-rag" value="" style="endArrow=classic;startArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#FF6F00;exitX=0;exitY=0.75;exitDx=0;exitDy=0;entryX=1;entryY=0.75;entryDx=0;entryDy=0;dashed=1;" parent="1" source="trained-model" target="rag-architecture" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="700" y="650" as="sourcePoint"/>
                        <mxPoint x="750" y="600" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-label-runtime" value="Runtime Queries&#xa;(10-20% PDCA history)&#xa;(30% tool injection)" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#FF6F00;fillColor=#FFF3E0;strokeColor=#FB8C00;rounded=1;" parent="arrow-runtime-rag" vertex="1" connectable="0">
                    <mxGeometry x="-0.05" y="1" relative="1" as="geometry">
                        <mxPoint y="-30" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#D84315&quot;&gt;🌙 EVENING TRAINING LOOP&lt;/font&gt;&#xa;&#xa;Daily Buffer → Query Untrained → Generate Samples&#xa;→ Incremental Training (1 epoch) → Mark as Trained&#xa;→ Move to Historical → Clear Buffer&#xa;&#xa;Scheduled: 10 PM daily | Duration: 2-3 hours" tooltip="The evening training loop implements continuous learning, automatically incorporating each day work into the model every night at 10 PM. This creates a virtuous cycle where the model improves daily from real project work. The 7-step nightly process: Daily Buffer Collection throughout the day with new PDCAs indexed into daily_buffer with metadata. Query Untrained Patterns at 10 PM to identify what is new since yesterday. Generate Incremental Samples extracting patterns from today work with quality scoring. Incremental LoRA Training for these new samples for 1 epoch with adjusted hyperparameters, taking 2-3 hours for typical 50-sample daily batch. Mark as Trained updating RAG metadata for all trained chunks. Move to Historical with PDCAs from daily_buffer moved to pdca_historical collection and Redis Graph updated with new PRECEDES edges. Clear Daily Buffer with the collection archived and system reset for tomorrow. The evening loop advantages include continuous improvement, pattern discovery, adaptation, efficiency, and metadata-driven smart querying. Evening loop monitoring includes daily logs tracking sample count, training loss, memory usage, and completion time with alerting for issues." id="evening-loop">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="50" y="690" width="750" height="210" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1B5E20&quot;&gt;🚀 PRODUCTION DEPLOYMENT&lt;/font&gt;&#xa;&#xa;Ollama Model: web4-agent:latest&#xa;Fast Inference: ~20 tokens/sec on M1&#xa;Memory: ~4GB loaded&#xa;Response: 80-90% from training, 10-20% with RAG&#xa;Latency: under 200ms (trained), ~500ms (with RAG)" tooltip="Production deployment architecture optimizes for fast, reliable inference while maintaining access to historical context when needed. Ollama Integration provides REST API for LLM queries, chat interface for interactive sessions, and embedding endpoint for RAG similarity search. Ollama handles model lifecycle, request batching, and response streaming. Performance Characteristics include fast inference at approximately 20 tokens per second on M1 Mac, low memory footprint of approximately 4GB loaded, quick cold start of approximately 3 seconds to load model, and sub-200ms response latency for trained knowledge queries not requiring RAG. Decision Logic for RAG: the model first attempts to answer from trained knowledge (80-90 percent of queries), and for queries requiring specific historical context, the model queries RAG (10-20 percent of queries). RAG augmentation adds approximately 300ms latency but provides accurate historical reference. Hybrid Response Generation retrieves 3-5 relevant chunks, formats retrieved context, generates response incorporating both trained knowledge and retrieved facts, and includes source citations for traceability. Monitoring and Observability tracks response time metrics, RAG hit rate, quality metrics, and user feedback. Error handling gracefully degrades if RAG is unavailable, times out slow RAG queries, caches frequently accessed PDCAs, and logs all errors for analysis." id="production">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1610" y="550" width="770" height="210" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#01579B&quot;&gt;📊 KEY SUCCESS METRICS&lt;/font&gt;&#xa;&#xa;Training: Loss 0.6-1.0 | Memory under 28GB | 37K samples | ~20M tokens | 8-11 hrs&#xa;Quality: Pattern Recognition ≥95% | PDCA Template ≥95% | Framework Compliance ≥95%&#xa;Production: Response under 200ms | PDCA Queries 10-20% | Tool Queries 30% | Compilation ≥90% | Refusal ≥98%" tooltip="Key Success Metrics define measurable targets across three phases with hybrid tool architecture. Training Success includes loss convergence to 0.6-1.0 range indicating good learning, memory usage staying under 28GB ensuring stable training, successfully training 37K samples (down from 46K) in 8-11 hours (20 percent faster) validating the optimized token budget, and gradient norms staying stable confirming proper learning. Quality Success includes Pattern Recognition at least 95 percent measuring whether the model correctly identifies when to apply Web4 patterns, PDCA Template at least 95 percent evaluating generated PDCAs for completeness and compliance, Framework Compliance at least 95 percent checking generated code for proper architecture and conventions, Empty Constructor at least 95 percent for pattern adherence, CMM Understanding at least 90 percent for framework knowledge, Historical Retrieval at least 85 percent for RAG integration, and Refusal Accuracy at least 98 percent for guardrail effectiveness. Production Success includes response latency under 200ms for trained knowledge queries (no RAG), PDCA History Queries 10-20 percent of total requests validating historical reference usage, Tool Queries 30 percent of requests requiring tool example injection from RAG, Tool Injection Latency approximately 150ms additional for tool queries, Compilation Success at least 90 percent measuring whether generated code compiles on first attempt, and IDE Switching Time 5 minutes to swap Continue for Cursor versus 10-14 hours retraining. These metrics are continuously monitored via automated evaluation pipeline with alerting if any metric drops below threshold." id="metrics">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FE;strokeColor=#0277BD;strokeWidth=3;fontSize=12;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="50" y="950" width="2300" height="110" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="footer" value="🎯 Training-First Production: Model uses trained patterns FIRST (80-90% queries, ~2000ms) | RAG for historical reference (10-20% queries, +300ms) and tool syntax (30% queries, +150ms) | 5-min IDE switching vs 10-14hr retraining" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;strokeWidth=3;align=center;verticalAlign=middle;fontSize=13;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="50" y="1090" width="2300" height="60" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
    <diagram name="RAG-First Training Pipeline" id="pipeline-flow">
        <mxGraphModel dx="1603" dy="855" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="2400" pageHeight="1800" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="flow-title" value="Web4 Training Data Generation Pipeline (Training Preparation Phase)" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="400" y="20" width="1600" height="50" as="geometry"/>
                </mxCell>
                <mxCell id="flow-subtitle" value="RAG-Driven Sample Generation: Use RAG to Generate Training Data (Week 1-2) → Train LoRA (8-11 hrs) → Deploy → Production is Training-First with RAG Reference (10-20% history, 30% tools)" style="text;html=1;strokeColor=none;fillColor=#E3F2FD;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=16;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="200" y="80" width="2000" height="35" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#01579B&quot;&gt;PHASE 0: BOOTSTRAP RAG&lt;/font&gt;&#xa;&#xa;Day 1 | Duration: ~1 hour&#xa;&#xa;Install: ChromaDB + Redis Graph + SQLite&#xa;Index: 534 PDCAs → ~2,670 chunks&#xa;Index: 3,477 TypeScript files&#xa;Index: 238 process docs&#xa;Index: 12K tool examples&#xa;&#xa;Result: Complete RAG data store" tooltip="Phase 0 bootstraps the three-tier RAG system which serves as the single source of truth for all training data. Day 1 setup takes approximately 1 hour. Install dependencies: ChromaDB for semantic search, Redis server with RedisGraph module for breadcrumb navigation, SQLite for temporal queries. Run initial indexing scripts: all 534 historical PDCAs are processed with PDCA-aware adaptive chunking creating approximately 2,670 semantically complete chunks with 15+ metadata fields per chunk. The 3,477 TypeScript component files are indexed by layer and pattern. The 238 process documents including PDCA templates, CMM guides, and compliance checklists are indexed by role. The 12K tool examples from tool_core.jsonl and tool_neg.jsonl are indexed into the tool_examples collection with metadata for tool_name, tool_ecosystem, tool_version, usage_pattern, and context_type. Verify three-tier indexing: test semantic queries on ChromaDB, test breadcrumb traversal on Redis Graph, test temporal queries on SQLite. Test retrieval across all three tiers with sample queries. Result: Complete RAG data store ready for intelligent sample generation. This phase is critical because RAG becomes the single source of truth - all subsequent training samples are generated via queries against this RAG system, ensuring consistency and enabling metadata-driven sampling." id="phase0">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FE;strokeColor=#01579B;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="150" width="360" height="220" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#F57F17&quot;&gt;PHASE 1: RAG-DRIVEN SAMPLE GENERATION&lt;/font&gt;&#xa;&#xa;Week 1-2 | Duration: ~10 days&#xa;&#xa;Query RAG for patterns (semantic + graph + temporal)&#xa;Generate 37K training samples:&#xa;• style_core: 12K&#xa;• domain_patterns: 8K&#xa;• process_framework: 5K&#xa;• domain_representatives: 3K&#xa;• style_refactor: 3K&#xa;• guardrails: 2K&#xa;• eval: 2K&#xa;• tool_awareness: 1K&#xa;&#xa;Save to JSONL files (~20M tokens)&#xa;&#xa;Result: Complete training dataset" tooltip="Phase 1 generates all 37K training samples via intelligent RAG queries over 10 days. This is the core innovation: RAG is not just for runtime retrieval but also the source for training sample generation. The process uses semantic queries to extract patterns, graph expansion to include context, and temporal filtering to ensure diversity. Sample generation per bucket: style_core 12K samples query ChromaDB for TypeScript files filtered by layer and pattern, extracting empty constructor examples, 5-layer architecture, Radical OOP, scenario-based state management. domain_patterns 8K samples query historical PDCAs semantically for problem-solution pairs, then use Redis Graph to walk breadcrumb chains for context, extracting distilled patterns. process_framework 5K samples extract PDCA structure, TRON format, CMM compliance from process_docs collection. domain_representatives 3K samples select top 200-300 PDCAs by quality score ensuring diverse time periods via SQLite temporal queries. style_refactor 3K samples query for CMM2 to CMM3 transformation PDCAs. guardrails 2K samples extract from violation reports. eval 2K samples stratify across all categories, NEVER trained. tool_awareness 1K samples curate generic tool concepts from tool_core.jsonl. Each sample includes input prompt, expected output, and metadata. Samples are saved to JSONL files totaling approximately 20M tokens. Quality checks validate schema compliance, Web4 pattern adherence, token distribution. Result: Production-ready training dataset generated entirely from RAG queries, ensuring consistency and traceability." id="phase1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="540" y="150" width="360" height="370" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#2E7D32&quot;&gt;PHASE 2: LORA TRAINING&lt;/font&gt;&#xa;&#xa;Week 3 | Duration: 8-11 hours&#xa;&#xa;Base: Qwen2.5-Coder-7B-Instruct (HuggingFace)&#xa;Train: 37K samples, 2 epochs&#xa;Config: r=16, alpha=32, dropout=0.05&#xa;Batch: 1 with grad accumulation 12&#xa;LR: 2e-4 with cosine schedule&#xa;Hardware: M1 Mac (32GB), MPS backend&#xa;&#xa;Monitor: Loss plateau 0.6-1.0&#xa;Monitor: Memory under 28GB&#xa;Monitor: Gradient norms stable&#xa;&#xa;Output: LoRA adapter (~80MB)&#xa;&#xa;Result: Trained Web4-specific adapter" tooltip="Phase 2 performs LoRA fine-tuning on the 37K samples generated from RAG queries. Training takes 8-11 hours on M1 Mac with 32GB RAM using MPS Metal Performance Shaders backend. Base model: Qwen/Qwen2.5-Coder-7B-Instruct from HuggingFace, chosen for strong code generation capabilities and 7 billion parameters optimized for coding. Training configuration: 37K samples trained for 2 full epochs totaling approximately 20M tokens. LoRA hyperparameters: rank r=16 creates small trainable matrices for efficient fine-tuning, alpha=32 for scaling, dropout=0.05 for regularization. Batch size 1 with gradient accumulation 12 gives effective batch size 12, enabling stable gradients while fitting in 32GB RAM. Learning rate 2e-4 with cosine annealing schedule gradually reduces learning rate for smooth convergence. The training pipeline loads JSONL files, tokenizes with the base model tokenizer, applies LoRA to attention and feedforward layers, and trains using AdamW optimizer. Real-time monitoring tracks loss convergence expecting plateau at 0.6-1.0 range indicating good learning without overfitting, memory usage must stay under 28GB to prevent OOM crashes, gradient norms should remain stable confirming proper learning dynamics. Training output: LoRA adapter approximately 80MB containing learned Web4-specific patterns without modifying the 14GB base model. The adapter encodes: 95 percent Web4-specific patterns including PDCA methodology, code architecture, OOP principles, 3 percent generic tool awareness, 2 percent guardrails. Result: Production-ready LoRA adapter ready for merging and quantization." id="phase2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="980" y="150" width="360" height="370" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;PHASE 3: MERGE AND QUANTIZE&lt;/font&gt;&#xa;&#xa;Week 3 | Duration: ~2 hours&#xa;&#xa;Merge: LoRA adapter + Base model&#xa;Quantize: FP16 → Q4_K_M (4-bit)&#xa;Convert: GGUF format&#xa;Size: 14GB → 4GB (4x smaller)&#xa;Quality: 95% retained&#xa;&#xa;Create: Ollama modelfile&#xa;Import: web4-agent:latest&#xa;&#xa;Test: Load time ~3 seconds&#xa;Test: Generation ~20 tokens/sec&#xa;&#xa;Result: Deployable 4GB GGUF model" tooltip="Phase 3 merges the trained LoRA adapter with the base model and quantizes for efficient deployment. Duration approximately 2 hours. Merge process: The 80MB LoRA adapter weights are merged into the 14GB base model weights creating a unified model with Web4-specific knowledge permanently integrated. The merged model combines general coding knowledge from Qwen2.5-Coder with Web4-specific patterns from LoRA training. Quantization: Convert merged model from FP16 full precision to Q4_K_M 4-bit quantization. Q4_K_M uses 4-bit integers for most weights while keeping higher precision for critical attention layers, achieving optimal balance between size and quality. Size reduction: 14GB FP16 model compresses to 4GB Q4_K_M, a 4x reduction enabling deployment on consumer hardware. Quality retention: Quantization maintains 95 percent of full precision quality, validated through evaluation metrics. Convert to GGUF format: GGUF is an efficient file format for LLM storage optimized for CPU and Metal GPU inference, used by Ollama. Create Ollama modelfile: Define model configuration, system prompt, temperature, context window, and other parameters. Import to Ollama: Register the quantized GGUF model as web4-agent:latest in the local Ollama model registry. Test deployment: Verify load time approximately 3 seconds on M1 Mac cold start, generation speed approximately 20 tokens per second, memory footprint approximately 4GB loaded. Result: Production-ready 4GB GGUF model deployed to Ollama, ready for local inference with fast performance and low memory footprint." id="phase3">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1565C0;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1450" y="160" width="360" height="360" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#6A1B9A&quot;&gt;PHASE 4: EVALUATION AND QUALITY GATES&lt;/font&gt;&#xa;&#xa;Week 3 | Duration: ~4 hours&#xa;&#xa;Run: 2K eval samples (hold-out set)&#xa;&#xa;Test Harnesses:&#xa;✓ Pattern Compliance ≥95% (schema validator)&#xa;✓ PDCA Template ≥95% (section regex)&#xa;✓ TRON Format ≥90% (structure detector)&#xa;✓ Empty Constructor ≥95% (ESLint + AST)&#xa;✓ Tool Success ≥85% (100 scripted tasks)&#xa;✓ Refusal F1 ≥0.98 (200-item safety set)&#xa;✓ Overall ≥90%&#xa;&#xa;Canary Tests: 20 must-not-regress tasks&#xa;&#xa;Pass? → Deploy | Fail? → Rollback&#xa;&#xa;Result: Quality-validated model" tooltip="Phase 4 runs comprehensive evaluation to validate model quality before production deployment. Duration approximately 4 hours. Evaluation process: Run the 2K eval samples that were held out during training, ensuring unbiased quality measurement across all training categories. Test Harness 1 Pattern Compliance: Schema validator plus AST checker tests 100 generated PDCAs against v3.2.4.2 schema, must pass 95 out of 100. Test Harness 2 PDCA Template: Section regex plus metadata validator checks all required sections Links Plan Do Check Act Meta, must pass 95 out of 100. Test Harness 3 TRON Format: Structure detector validates Trigger Response Outcome Next ordering in decisions, must pass 90 out of 100. Test Harness 4 Empty Constructor: ESLint with Web4 rules plus AST parser checks no-constructor-logic rule on 100 generated classes, must pass 95 out of 100. Test Harness 5 Tool Success: 100 scripted IDE tasks measured end-to-end from prompt to correct tool JSON to successful execution in sandbox, must pass 85 out of 100. Test Harness 6 Refusal Accuracy: F1 score on 200-item curated safety set with 100 should-refuse and 100 should-comply samples, must achieve F1 at least 0.98. Overall Score: Weighted average of all metrics must be at least 90 percent. Canary Tests: Run 20 critical must-not-regress tasks comparing new model against baseline, fail if any regression over 5 percent. Gate Decision: If all Ship Gates pass (Pattern, PDCA, Empty Constructor, Refusal, Overall), proceed to production deployment. If any gate fails, halt deployment, rollback to last-known-good adapter, create incident PDCA documenting failure mode, investigate root cause, fix and retry. Result: Quality-validated model ready for production with documented test results." id="phase4">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#6A1B9A;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1900" y="130" width="380" height="420" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#558B2F&quot;&gt;PHASE 5: PRODUCTION DEPLOYMENT&lt;/font&gt;&#xa;&#xa;Week 3 | Duration: ~1 hour&#xa;&#xa;Deploy: web4-agent:latest to Ollama&#xa;Connect: RAG system for historical queries&#xa;Configure: ToolAwarePromptBuilder&#xa;Start: Ollama server&#xa;&#xa;Response Logic:&#xa;• 80-90% queries: From training (under 200ms)&#xa;• 10-20% queries: With RAG PDCA history (+300ms)&#xa;• 30% queries: With RAG tool injection (+150ms)&#xa;&#xa;Monitor: Response time, RAG hit rate, quality&#xa;&#xa;Result: Production-ready system" tooltip="Phase 5 deploys the validated model to production with full RAG integration. Duration approximately 1 hour. Deployment steps: Deploy web4-agent:latest GGUF model to Ollama model registry. Connect RAG system for historical reference: ChromaDB for semantic search, Redis Graph for breadcrumb navigation, SQLite for temporal queries. Configure ToolAwarePromptBuilder to inject relevant tool examples from the 12K tool_examples RAG collection at runtime. Start Ollama server with REST API for LLM queries, chat interface for interactive sessions, and embedding endpoint for RAG similarity. Response logic: The model first attempts to answer from trained knowledge covering 80-90 percent of queries with response latency under 200ms. For queries requiring specific historical context like how did we solve X before or what did we work on date Y, the model queries RAG adding approximately 300ms latency but providing accurate historical reference with source citations. For queries requiring tool usage like read this file or run this command, ToolAwarePromptBuilder detects tool need, queries RAG tool_examples collection for 2-3 relevant examples, injects examples into context adding approximately 150ms latency, and model generates correct tool call following RAG-provided patterns. Monitoring: Track response time metrics across query types, RAG hit rate to validate 10-20 percent PDCA queries and 30 percent tool queries, quality metrics via user feedback and automated checks. Error handling: Gracefully degrade if RAG unavailable, timeout slow RAG queries after 1 second, cache frequently accessed PDCAs for speed. Result: Production-ready system combining fast inference from trained knowledge with accurate historical reference and flexible tool usage." id="phase5">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#DCEDC8;strokeColor=#689F38;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="680" width="480" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#D84315&quot;&gt;PHASE 6: EVENING TRAINING LOOP&lt;/font&gt;&#xa;&#xa;Nightly | Scheduled: 10 PM | Duration: 2-3 hours&#xa;&#xa;Step 1: Query daily_buffer for untrained patterns&#xa;Step 2: Quality scoring and pattern extraction&#xa;Step 3: Generate incremental samples (50-200)&#xa;Step 4: Incremental LoRA training (1 epoch)&#xa;Step 5: Canary test (validate no regressions)&#xa;Step 6: Mark as trained in RAG metadata&#xa;Step 7: Move PDCAs to pdca_historical&#xa;Step 8: Clear daily_buffer, archive logs&#xa;&#xa;Monitoring: Loss, memory, sample count, completion&#xa;Rollback: Keep last-5 adapters, auto-rollback on failure&#xa;&#xa;Result: Continuous daily improvement" tooltip="Phase 6 implements the evening training loop for continuous learning, running every night at 10 PM for 2-3 hours. This creates a virtuous cycle where the model improves daily from real project work. Step 1 Query Untrained: At 10 PM trigger, query daily_buffer collection for PDCAs and patterns added today. Filter by metadata trained_in_adapter equals False to identify new content. Typical daily yield: 50-200 new samples depending on activity. Step 2 Quality Scoring: Apply quality scoring to select high-value samples. Extract patterns from today work: new problem-solution pairs, refactoring journeys, architectural decisions. Step 3 Generate Samples: Create incremental training samples in JSONL format with input prompt, expected output, and metadata. Samples follow same schema as initial training for consistency. Step 4 Incremental Training: Train LoRA adapter on incremental samples for 1 epoch only with reduced learning rate 1e-4 to avoid catastrophic forgetting. Training takes 2-3 hours for typical 50-sample batch. Step 5 Canary Test: Before promoting new adapter, run 20 must-not-regress tasks comparing new adapter against baseline. Fail if any regression over 5 percent. Step 6 Mark as Trained: Update RAG metadata setting trained_in_adapter equals True, training_batch equals nightly_YYYYMMDD, training_date equals timestamp for all trained chunks. Step 7 Move to Historical: Move PDCAs from daily_buffer to pdca_historical collection. Update Redis Graph with new PRECEDES edges for breadcrumb navigation. Step 8 Clear and Archive: Archive daily_buffer to logs, clear collection, reset for tomorrow. Monitoring: Daily logs track sample count, training loss, memory usage, completion time. Alert on failures or anomalies. Rollback: Keep last-5 nightly adapters. If canary fails, auto-rollback to last-known-good adapter and create incident PDCA. Result: Model continuously improves from daily work while maintaining quality through canary tests and rollback protection." id="phase6">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="790" y="680" width="520" height="420" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#4A148C&quot;&gt;CONTINUOUS OPERATION&lt;/font&gt;&#xa;&#xa;Daily Workflow:&#xa;&#xa;09:00 - 22:00: Production serving&#xa;  • Answer user queries&#xa;  • Generate code and PDCAs&#xa;  • New work indexed to daily_buffer&#xa;&#xa;22:00 - 01:00: Evening training&#xa;  • Train today patterns&#xa;  • Update model&#xa;  • Quality gates&#xa;&#xa;01:00 - 09:00: Production serving&#xa;  • Improved model in production&#xa;  • New patterns available&#xa;&#xa;Result: Self-improving system" tooltip="Continuous operation shows the daily rhythm of the Web4 training system. During daytime 09:00 to 22:00, the production model serves user queries, generates code following Web4 patterns, creates PDCAs with proper structure, and provides historical context via RAG when needed. All new work created during the day including PDCAs, code, decisions, and learnings are automatically indexed into the daily_buffer RAG collection with metadata. At night 22:00 to 01:00, the evening training loop activates: query daily_buffer for untrained patterns, extract and score patterns, generate incremental training samples typically 50-200 samples, train LoRA adapter for 1 epoch with reduced learning rate, run canary tests to validate no regressions, mark trained data in RAG metadata, move PDCAs to historical collection, clear daily_buffer. If all quality gates pass, the improved adapter is promoted to production. If canary fails, rollback to previous adapter and create incident PDCA. From 01:00 to 09:00 next morning, the improved model is in production with yesterday patterns now trained in. Users benefit from model that learned from yesterday work. This cycle repeats daily creating a self-improving system. Benefits: Continuous improvement from real project work, pattern discovery from daily activities, adaptation to evolving practices, efficient incremental learning without full retraining, metadata-driven sample selection ensures quality. The system gets smarter every day while maintaining production stability through canary tests and rollback protection. Over time, the model accumulates deep Web4 domain expertise from hundreds of days of project work." id="continuous">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#7B1FA2;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1570" y="780" width="420" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-phase0-phase1" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#01579B;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase0" target="phase1" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="460" y="260" as="sourcePoint"/>
                        <mxPoint x="520" y="260" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase0-1" value="RAG Ready" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#01579B;fillColor=#E1F5FE;strokeColor=#0288D1;rounded=1;" parent="arrow-phase0-phase1" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase1-phase2" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#F57F17;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase1" target="phase2" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="880" y="300" as="sourcePoint"/>
                        <mxPoint x="940" y="300" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase1-2" value="37K Samples&#xa;~20M Tokens" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#F57F17;fillColor=#FFF9C4;strokeColor=#F9A825;rounded=1;" parent="arrow-phase1-phase2" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase2-phase3" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#2E7D32;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase2" target="phase3" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1300" y="340" as="sourcePoint"/>
                        <mxPoint x="1360" y="340" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase2-3" value="LoRA Adapter&#xa;80MB" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#2E7D32;fillColor=#C8E6C9;strokeColor=#43A047;rounded=1;" parent="arrow-phase2-phase3" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase3-phase4" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#1565C0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase3" target="phase4" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1720" y="300" as="sourcePoint"/>
                        <mxPoint x="1780" y="300" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase3-4" value="GGUF Model&#xa;4GB" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#1565C0;fillColor=#BBDEFB;strokeColor=#1976D2;rounded=1;" parent="arrow-phase3-phase4" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase4-phase5" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#6A1B9A;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.454;entryY=0.006;entryDx=0;entryDy=0;entryPerimeter=0;" parent="1" source="phase4" target="phase5" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1780" y="570" as="sourcePoint"/>
                        <mxPoint x="580" y="500" as="targetPoint"/>
                        <Array as="points">
                            <mxPoint x="2090" y="620"/>
                            <mxPoint x="1780" y="620"/>
                            <mxPoint x="1200" y="620"/>
                            <mxPoint x="320" y="620"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase4-5" value="Quality Validated&#xa;All Gates Pass ✓" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#6A1B9A;fillColor=#F3E5F5;strokeColor=#8E24AA;rounded=1;" parent="arrow-phase4-phase5" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint x="320" y="-10" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase5-phase6" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#558B2F;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase5" target="phase6" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="580" y="840" as="sourcePoint"/>
                        <mxPoint x="640" y="840" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase5-6" value="Daily Work&#xa;→ daily_buffer" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#558B2F;fillColor=#DCEDC8;strokeColor=#689F38;rounded=1;" parent="arrow-phase5-phase6" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-phase6-continuous" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#D84315;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="phase6" target="continuous" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1160" y="710" as="sourcePoint"/>
                        <mxPoint x="1220" y="760" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-phase6-cont" value="Improved Model&#xa;Every Night" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#D84315;fillColor=#FFCCBC;strokeColor=#FF5722;rounded=1;" parent="arrow-phase6-continuous" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-continuous-loop" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#7B1FA2;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0.456;entryY=0.996;entryDx=0;entryDy=0;dashed=1;entryPerimeter=0;" parent="1" source="continuous" target="phase5" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1640" y="940" as="sourcePoint"/>
                        <mxPoint x="1700" y="940" as="targetPoint"/>
                        <Array as="points">
                            <mxPoint x="2080" y="940"/>
                            <mxPoint x="2080" y="1150"/>
                            <mxPoint x="319" y="1160"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-loop" value="Continuous&#xa;Daily Cycle" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#7B1FA2;fillColor=#E1BEE7;strokeColor=#8E24AA;rounded=1;" parent="arrow-continuous-loop" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint x="15" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#BF360C&quot;&gt;⚠️ FAILURE HANDLING&lt;/font&gt;&#xa;&#xa;Quality Gate Failure (Phase 4):&#xa;• Halt deployment immediately&#xa;• Keep current production model&#xa;• Rollback to last-known-good adapter&#xa;• Create incident PDCA&#xa;• Root cause analysis&#xa;• Fix issues and retry&#xa;&#xa;Canary Test Failure (Phase 6):&#xa;• Auto-rollback to previous adapter&#xa;• Create incident PDCA&#xa;• Alert on-call&#xa;• Investigate training data quality&#xa;• Skip tonight update, retry tomorrow" tooltip="Failure handling ensures production stability when quality gates or canary tests fail. Quality Gate Failure in Phase 4: If any ship gate fails during evaluation Pattern Compliance under 95 percent, PDCA Template under 95 percent, Refusal F1 under 0.98, or Overall under 90 percent, immediately halt deployment. Keep current production model serving traffic. Rollback training to last-known-good adapter saved from previous successful training. Create incident PDCA documenting which gate failed, by how much, sample failures, and initial observations. Conduct root cause analysis: inspect training data quality, review hyperparameters, check for data distribution shifts, validate evaluation harness correctness. Fix identified issues: curate better training samples, adjust hyperparameters, fix bugs in data pipeline. Retry training after fixes applied. Do not deploy to production until all gates pass. Canary Test Failure in Phase 6: If nightly canary test detects regression over 5 percent on any of 20 must-not-regress tasks, automatically rollback to previous nightly adapter without human intervention. Create incident PDCA documenting which canary task regressed, baseline score, new score, and regression magnitude. Alert on-call engineer via PagerDuty or Slack for investigation. Investigate training data quality from today daily_buffer: were there low-quality samples, outliers, or distribution shifts. Skip tonight evening loop update, keeping yesterday model in production. Retry tomorrow night after data quality issues addressed. The system maintains last-5 nightly adapters enabling rollback to any recent version. Failure handling philosophy: Fail closed, never deploy broken model. Automate rollback for speed. Document failures for learning. Investigate root causes systematically. Production stability over feature velocity." id="failure">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFEBEE;strokeColor=#C62828;strokeWidth=3;fontSize=12;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1700" y="1230" width="460" height="300" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1B5E20&quot;&gt;📊 TIMELINE SUMMARY&lt;/font&gt;&#xa;&#xa;Day 1: Bootstrap RAG (1 hour)&#xa;Week 1-2: Generate 37K samples from RAG queries (10 days)&#xa;Week 3: Train LoRA (8-11 hours) → Merge and Quantize (2 hours) → Evaluate (4 hours) → Deploy (1 hour)&#xa;Week 3+: Production serving + Nightly improvements (continuous)&#xa;&#xa;Total Initial: ~3 weeks from zero to production&#xa;Continuous: Daily 2-3 hour training overnight, improved model every morning&#xa;&#xa;Key Innovation: RAG is both training data source AND runtime reference library" tooltip="Timeline summary shows the complete journey from zero to production in approximately 3 weeks. Day 1 Bootstrap: 1 hour to install ChromaDB, Redis Graph, SQLite and index all 534 PDCAs, 3,477 TypeScript files, 238 process docs, and 12K tool examples. This creates the three-tier RAG system as single source of truth. Week 1-2 Sample Generation: 10 days to generate all 37K training samples via intelligent RAG queries. Use semantic search, graph expansion, and temporal filtering to extract patterns, select representatives, and ensure diversity. Save to JSONL files totaling approximately 20M tokens. Week 3 Training: 8-11 hours to train LoRA adapter on 37K samples using M1 Mac with MPS backend. Monitor loss convergence, memory usage, gradient stability. Output: 80MB LoRA adapter. Week 3 Post-Processing: 2 hours to merge adapter with base model and quantize from FP16 to Q4_K_M 4GB GGUF. 4 hours to run comprehensive evaluation with 2K hold-out samples across 6 test harnesses and 20 canary tasks. 1 hour to deploy validated model to Ollama and configure RAG integration. Week 3+ Continuous Operation: Production serving during daytime with model answering queries, generating code, creating PDCAs. Daily work indexed to daily_buffer. Every night at 10 PM, evening training loop activates: extract patterns from daily_buffer, train incrementally for 1 epoch taking 2-3 hours, validate with canary tests, promote if passed. Improved model in production next morning. This cycle repeats indefinitely creating self-improving system. Key Innovation: RAG serves dual purpose as training data source during initial sample generation AND runtime reference library for historical queries and tool injection. This ensures consistency between training and deployment. Total timeline: 3 weeks initial setup, then continuous daily improvements forever." id="timeline">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E8F5E9;strokeColor=#2E7D32;strokeWidth=3;fontSize=12;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="1230" width="840" height="240" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#004D40&quot;&gt;💡 KEY BENEFITS&lt;/font&gt;&#xa;&#xa;✓ Single Source of Truth: RAG for both training and runtime&#xa;✓ Metadata-Driven: Intelligent sampling via semantic + graph + temporal&#xa;✓ Quality Assurance: 6 test harnesses + 20 canary tasks&#xa;✓ Continuous Learning: Nightly improvements from daily work&#xa;✓ Production Stability: Canary tests + auto-rollback&#xa;✓ Token Efficiency: 37K samples, 20M tokens, 95% Web4-focused&#xa;✓ Fast Training: 8-11 hours (20% faster than 46K/25M approach)&#xa;✓ Hybrid Tools: 1K trained + 12K RAG for IDE flexibility&#xa;✓ Self-Improving: Gets smarter every day, learns from real work" tooltip="Key benefits of the RAG-first training pipeline. Single Source of Truth: RAG serves as the authoritative data source for both initial training sample generation and runtime historical queries, ensuring consistency and traceability. All 37K training samples are generated via RAG queries, not from raw files. Metadata-Driven Sampling: Intelligent sample generation combines semantic search to find patterns, graph expansion to include context, and temporal filtering to ensure diversity. Metadata fields enable precise filtering by CMM level, task type, agent, date, and quality score. Quality Assurance: Comprehensive evaluation with 6 automated test harnesses Pattern Compliance, PDCA Template, TRON Format, Empty Constructor, Tool Success, Refusal F1 plus 20 canary tasks for must-not-regress validation. Binary pass fail gates prevent broken models from reaching production. Continuous Learning: The evening training loop runs nightly extracting patterns from today daily_buffer, training incrementally for 1 epoch, and promoting improved adapter to production every morning. Model gets smarter from real project work. Production Stability: Canary tests validate no regressions before promoting nightly adapters. Auto-rollback on failure keeps production stable. Keep last-5 adapters for safety. Token Efficiency: Optimized 37K samples with 20M tokens (reduced from 46K/25M) achieves 95 percent Web4-specific focus versus 74 percent in old approach. Distillation and RAG storage save 5M tokens. Fast Training: 8-11 hours full training on M1 Mac is 20 percent faster than previous 10-14 hours, enabled by reduced token count and optimized sampling. Hybrid Tool Architecture: Train 1K generic tool awareness, store 12K IDE-specific examples in RAG. Switch IDEs in 5 minutes versus 10-14 hours retraining. Supports Continue, Cursor, and custom tools simultaneously. Self-Improving System: The model accumulates deep Web4 domain expertise from hundreds of days of project work, learning from successes and failures, adapting to evolving practices, discovering new patterns organically." id="benefits">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E0F2F1;strokeColor=#00695C;strokeWidth=3;fontSize=12;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1000" y="1230" width="640" height="300" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="flow-footer" value="📝 Note: This diagram shows TRAINING PREPARATION (Week 1-2). Production architecture is Training-First: Model uses trained patterns (80-90%), RAG for history (10-20%), RAG for tool syntax (30%). See Knowledge Hierarchy section." style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E3F2FD;strokeColor=#1565C0;strokeWidth=3;align=center;verticalAlign=middle;fontSize=13;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="100" y="1590" width="2060" height="60" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
    <diagram name="Three-Tier RAG Architecture" id="rag-deep-dive">
        <mxGraphModel dx="1603" dy="855" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="2400" pageHeight="1800" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="rag-title" value="Web4 Three-Tier RAG Architecture Deep Dive" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="400" y="20" width="1600" height="50" as="geometry"/>
                </mxCell>
                <mxCell id="rag-subtitle" value="Hybrid Architecture: ChromaDB (Semantic) + Redis Graph (Breadcrumb) + SQLite (Temporal) | 534 PDCAs → 2,670 chunks | PDCA-Aware Adaptive Chunking | 15+ Metadata Fields | Hybrid Retrieval for Complete Context" style="text;html=1;strokeColor=none;fillColor=#FFF9C4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=16;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="200" y="80" width="2000" height="35" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;TIER 1: CHROMADB&lt;/font&gt;&lt;br&gt;&lt;font color=&quot;#E65100&quot;&gt;(Semantic Search)&lt;/font&gt;&#xa;&#xa;Purpose: Find similar PDCAs by meaning&#xa;&#xa;Data Structure:&#xa;• Collection: pdca_historical&#xa;• Documents: 534 PDCAs&#xa;• Chunks: ~2,670 (5 per PDCA)&#xa;• Embeddings: 768-dimensional vectors&#xa;• Index: HNSW (fast similarity search)&#xa;&#xa;Query Speed: ~500ms&#xa;Best For: Semantic similarity" tooltip="Tier 1 ChromaDB provides semantic search capabilities using vector embeddings. ChromaDB is an open-source vector database optimized for embedding-based similarity search. Each PDCA chunk is converted to a 768-dimensional vector using sentence-transformers/all-MiniLM-L6-v2 model, capturing semantic meaning beyond keyword matching. The 534 historical PDCAs are processed with PDCA-aware adaptive chunking creating approximately 2,670 semantically complete chunks (average 5 per PDCA). Chunks preserve document structure by splitting on section boundaries: header metadata, Plan section, Do section, Check section, Act plus Meta sections. Each chunk includes 15+ metadata fields enabling filtered queries. ChromaDB uses HNSW Hierarchical Navigable Small World index for fast approximate nearest neighbor search, returning results in approximately 500ms. The metadata fields enable precise filtering by chunk_type (header, plan, do, check, act, meta), cmm_level (CMM1-CMM4), task_type (component creation, debugging, refactoring, integration), date (YYYY-MM-DD), agent_name, agent_role, session_id, branch, sprint, verification_status (verified, unverified), and quality_score (0-100). ChromaDB is ideal for queries like show me similar debugging approaches, find PDCAs about component versioning, or what patterns exist for refactoring. The semantic search finds relevant PDCAs even if they use different terminology because embeddings capture meaning. During training sample generation, ChromaDB is queried thousands of times to extract patterns: query for empty constructor examples filtered by pattern equals empty_constructor, query for CMM3 transformations filtered by cmm_level equals CMM3 and task_type equals refactoring. ChromaDB also indexes 3,477 TypeScript component files organized by layer (layer2, layer3, layer5) and pattern (empty_constructor, scenario_state, radical_oop), plus 238 process documents organized by role." id="tier1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="150" width="600" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#C62828&quot;&gt;TIER 2: REDIS GRAPH&lt;/font&gt;&lt;br&gt;&lt;font color=&quot;#C62828&quot;&gt;(Breadcrumb Navigation)&lt;/font&gt;&#xa;&#xa;Purpose: Walk PDCA chains for context&#xa;&#xa;Data Structure:&#xa;• Nodes: 534 PDCAs&#xa;• Edges: PRECEDES relationships&#xa;• Properties: pdca_id, agent, date, session&#xa;• Graph traversal: Cypher-like queries&#xa;&#xa;Query Speed: ~10ms&#xa;Best For: Adjacency, context expansion" tooltip="Tier 2 Redis Graph stores PDCA breadcrumb relationships as a graph database, enabling fast traversal of prev/next links. Redis Graph is built on Redis using sparse adjacency matrices for efficient graph operations. Each of the 534 PDCAs becomes a node with properties: pdca_id (unique identifier), agent_name (who created it), agent_role (SaveRestartAgent, NegotiatorAgent, etc), date (YYYY-MM-DD), session_id (timestamp-based), branch (dev, main, feat), sprint (sprint number if applicable), objective (one-line summary). PRECEDES edges connect chronologically related PDCAs based on backward and forward links extracted from PDCA metadata. Graph queries are extremely fast approximately 10ms compared to vector search approximately 500ms because they use index lookups rather than similarity computation. The primary use case is read-to-depth-3: when semantic search finds a relevant PDCA, walk the graph backward and forward up to 3 levels deep to understand the full context. This implements the Web4 principle that context matters - a single PDCA in isolation may miss important background from previous work or follow-up from subsequent PDCAs. Example query: MATCH (p:PDCA {pdca_id: &#39;found_pdca&#39;}) OPTIONAL MATCH (prev1)-[:PRECEDES]-&gt;(p) OPTIONAL MATCH (prev2)-[:PRECEDES]-&gt;(prev1) OPTIONAL MATCH (prev3)-[:PRECEDES]-&gt;(prev2) OPTIONAL MATCH (p)-[:PRECEDES]-&gt;(next1) OPTIONAL MATCH (next1)-[:PRECEDES]-&gt;(next2) OPTIONAL MATCH (next2)-[:PRECEDES]-&gt;(next3) RETURN prev3, prev2, prev1, p, next1, next2, next3. This query walks 3 levels backward and 3 levels forward in the breadcrumb chain. Redis Graph supports Cypher-like query language familiar to Neo4j users. During training sample generation, graph expansion enriches semantic search results: query ChromaDB for relevant PDCA, get pdca_id from result, query Redis Graph for breadcrumb chain, fetch full context from ChromaDB for all PDCAs in chain, include context in training sample. This ensures training samples have rich context rather than isolated snippets." id="tier2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCDD2;strokeColor=#C62828;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="800" y="150" width="600" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;TIER 3: SQLITE&lt;/font&gt;&lt;br&gt;&lt;font color=&quot;#1565C0&quot;&gt;(Temporal Queries)&lt;/font&gt;&#xa;&#xa;Purpose: Fast date-based queries&#xa;&#xa;Data Structure:&#xa;• Table: pdca_timeline&#xa;• Columns: pdca_id, timestamp, agent, role, session, branch, sprint, cmm_level, objective&#xa;• Indexes: B-tree on timestamp, agent, cmm_level&#xa;&#xa;Query Speed: ~5ms&#xa;Best For: Date ranges, agent timelines" tooltip="Tier 3 SQLite stores temporal and categorical metadata in a relational schema optimized for fast date-range queries, agent timelines, and sprint aggregations. SQLite is a lightweight embedded SQL database requiring no server, perfect for local queries. The pdca_timeline table schema: pdca_id TEXT PRIMARY KEY (unique identifier), timestamp INTEGER (Unix timestamp for precise ordering), date TEXT (YYYY-MM-DD for date-range queries), session TEXT (session identifier YYYY-MM-DD-UTC-HHMM-agent), agent_name TEXT (creator), agent_role TEXT (role type), branch TEXT (git branch), sprint TEXT (sprint number), cmm_level TEXT (CMM1, CMM2, CMM3, CMM4), objective TEXT (one-line summary), quality_score INTEGER (0-100). Indexes enable sub-5ms queries: CREATE INDEX idx_date ON pdca_timeline(date), CREATE INDEX idx_agent ON pdca_timeline(agent_name), CREATE INDEX idx_cmm ON pdca_timeline(cmm_level), CREATE INDEX idx_timestamp ON pdca_timeline(timestamp). These B-tree indexes are 100x faster than scanning ChromaDB with metadata filters because SQL databases are optimized for structured queries. Example queries: SELECT * FROM pdca_timeline WHERE date BETWEEN &#39;2025-10-15&#39; AND &#39;2025-10-20&#39; ORDER BY timestamp (all work from Oct 15-20), SELECT agent_name, COUNT(*) as count FROM pdca_timeline GROUP BY agent_name ORDER BY count DESC (most active agents), SELECT cmm_level, COUNT(*) FROM pdca_timeline WHERE date &gt; &#39;2025-10-01&#39; GROUP BY cmm_level (CMM distribution this month), SELECT * FROM pdca_timeline WHERE agent_name = &#39;SaveRestartAgent&#39; ORDER BY timestamp DESC LIMIT 10 (recent work by specific agent). During training sample generation, temporal queries ensure diverse time period coverage to prevent temporal bias where the model only learns the newest patterns. Stratified sampling: query SQLite to get PDCA IDs from different date ranges (2024-Q1, 2024-Q2, 2024-Q3, 2024-Q4, 2025-Q1), ensure representative samples from each period, query ChromaDB for full content using filtered PDCA IDs. SQLite is also used for analytics dashboards: track PDCAs per day, measure sprint velocity, monitor CMM compliance trends over time." id="tier3">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1565C0;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1500" y="150" width="600" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#F57F17&quot;&gt;PDCA-AWARE ADAPTIVE CHUNKING&lt;/font&gt;&#xa;&#xa;Problem: Standard 512-token chunks destroy structure&#xa;Solution: Section-aware semantic chunking&#xa;&#xa;Chunking Strategy:&#xa;1. Parse PDCA markdown structure&#xa;2. Identify section boundaries (## headers)&#xa;3. Create chunks preserving sections:&#xa;   • Chunk 1: Header + metadata (links, objective)&#xa;   • Chunk 2: Plan section (TRON, strategy)&#xa;   • Chunk 3: Do section (execution)&#xa;   • Chunk 4: Check section (verification, QA)&#xa;   • Chunk 5: Act + Meta (learnings, CMM badge)&#xa;4. Each chunk 400-800 tokens (semantic unit)&#xa;5. Add comprehensive metadata (15+ fields)&#xa;&#xa;Result: Semantically complete retrievals" tooltip="PDCA-Aware Adaptive Chunking solves the problem of structure loss in standard fixed-size chunking. Standard RAG approach: split documents into fixed 512-token chunks without regard to structure. This destroys PDCA sections, scatters metadata across chunks, loses context between sections, and creates false positive retrievals when queries match fragments. Web4 solution: section-aware semantic chunking that preserves document structure. The chunking algorithm: Step 1 Parse PDCA markdown using regex to identify section headers (## LINKS, ## PLAN, ## DO, ## CHECK, ## ACT, ## META). Step 2 Extract metadata from header: backward link, forward link, objective, agent, role, date, session, branch, sprint. Step 3 Create semantically complete chunks: Chunk 1 Header contains links section, objective, agent metadata approximately 200-400 tokens with metadata chunk_type equals header. Chunk 2 Plan contains TRON decision (Trigger, Response, Outcome, Next), strategy, approach approximately 400-800 tokens with metadata chunk_type equals plan. Chunk 3 Do contains execution steps, code changes, commands run approximately 400-800 tokens with metadata chunk_type equals do. Chunk 4 Check contains verification steps, QA decisions, test results approximately 400-800 tokens with metadata chunk_type equals check. Chunk 5 Act plus Meta contains learnings, improvements, next steps, CMM badge, template version approximately 300-600 tokens with metadata chunk_type equals act. Step 4 Add comprehensive metadata to each chunk: pdca_id (unique identifier), chunk_index (0-4), chunk_type (header/plan/do/check/act), chunk_content (actual text), agent_name, agent_role, date, session_id, branch, sprint, cmm_level (CMM1-CMM4), task_type (component creation, debugging, refactoring, integration, collaboration), objective (one-line summary), quality_score (0-100 based on completeness, CMM compliance, dual links), verification_status (verified if dual links valid, unverified otherwise), trained_in_adapter (False initially, True after training), training_batch (which batch trained this), training_date (when trained). Step 5 Index chunks into ChromaDB with embeddings and metadata, store PRECEDES edges in Redis Graph, store timeline in SQLite. Benefits: Retrieval returns complete sections not fragments, metadata enables precise filtering, structure preserved for pattern extraction, false positives reduced, training samples have full context." id="chunking">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F9A825;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="540" width="1000" height="380" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#6A1B9A&quot;&gt;COMPREHENSIVE METADATA SCHEMA (15+ Fields)&lt;/font&gt;&#xa;&#xa;Temporal: date, timestamp, session_id&#xa;Agent Context: agent_name, agent_role, branch, sprint&#xa;Work Context: task_type, objective, cmm_level&#xa;Task Classification: chunk_type, chunk_index&#xa;Quality Signals: quality_score, verification_status&#xa;Training Lifecycle: trained_in_adapter, training_batch, training_date&#xa;&#xa;Enables:&#xa;• Filtered queries (CMM3 refactorings from Oct 2025)&#xa;• Stratified sampling (diverse dates, agents, tasks)&#xa;• Quality ranking (sort by quality_score DESC)&#xa;• Training tracking (query untrained with trained_in_adapter = False)&#xa;• Audit trail (when was this trained, in which batch)" tooltip="Comprehensive metadata schema enables intelligent querying and sampling beyond simple semantic search. The 15+ metadata fields are organized into categories. Temporal metadata: date TEXT (YYYY-MM-DD for date-range queries), timestamp INTEGER (Unix timestamp for precise ordering), session_id TEXT (session identifier YYYY-MM-DD-UTC-HHMM-agent for grouping work). Agent context metadata: agent_name TEXT (who created this PDCA), agent_role TEXT (SaveRestartAgent, NegotiatorAgent, BuilderAgent, TesterAgent, RefinerAgent, IntegratorAgent), branch TEXT (git branch where work happened dev, main, feat/ticket), sprint TEXT (sprint number if part of sprint work). Work context metadata: task_type TEXT (component_creation, debugging, refactoring, integration, collaboration, architectural_decision, violation_fix, testing), objective TEXT (one-line summary of PDCA purpose), cmm_level TEXT (CMM1, CMM2, CMM3, CMM4 indicating process maturity). Task classification metadata: chunk_type TEXT (header, plan, do, check, act indicating which section this chunk contains), chunk_index INTEGER (0-4 indicating position in document). Quality signals metadata: quality_score INTEGER (0-100 computed from completeness of sections, CMM compliance, dual link validity, TRON format adherence), verification_status TEXT (verified if dual links valid and file pushed, unverified if links missing or not pushed). Training lifecycle metadata: trained_in_adapter BOOLEAN (False initially, True after training), training_batch TEXT (initial_20251027 or nightly_YYYYMMDD indicating which batch trained this), training_date TEXT (ISO8601 timestamp when training occurred). Use cases enabled by metadata: Filtered queries - ChromaDB query with where filter cmm_level equals CMM3 AND task_type equals refactoring AND date greater than 2025-10-01 finds CMM3 refactorings from October. Stratified sampling - SQLite query SELECT pdca_id FROM pdca_timeline WHERE date BETWEEN &#39;2024-01-01&#39; AND &#39;2024-03-31&#39; LIMIT 50 gets Q1 2024 samples, repeat for Q2, Q3, Q4, Q1 2025 ensuring temporal diversity. Quality ranking - ChromaDB query with where filter quality_score greater than 80 sorted by quality_score DESC finds highest quality PDCAs for representative samples. Training tracking - Evening loop query ChromaDB with where filter trained_in_adapter equals False finds untrained patterns from daily_buffer. Audit trail - Query training_batch equals nightly_20251028 finds all chunks trained in tonight batch for rollback if needed. The rich metadata transforms RAG from simple semantic search to intelligent data warehouse enabling sophisticated sampling strategies for training." id="metadata">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#7B1FA2;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1200" y="540" width="900" height="380" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#2E7D32&quot;&gt;HYBRID RETRIEVAL: COMBINING ALL THREE TIERS&lt;/font&gt;&#xa;&#xa;Query: How did we solve component versioning conflicts?&#xa;&#xa;Step 1: Semantic Search (ChromaDB) - ~500ms&#xa;  query = &#39;component versioning conflicts&#39;&#xa;  where = {task_type: &#39;debugging&#39;, cmm_level: &#39;CMM3&#39;}&#xa;  results = 5 relevant PDCAs&#xa;&#xa;Step 2: Graph Expansion (Redis Graph) - ~10ms per PDCA&#xa;  For each result, walk breadcrumb chain depth 3&#xa;  Get context: what came before, what came after&#xa;  Total: 5 * 7 PDCAs = 35 PDCAs (with context)&#xa;&#xa;Step 3: Temporal Filtering (SQLite) - ~5ms&#xa;  Ensure diverse time periods (not all from last month)&#xa;  SELECT FROM pdca_timeline WHERE pdca_id IN (results)&#xa;  AND date distributed across quarters&#xa;&#xa;Step 4: Re-rank and Return&#xa;  Sort by quality_score DESC&#xa;  Return top 5 with full breadcrumb context&#xa;  Total latency: ~600ms&#xa;&#xa;Result: Semantically relevant + contextually complete + temporally diverse" tooltip="Hybrid retrieval combines all three tiers to provide optimal results. Single-tier approaches have limitations: ChromaDB alone returns relevant PDCAs but misses context from breadcrumb chains. Redis Graph alone requires knowing which PDCA to start from. SQLite alone has no semantic understanding. Hybrid approach leverages each tier strength. Example query: How did we solve component versioning conflicts? Step 1 Semantic Search (ChromaDB approximately 500ms): Build query vector from user question using same sentence-transformer model. Query ChromaDB pdca_historical collection with semantic search. Apply metadata filters: where equals task_type colon debugging AND cmm_level colon CMM3 to find high-quality debugging PDCAs. Set n_results equals 10 to get top 10 semantically relevant PDCAs. ChromaDB returns: list of 10 PDCA chunks with pdca_id, chunk_content, metadata, and similarity scores. Step 2 Graph Expansion (Redis Graph approximately 10ms per PDCA): For each of the 10 PDCAs, query Redis Graph for breadcrumb context. Cypher query: MATCH path equals (prev3)-[:PRECEDES]-&gt;(prev2)-[:PRECEDES]-&gt;(prev1)-[:PRECEDES]-&gt;(p {pdca_id: found_id})-[:PRECEDES]-&gt;(next1)-[:PRECEDES]-&gt;(next2)-[:PRECEDES]-&gt;(next3) RETURN prev3, prev2, prev1, p, next1, next2, next3. This walks 3 levels backward and 3 levels forward in the breadcrumb chain. Result: 7 PDCAs per original result (3 before, 1 original, 3 after). Total: 10 times 7 equals 70 PDCAs with context. Graph traversal is 50x faster than semantic search because it uses index lookups. Step 3 Temporal Filtering (SQLite approximately 5ms): Query SQLite pdca_timeline table to check date distribution. SELECT date, COUNT(*) FROM pdca_timeline WHERE pdca_id IN (70 pdca_ids) GROUP BY date. If results are clustered in recent dates, diversify: keep top 5 from recent, add 5 from older periods (Q3 2024, Q4 2024, Q1 2025). This prevents temporal bias where model only learns newest approaches. Step 4 Re-rank and Return: Combine semantic similarity scores from ChromaDB with quality_scores from metadata. Sort by weighted score: 0.7 times similarity plus 0.3 times (quality_score / 100). Return top 5 PDCAs with full breadcrumb context (35 PDCAs total). Total hybrid retrieval latency: approximately 600ms (500ms semantic plus 100ms graph plus 5ms temporal plus 5ms reranking). Result: User gets semantically relevant PDCAs for their question, contextually complete with breadcrumb chains showing what led to and followed from each solution, temporally diverse preventing recency bias, quality-ranked ensuring best examples shown first. This hybrid approach provides far better results than any single tier alone." id="hybrid">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="1000" width="2000" height="500" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#D84315&quot;&gt;🎯 WHY THREE TIERS?&lt;/font&gt;&#xa;&#xa;Single-Tier Limitations:&#xa;❌ ChromaDB alone: Relevant but no context, no date filtering, slow for structured queries&#xa;❌ Graph alone: Fast but requires knowing start node, no semantic understanding&#xa;❌ SQL alone: Fast structured queries but no semantic search, no graph relationships&#xa;&#xa;Three-Tier Benefits:&#xa;✅ ChromaDB: Semantic understanding (find by meaning)&#xa;✅ Redis Graph: Contextual expansion (walk breadcrumb chains)&#xa;✅ SQLite: Temporal diversity (prevent recency bias)&#xa;✅ Combined: Best of all three in 600ms&#xa;&#xa;Real-World Impact:&#xa;• Training sample generation: 37K samples from intelligent RAG queries&#xa;• Runtime retrieval: 10-20% of queries need historical context&#xa;• Quality: Semantically relevant + contextually complete + temporally diverse" tooltip="Why three tiers instead of one? Single-tier limitations: ChromaDB alone provides semantic search which is powerful for finding relevant PDCAs by meaning, but it lacks context from breadcrumb chains (single PDCA in isolation misses what came before and after), has no efficient date-range queries (must scan all chunks with metadata filters which is slow), and cannot handle graph traversal (finding connected PDCAs requires multiple queries). Redis Graph alone provides fast graph traversal approximately 10ms which is excellent for breadcrumb navigation, but it requires knowing which PDCA to start from (cannot search by semantic meaning), has no semantic understanding (cannot find similar approaches if you don&#39;t know the PDCA ID), and stores minimal properties (full content lives in ChromaDB). SQLite alone provides fast structured queries approximately 5ms on indexed columns for date ranges, agent timelines, and aggregations, but it has no semantic search capability (cannot find similar debugging approaches), has no graph relationships (cannot walk breadcrumb chains), and stores only metadata not full content. Three-tier benefits: ChromaDB provides semantic understanding via vector embeddings capturing meaning beyond keywords, enabling queries like find similar debugging approaches even if different terminology used. Redis Graph provides contextual expansion via breadcrumb traversal, enabling read-to-depth-3 principle to include predecessor and successor context, essential for understanding full problem-solving journey. SQLite provides temporal diversity via fast date-range queries preventing recency bias, enabling stratified sampling across time periods to ensure model learns historical patterns not just newest approaches. Combined these three tiers provide: semantic search to find relevant PDCAs, graph expansion to include context, temporal filtering to ensure diversity, all in approximately 600ms total latency. Real-world impact: Training sample generation uses hybrid queries to generate all 37K training samples: semantic queries to find patterns, graph expansion to include context, temporal filtering to ensure representatives from all time periods. Runtime retrieval for 10-20 percent of production queries requiring historical context like how did we solve X before or what did we work on date Y. Quality of results: semantically relevant (ChromaDB finds by meaning), contextually complete (Redis Graph includes breadcrumb context), temporally diverse (SQLite ensures not all recent). This three-tier architecture is the innovation that enables RAG-first training pipeline: RAG becomes single source of truth for both training data generation and runtime historical queries, ensuring consistency and traceability between training and deployment." id="why-three">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="1560" width="1000" height="460" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#01579B&quot;&gt;📊 PERFORMANCE CHARACTERISTICS&lt;/font&gt;&#xa;&#xa;Query Speed Comparison:&#xa;• Semantic Search (ChromaDB): ~500ms&#xa;• Graph Traversal (Redis): ~10ms (50x faster)&#xa;• Temporal Query (SQLite): ~5ms (100x faster)&#xa;• Hybrid Retrieval: ~600ms (all three combined)&#xa;&#xa;Scale:&#xa;• 534 PDCAs indexed&#xa;• ~2,670 chunks in ChromaDB&#xa;• 534 nodes + PRECEDES edges in Redis Graph&#xa;• 534 rows in SQLite pdca_timeline&#xa;• Total storage: ~15MB (compressed)&#xa;&#xa;Indexing Time:&#xa;• Initial bootstrap: ~1 hour (all 534 PDCAs)&#xa;• Incremental: ~2 seconds per new PDCA&#xa;• Evening loop: ~5 minutes for 10-50 PDCAs&#xa;&#xa;Hardware Requirements:&#xa;• RAM: ~2GB for ChromaDB, ~500MB for Redis, ~50MB for SQLite&#xa;• Disk: ~15MB compressed, ~50MB uncompressed&#xa;• CPU: Minimal (batch indexing uses MPS if available)" tooltip="Performance characteristics of the three-tier RAG system. Query speed comparison: Semantic search via ChromaDB takes approximately 500ms because it computes vector similarity across 2,670 chunks using HNSW index which is fast for approximate nearest neighbor but slower than index lookups. Graph traversal via Redis Graph takes approximately 10ms because it uses sparse adjacency matrices and index lookups for PRECEDES edges, making it 50x faster than semantic search for breadcrumb navigation. Temporal query via SQLite takes approximately 5ms because B-tree indexes on timestamp, date, agent, and cmm_level columns enable instant lookups, making it 100x faster than scanning ChromaDB metadata filters. Hybrid retrieval combining all three tiers takes approximately 600ms total: 500ms for semantic search, 10ms times 10 PDCAs equals 100ms for graph expansion, 5ms for temporal filtering, 5ms for reranking. Scale at current dataset size: 534 PDCAs indexed across all three tiers, approximately 2,670 chunks in ChromaDB (average 5 chunks per PDCA preserving section structure), 534 nodes plus PRECEDES edges in Redis Graph (one node per PDCA, edges based on backward forward links), 534 rows in SQLite pdca_timeline table (one row per PDCA with temporal and categorical metadata), total storage approximately 15MB compressed or approximately 50MB uncompressed. Indexing time for various operations: Initial bootstrap indexing all 534 PDCAs takes approximately 1 hour (parsing markdown, generating embeddings, indexing to ChromaDB, building Redis Graph, populating SQLite). Incremental indexing for one new PDCA takes approximately 2 seconds (parse, embed, insert to all three tiers). Evening loop indexing 10-50 new PDCAs takes approximately 5 minutes as part of the nightly training process. Hardware requirements are modest: RAM usage approximately 2GB for ChromaDB collection in memory, approximately 500MB for Redis Graph data structures, approximately 50MB for SQLite database. Disk storage approximately 15MB compressed for all three tiers, approximately 50MB uncompressed if indexes expanded. CPU usage is minimal during queries (sub-second latency), batch indexing can leverage MPS Metal Performance Shaders on M1 Mac for embedding generation which speeds up bootstrap from 2 hours to 1 hour. The system is designed for single-machine deployment on M1 Mac with 32GB RAM, no distributed systems needed at current scale. Future scaling: If PDCAs grow to 5,000 (10x), ChromaDB will still be fast with HNSW index, Redis Graph scales linearly with nodes, SQLite handles millions of rows efficiently. At 50,000 PDCAs (100x) may need to shard ChromaDB or switch to Qdrant for production-grade vector DB, but current three-tier architecture remains sound." id="performance">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FE;strokeColor=#0277BD;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1200" y="1560" width="900" height="460" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="rag-footer" value="🎯 Three-Tier Innovation: Semantic (ChromaDB) + Graph (Redis) + Temporal (SQLite) = Complete Context in 600ms | PDCA-Aware Chunking Preserves Structure | 15+ Metadata Fields Enable Intelligent Sampling | Hybrid Retrieval Combines Best of All Three Tiers" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F9A825;strokeWidth=3;align=center;verticalAlign=middle;fontSize=13;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="100" y="2090" width="2000" height="60" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
    <diagram name="Tool Orchestration Flow" id="tool-flow">
        <mxGraphModel dx="2325" dy="855" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="2400" pageHeight="1800" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="tool-title" value="Web4 Tool Orchestration Flow: RAG-First with Detection" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=32;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="300" y="20" width="1800" height="50" as="geometry"/>
                </mxCell>
                <mxCell id="tool-subtitle" value="Production Runtime Detail: Model ALWAYS uses trained knowledge (patterns/methodology) FIRST | RAG supplements with tool syntax when needed (30% of queries, +150ms) | Training-First, not RAG-First" style="text;html=1;strokeColor=none;fillColor=#FFF9C4;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=16;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="200" y="80" width="2000" height="35" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#1565C0&quot;&gt;USER PROMPT&lt;/font&gt;&lt;br&gt;&lt;br&gt;Read Button.ts and check&lt;br&gt;for constructor violations" tooltip="The user provides a natural language prompt that may or may not require tool usage. Example: Read Button.tsx and check for constructor violations. This prompt requires two tools: read_file to access the file contents and potentially grep to search for constructor patterns. The challenge for a 7B parameter small LLM is that it has only 1K generic tool awareness samples trained (3 percent of training data) which teaches the CONCEPT of tools but not specific IDE implementations. The model needs explicit examples at runtime to generate correct tool call syntax for the current IDE ecosystem Continue or Cursor. The orchestration system must quickly detect if tools are needed, query RAG for relevant examples if so, inject those examples into the LLM context, and let the model generate the correct tool call. This happens transparently to the user with minimal latency overhead." id="user-prompt">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E3F2FD;strokeColor=#1565C0;strokeWidth=4;fontSize=14;fontStyle=1;verticalAlign=middle;" parent="1" vertex="1">
                        <mxGeometry x="70" y="150" width="280" height="120" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#F57F17&quot;&gt;KEYWORD DETECTOR&lt;/font&gt;&#xa;&#xa;Duration: ~1ms&#xa;Rule-based pattern matching&#xa;&#xa;Detected: read, .tsx&#xa;Likely tools: read_file" tooltip="The keyword detector is a fast rule-based system that analyzes the user prompt for tool-indicating keywords. It runs in approximately 1ms using simple pattern matching, making it negligible overhead. The detector maintains a dictionary of tool keywords: read shows read_file or read_currently_open_file, create shows create_new_file, edit shows edit_existing_file or single_find_and_replace, run shows run_terminal_command, search shows file_glob_search, list shows ls, fetch shows fetch_url_content. File extensions like .ts, .tsx, .py, .js, .json, .md also suggest read_file. Command patterns like npm, git, install suggest run_terminal_command. For the example prompt Read Button.tsx and check for constructor violations, the detector finds keyword read and file extension .tsx, concluding that read_file is likely needed. This simple heuristic achieves 95 percent plus accuracy because tool usage patterns are predictable. The detector returns a boolean needs_tools and a list of likely_tools. If needs_tools is False, the system skips RAG entirely and goes straight to LLM inference, saving 150ms. This optimization is critical because 70 percent of queries do not need tools." id="keyword-detector">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="610" y="150" width="280" height="180" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#2E7D32&quot;&gt;RAG QUERY&lt;/font&gt;&#xa;&#xa;Duration: ~150ms&#xa;Collection: tool_examples&#xa;&#xa;Where: {&#xa;  tool_ecosystem: continue,&#xa;  tool_name: read_file&#xa;}&#xa;N_results: 2-3 examples" tooltip="If the keyword detector determines that tools are needed, the system queries the RAG tool_examples collection for relevant examples. This query takes approximately 150ms using ChromaDB semantic search. The query filters by tool_ecosystem (continue or cursor depending on current IDE), tool_name (the detected tools like read_file), and optionally usage_pattern (simple, intermediate, complex) and context_type (typescript, python, javascript). The query requests 2-3 examples which is optimal: enough to show the pattern without overloading the context (approximately 300 tokens total). The tool_examples collection contains 12K IDE-specific tool examples stored with comprehensive metadata: tool_name (read_file, grep, run_terminal_command, etc), tool_ecosystem (continue, cursor, custom), tool_version (for ecosystem versioning), usage_pattern (simple, intermediate, complex, edge_case), context_type (typescript, python, javascript, general), is_negative_example (False for correct usage, True for anti-patterns), trained_in_adapter (False, these are NOT trained). These 12K examples are swappable: switching from Continue to Cursor takes 5 minutes (clear Continue tools, index Cursor tools, update ecosystem filter) versus 10-14 hours full model retraining. The RAG query returns the most relevant examples based on semantic similarity to the user prompt, ensuring the injected examples are contextually appropriate." id="rag-query">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#2E7D32;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="610" y="380" width="280" height="260" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#6A1B9A&quot;&gt;CONTEXT INJECTION&lt;/font&gt;&#xa;&#xa;Duration: ~5ms&#xa;Format: System prompt with examples&#xa;&#xa;[TOOL EXAMPLES - Continue]&#xa;Example 1: read_file syntax&#xa;Example 2: grep syntax&#xa;[END EXAMPLES]&#xa;&#xa;User: Read Button.tsx..." tooltip="The context injection step formats the RAG-retrieved tool examples into a structured prompt that guides the LLM. This takes approximately 5ms for text formatting. The augmented prompt structure: System section contains You are a Web4 assistant with access to tools. Tool Examples section in a clearly delimited block showing 2-3 concrete examples with correct XML syntax for the current IDE. Each example includes the tool call structure, required parameters, and expected usage pattern. User Query section with the original user prompt. This structure ensures the LLM sees the correct tool syntax immediately before generating its response. The injection happens transparently - the user never sees the injected examples, only the final result. The approximately 300 tokens of injected examples add minimal context pressure because the base model has a 32K context window. The examples are formatted to match the IDE tool calling convention: Continue uses XML-style tags, Cursor uses JSON function calls, custom tools use their defined formats. The ToolAwarePromptBuilder class handles this formatting automatically based on the tool_ecosystem setting." id="context-injection">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#7B1FA2;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1260" y="140" width="300" height="260" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#D84315&quot;&gt;LLM INFERENCE&lt;/font&gt;&#xa;&#xa;Duration: ~2000ms&#xa;Model: web4-agent:latest&#xa;(Qwen2.5-Coder-7B + LoRA)&#xa;&#xa;Sees: Examples from RAG&#xa;Generates: Correct tool call&#xa;Follows: Exact syntax&#xa;&#xa;Result: Tool call JSON/XML" tooltip="The LLM processes the augmented prompt and generates the appropriate tool call. Inference takes approximately 2000ms on M1 Mac at approximately 20 tokens per second. The model is web4-agent:latest which combines Qwen2.5-Coder-7B base model general coding knowledge with Web4-specific LoRA adapter patterns and methodology. The adapter was trained on only 1K generic tool awareness samples (3 percent of training) which teaches the CONCEPT of tools: tools have names, take parameters, return results, should be used when reading files or running commands. However, the adapter does NOT contain IDE-specific syntax because that would require training 12K samples (23 percent of budget) and make the model inflexible to IDE changes. Instead, the model learns from the RAG-injected examples at runtime. The LLM sees the system prompt with 2-3 concrete tool examples showing the exact XML or JSON syntax required by the current IDE. The model pattern-matches against these examples and generates a tool call following the same structure. Because the examples are semantically relevant to the user query (retrieved via semantic search), the model can adapt the pattern to the specific task. Example output for Continue: XML tag read_file with child target_file tag containing src/components/Button.tsx. The generated tool call is then executed by the IDE orchestration layer." id="llm-inference">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1260" y="490" width="300" height="300" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#01579B&quot;&gt;TOOL EXECUTION&lt;/font&gt;&#xa;&#xa;Duration: ~100ms&#xa;Execute: read_file&#xa;Path: Button.tsx&#xa;&#xa;Return: File contents&#xa;&#xa;Model continues with&#xa;constructor violation check" tooltip="The generated tool call is executed by the IDE tool orchestration layer. Execution takes approximately 100ms depending on the tool: read_file is fast (disk I/O), run_terminal_command varies by command, grep depends on search scope. The tool execution layer parses the LLM-generated tool call JSON or XML, validates parameters (file paths exist, commands are safe), executes the tool action, captures the result (file contents, command output, search results), formats the result for the LLM, and returns it to the conversation context. The LLM then continues processing with the tool result available. For the example prompt Read Button.tsx and check for constructor violations, the execution sequence: LLM generates read_file tool call, tool layer reads src/components/Button.tsx, file contents returned to LLM context, LLM analyzes contents for constructor violations using its trained Web4 patterns (empty constructor rule), LLM generates response explaining any violations found. This multi-turn interaction happens seamlessly with the tool execution transparent to the user. Total latency for tool-requiring query: 1ms detection plus 150ms RAG query plus 5ms injection plus 2000ms LLM inference plus 100ms tool execution equals approximately 2256ms. Compare to non-tool query: 1ms detection plus 2000ms LLM inference equals approximately 2001ms, saving 255ms by skipping RAG." id="tool-execution">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FE;strokeColor=#0277BD;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1840" y="150" width="300" height="260" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#558B2F&quot;&gt;RESPONSE TO USER&lt;/font&gt;&#xa;&#xa;File contents displayed&#xa;Constructor violations found:&#xa;&#xa;Line 15: Logic in constructor&#xa;Violation: Web4 empty&#xa;constructor pattern requires&#xa;all initialization in init()&#xa;&#xa;Suggestion: Move logic to&#xa;init() method" tooltip="The final response combines tool execution results with the LLM trained Web4 knowledge to provide a comprehensive answer. The response includes file contents if relevant, analysis based on Web4 patterns (empty constructor, 5-layer architecture, Radical OOP), specific violations found with line numbers and explanations, suggestions for fixes following Web4 conventions, and source citations if RAG PDCA history was also consulted. The user receives a high-quality response that correctly used tools (thanks to RAG-injected examples) and applied Web4 patterns (thanks to LoRA-trained knowledge). The entire interaction from user prompt to final response takes approximately 2256ms for tool queries or approximately 2001ms for non-tool queries. The system architecture enables fast responses while maintaining flexibility: the 7B parameter model is small enough to run locally on consumer hardware, the LoRA adapter can be updated nightly without full retraining, the RAG tool examples can be swapped in 5 minutes when changing IDEs, and the keyword detector minimizes overhead for non-tool queries. This RAG-first-with-detection approach is optimal for small LLMs that need explicit examples but cannot afford to query RAG on every request." id="response">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#DCEDC8;strokeColor=#689F38;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="1840" y="530" width="300" height="300" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow-prompt-detector" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#1565C0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.25;entryDx=0;entryDy=0;" parent="1" source="user-prompt" target="keyword-detector" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="380" y="210" as="sourcePoint"/>
                        <mxPoint x="460" y="195" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-prompt-detector" value="Analyze prompt" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#1565C0;fillColor=#E3F2FD;strokeColor=#1976D2;rounded=1;" parent="arrow-prompt-detector" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-detector-rag" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#F57F17;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" source="keyword-detector" target="rag-query" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="600" y="330" as="sourcePoint"/>
                        <mxPoint x="600" y="380" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-detector-rag" value="Tools needed&#xa;Query RAG" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#F57F17;fillColor=#FFF9C4;strokeColor=#F9A825;rounded=1;" parent="arrow-detector-rag" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-rag-injection" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#2E7D32;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="rag-query" target="context-injection" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="740" y="510" as="sourcePoint"/>
                        <mxPoint x="820" y="280" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-rag-injection" value="2-3 examples&#xa;~300 tokens" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#2E7D32;fillColor=#C8E6C9;strokeColor=#43A047;rounded=1;" parent="arrow-rag-injection" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint x="44" y="-31" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-injection-llm" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#6A1B9A;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" source="context-injection" target="llm-inference" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="970" y="410" as="sourcePoint"/>
                        <mxPoint x="970" y="460" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-injection-llm" value="Augmented prompt&#xa;with examples" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#6A1B9A;fillColor=#F3E5F5;strokeColor=#8E24AA;rounded=1;" parent="arrow-injection-llm" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-llm-execution" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#D84315;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="llm-inference" target="tool-execution" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1120" y="610" as="sourcePoint"/>
                        <mxPoint x="1200" y="280" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-llm-execution" value="Tool call&#xa;JSON/XML" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#D84315;fillColor=#FFCCBC;strokeColor=#FF5722;rounded=1;" parent="arrow-llm-execution" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-execution-response" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#01579B;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;" parent="1" source="tool-execution" target="response" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1350" y="410" as="sourcePoint"/>
                        <mxPoint x="1350" y="460" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-execution-response" value="Tool result&#xa;+ Analysis" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#01579B;fillColor=#E1F5FE;strokeColor=#0288D1;rounded=1;" parent="arrow-execution-response" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-detector-llm-direct" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=3;strokeColor=#9E9E9E;exitX=1;exitY=0.75;exitDx=0;exitDy=0;entryX=0;entryY=0.25;entryDx=0;entryDy=0;dashed=1;" parent="1" source="keyword-detector" target="llm-inference" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="740" y="285" as="sourcePoint"/>
                        <mxPoint x="820" y="535" as="targetPoint"/>
                        <Array as="points">
                            <mxPoint x="1050" y="410"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
                <mxCell id="label-detector-llm-direct" value="No tools needed&#xa;Skip RAG (70%)" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=11;fontStyle=1;fontColor=#616161;fillColor=#F5F5F5;strokeColor=#9E9E9E;rounded=1;" parent="arrow-detector-llm-direct" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="1" relative="1" as="geometry">
                        <mxPoint x="62" y="49" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;📊 LATENCY BREAKDOWN&lt;/font&gt;&#xa;&#xa;Tool Query (30% of requests):&#xa;• Keyword detection: 1ms&#xa;• RAG query: 150ms&#xa;• Context injection: 5ms&#xa;• LLM inference: 2000ms&#xa;• Tool execution: 100ms&#xa;• Total: ~2256ms&#xa;&#xa;Non-Tool Query (70% of requests):&#xa;• Keyword detection: 1ms&#xa;• LLM inference: 2000ms&#xa;• Total: ~2001ms (255ms saved)&#xa;&#xa;Weighted Average: ~2077ms" tooltip="Latency breakdown shows the timing for different query types in the RAG-first-with-detection orchestration. Tool Query (30 percent of requests): These queries require tool usage like read this file or run this command. The latency breakdown: Keyword detection 1ms using fast rule-based pattern matching. RAG query 150ms querying ChromaDB tool_examples collection with metadata filters. Context injection 5ms formatting examples into augmented prompt. LLM inference 2000ms generating tool call following injected examples. Tool execution 100ms executing the tool action and returning results. Total approximately 2256ms end-to-end. Non-Tool Query (70 percent of requests): These queries are answered from trained knowledge like explain empty constructor pattern or what is PDCA. The latency breakdown: Keyword detection 1ms checking for tool keywords. LLM inference 2000ms answering directly from trained Web4 patterns. Total approximately 2001ms end-to-end, saving 255ms by skipping RAG. Weighted Average: (0.3 times 2256) plus (0.7 times 2001) equals approximately 2077ms average latency across all queries. The RAG-first-with-detection approach optimizes for the common case (70 percent non-tool) while maintaining correct tool usage (30 percent tool). Compare to RAG-always approach: all queries would take 2256ms for 2150ms average, wasting 73ms per query. Compare to LLM-first approach: tool queries would require double LLM inference (approximately 4000ms) for worse user experience. The hybrid approach achieves the best balance of speed and correctness." id="latency">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#EF6C00;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="820" width="600" height="400" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#4A148C&quot;&gt;🔄 IDE SWITCHING PROCEDURE&lt;/font&gt;&#xa;&#xa;From Continue to Cursor:&#xa;&#xa;Step 1: Clear Continue tools (2 min)&#xa;  chromadb.delete(where={tool_ecosystem: continue})&#xa;&#xa;Step 2: Index Cursor tools (3 min)&#xa;  Index 12K Cursor tool examples&#xa;  Metadata: tool_ecosystem=cursor&#xa;&#xa;Step 3: Update filter (~5 seconds)&#xa;  ToolAwarePromptBuilder.ecosystem = cursor&#xa;&#xa;Total Time: ~5 minutes&#xa;Compare: Full retraining would take 10-14 hours&#xa;&#xa;Benefits:&#xa;✓ No model retraining required&#xa;✓ Support multiple IDEs simultaneously&#xa;✓ Easy to add custom tools&#xa;✓ Rollback in seconds if needed" tooltip="IDE switching procedure demonstrates the flexibility of the hybrid tool architecture. The process to switch from Continue to Cursor takes approximately 5 minutes versus 10-14 hours for full model retraining. Step 1 Clear Continue Tools (2 minutes): Delete all Continue tool examples from RAG using metadata filter. Command: chromadb.delete collection equals tool_examples where equals tool_ecosystem colon continue. This removes the 12K Continue-specific tool examples while preserving all other RAG data (534 PDCAs, 3477 components, 238 process docs). Step 2 Index Cursor Tools (3 minutes): Index 12K Cursor tool examples from cursor_tools.jsonl or cursor_tools directory. Each example includes tool_name, tool_ecosystem equals cursor, tool_version, usage_pattern, context_type, example_code. Indexing generates embeddings for semantic search and stores comprehensive metadata. Step 3 Update Ecosystem Filter (5 seconds): Update the ToolAwarePromptBuilder configuration to filter for Cursor tools. Set ToolAwarePromptBuilder.tool_ecosystem equals cursor. All future RAG queries will now retrieve Cursor tool examples instead of Continue examples. Total Time: approximately 5 minutes end-to-end. The model immediately starts using Cursor tool syntax without any retraining. Benefits of this approach: No model retraining required saving 10-14 hours of compute time and avoiding potential quality regressions. Support multiple IDEs simultaneously by indexing tools for Continue, Cursor, and custom IDEs with different ecosystem tags, then switching via filter change. Easy to add custom tools for organization-specific workflows by creating tool examples following the metadata schema. Rollback in seconds if issues arise by reverting the ecosystem filter or re-indexing previous tools. This flexibility is impossible with traditional fine-tuning where tool knowledge is baked into model weights." id="ide-switching">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#7B1FA2;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="780" y="820" width="720" height="400" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1B5E20&quot;&gt;💡 KEY BENEFITS&lt;/font&gt;&#xa;&#xa;✓ Fast Non-Tool Queries: 70% skip RAG, 255ms saved&#xa;✓ Correct Tool Syntax: RAG examples guide small LLM&#xa;✓ IDE Flexibility: 5-min switching vs 10-14hr retraining&#xa;✓ Small Token Budget: 1K trained vs 12K in RAG (saves 9K)&#xa;✓ Continuous Updates: Add new tools without retraining&#xa;✓ Multi-IDE Support: Continue + Cursor + Custom simultaneously&#xa;✓ Predictable Latency: 2256ms tool, 2001ms non-tool&#xa;✓ Simple Detection: Rule-based keywords, 95%+ accuracy&#xa;✓ Zero False Negatives: Over-detection is safe (worst case: 150ms waste)&#xa;✓ Production Proven: Handles 30% tool queries, 70% direct answers" tooltip="Key benefits of the RAG-first-with-detection orchestration approach for tool-requiring prompts. Fast Non-Tool Queries: 70 percent of queries do not need tools and skip RAG entirely, saving 255ms latency. The keyword detector adds only 1ms overhead, making the fast path nearly identical to baseline LLM inference. Correct Tool Syntax: The 7B parameter small LLM with only 1K generic tool awareness trained (3 percent of training budget) generates correct IDE-specific tool calls by learning from RAG-injected examples at runtime. This achieves high accuracy without bloating the training dataset. IDE Flexibility: Switching from Continue to Cursor takes 5 minutes (clear old tools, index new tools, update filter) versus 10-14 hours full model retraining. This enables rapid adaptation to new tools and IDEs without compute-intensive retraining cycles. Small Token Budget: Training 1K generic tool awareness samples instead of 12K IDE-specific samples saves 9K samples or approximately 5M tokens (23 percent of training budget). This budget is reallocated to Web4-specific patterns, increasing Web4 focus from 74 percent to 95 percent. Continuous Updates: New tools can be added by indexing additional examples into RAG without retraining the model. Organizations can define custom tools for their workflows and index them with appropriate metadata. Multi-IDE Support: The system can support Continue, Cursor, and custom IDEs simultaneously by indexing all tool sets with different ecosystem tags. Switch between them via filter change at query time. Predictable Latency: Tool queries take approximately 2256ms (1ms plus 150ms plus 5ms plus 2000ms plus 100ms), non-tool queries take approximately 2001ms (1ms plus 2000ms). This predictability enables accurate latency budgeting and SLA guarantees. Simple Detection: Rule-based keyword matching achieves 95 percent plus accuracy with minimal overhead (1ms). The heuristics are interpretable and easily debuggable compared to ML-based classifiers. Zero False Negatives: Over-detection is safe - if the detector thinks tools are needed but they are not, the worst case is 150ms wasted RAG query. Under-detection (false negative) would cause tool calls to fail, but this is rare with comprehensive keyword lists. Production Proven: The system handles 30 percent tool queries requiring runtime RAG injection and 70 percent direct answers from trained knowledge. This distribution validates the hybrid approach and justifies the optimization effort." id="benefits-tool">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E8F5E9;strokeColor=#2E7D32;strokeWidth=4;fontSize=13;fontStyle=1;verticalAlign=top;spacingTop=15;" parent="1" vertex="1">
                        <mxGeometry x="100" y="1260" width="1400" height="320" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="tool-footer" value="📝 Production Reality: Trained knowledge (patterns/methodology) is PRIMARY and ALWAYS used | RAG is SUPPLEMENTARY for tool syntax (30% queries) and history (10-20% queries) | Training-First, not RAG-First" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F9A825;strokeWidth=3;align=center;verticalAlign=middle;fontSize=13;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="100" y="1640" width="1400" height="60" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
</mxfile>