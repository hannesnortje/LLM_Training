# ğŸš€ Web4 LoRA Core 10% Phase â€” Train, Package, Deploy

> This document details the 10% â€œhands-onâ€ phase of the Web4 LoRA project:  
> **Training â†’ Packaging â†’ Deployment.**  
> It assumes all data, schema, and evaluation groundwork (the 90%) are already complete.  

---

## âš™ï¸ Overview â€” The 10% Segment

| **Stage** | **What Happens** | **Primary Tools** | **Output Artifact** |
|------------|------------------|-------------------|----------------------|
| **5ï¸âƒ£ Training Orchestration** | Fine-tune the base model with your curated dataset via LoRA adapters. | PyTorch (MPS), Transformers, PEFT, TRL, Accelerate | LoRA adapter weights (`adapter_model.bin`, `adapter_config.json`) |
| **6ï¸âƒ£ Artifact Packaging (Merge â†’ GGUF â†’ Quantize)** | Merge the LoRA adapter with the base model, convert to GGUF, and quantize for fast inference. | llama.cpp tools (`convert-hf-to-gguf.py`, `quantize`) | Quantized `.gguf` model (~4â€“8 GB) |
| **7ï¸âƒ£ Deployment & Runtime** | Load the quantized model into Ollama or Docker Desktop Models for local inference and testing. | Ollama, Docker Desktop, llama.cpp | Ready-to-run model responding to prompts |

---

## ğŸ§  5ï¸âƒ£ Training Orchestration â€” â€œTeaching the Modelâ€

### ğŸ¯ Goal
Infuse the base model (e.g. DeepSeek-Coder 6.7B, Qwen2.5-Coder 7B) with your Web4 framework logic, OOP philosophy, and tool-use intelligence using LoRA adapters.

### ğŸ”© How It Works
1. Load the base model in **float16 (fp16)** on Metal (MPS).  
2. Attach **LoRA adapters** that train only selected attention heads â€” lightweight updates.  
3. Run **Supervised Fine-Tuning (SFT)** with your dataset (Tool-Core, Style-Core, Guardrails).  
4. Track training loss to ensure healthy convergence.  
5. Save only adapter deltas at the end.

### ğŸ§° Tools / Libraries
- **PyTorch (MPS)** for hardware acceleration  
- **Transformers** (â‰¥ 4.44) for model loading  
- **PEFT** for LoRA layers  
- **TRL** for SFT orchestration  
- **Accelerate** for single-device management  

### ğŸ§¾ Typical Outputs
```
outputs/
 â””â”€â”€ run_2025-10-23/
     â”œâ”€â”€ adapter_config.json
     â”œâ”€â”€ adapter_model.bin
     â”œâ”€â”€ trainer_state.json
     â”œâ”€â”€ tokenizer.json
     â””â”€â”€ logs/loss.csv
```

### âœ… Success Criteria
- Loss stabilizes between **0.6â€“1.0**  
- Generations return valid JSON tool calls and stylistically correct TypeScript code  
- No MPS OOM (Out-of-Memory) errors during training  

---

## ğŸ§± 6ï¸âƒ£ Artifact Packaging â€” â€œTurning Training Into a Deployable Modelâ€

### ğŸ¯ Goal
Transform the trained LoRA adapter into a single, portable, efficient `.gguf` model file.

### âš™ï¸ Process
1. **Merge adapters (optional)**  
   Merge LoRA deltas into the base model:  
   ```bash
   peft merge_and_unload
   ```
   Creates a complete `merged_model/` directory.
2. **Convert to GGUF**  
   ```bash
   python3 convert-hf-to-gguf.py merged_model/
   ```  
   Produces `model.gguf` (16â€“20 GB unquantized).
3. **Quantize for efficiency**  
   ```bash
   ./quantize model.gguf model.Q4_K_M.gguf Q4_K_M
   ```  
   Produces 4â€“8 GB model with negligible accuracy loss (~1â€“2%).
4. **Verify integrity**  
   - Run a checksum (`shasum -a 256`)  
   - Compare eval metrics pre- vs post-quantization

### ğŸ§° Tools
- `llama.cpp` utilities (pure CPU, M1-compatible)  
- Python 3.10+ environment  
- Optionally `safetensors` for archive consistency  

### âœ… Success Criteria
- Quantized `.gguf` loads without error  
- Eval scores drop â‰¤2% from pre-quantization  
- Disk footprint reduced by ~75%  

---

## ğŸš€ 7ï¸âƒ£ Deployment & Runtime â€” â€œMaking It Usableâ€

### ğŸ¯ Goal
Serve your trained and quantized model locally for interactive use, testing, or further evaluation.

### âš™ï¸ Deployment Paths

#### ğŸŸ© **Option A: Ollama (Recommended on M1)**
1. Create a `Modelfile`:
   ```text
   FROM ./model.Q4_K_M.gguf
   PARAMETER temperature 0
   TEMPLATE """{{ .System }}\n{{ .Prompt }}"""
   ```
2. Register and run:
   ```bash
   ollama create web4-lora -f Modelfile
   ollama run web4-lora
   ```
3. Test tool-use, code style, and guardrails.

#### ğŸŸ¦ **Option B: Docker Desktop Models**
1. Open Docker Desktop â†’ **Models** â†’ â€œAdd Modelâ€  
2. Import your `.gguf` file  
3. Assign metadata (name, version, description)  
4. Launch local API endpoint for apps to connect  

### ğŸ§ª Post-Deployment Tests

| **Test Type** | **Expected Result** |
|----------------|--------------------|
| Tool-JSON prompt | Returns valid `{ "tool": "...", "args": { ... } }` |
| Style prompt | Generates TypeScript in correct house format |
| Guardrail prompt | Proper `<REFUSAL>` response when required |
| Latency | ~1 token/sec for 7B Q4 model on M1 |
| Memory footprint | ~22â€“26 GB shared memory used |

---

## ğŸ“¦ Deliverables After the 10% Phase

| **Artifact** | **Description** |
|---------------|----------------|
| `adapter_model.bin` | LoRA weights (training result) |
| `merged_model.gguf` | Merged unquantized model |
| `model.Q4_K_M.gguf` | Quantized deployable model |
| `Modelfile` | Ollama runtime configuration |
| `eval_log.md` | Post-training metrics summary |
| `drawio_diagram.png` | Final pipeline visualization |

---

## ğŸ’¡ Why This 10% Matters

- Converts **weeks of data work** into a *real, usable AI model*  
- Establishes a **repeatable training loop**: data â†’ LoRA â†’ GGUF â†’ deploy  
- Keeps everything **local, secure, and verifiable** â€” essential for Web4 governance principles  
- Enables **incremental re-training** (fine-tune only new adapters, not the full base model)

---

## ğŸ§­ Recap

| **Step** | **Goal** | **Tools** | **Outcome** |
|-----------|-----------|------------|--------------|
| **Train** | Teach Web4 philosophy via LoRA | PEFT + TRL on MPS | Adapter weights |
| **Package** | Merge + Quantize for performance | llama.cpp scripts | `.gguf` model |
| **Deploy** | Serve locally in Ollama or Docker | Ollama / Docker | Interactive local AI model |

---

**End of Web4 LoRA Core 10% Phase â€” Train, Package, Deploy**
