# üçè Web4 Training ‚Äî Software Avenues for Mac M1 (32 GB)

| **Phase** | **Purpose** | **Primary Software / Tools (M1-compatible)** | **Optional / Alternatives** | **Notes** |
|------------|-------------|----------------------------------------------|-----------------------------|------------|
| **1Ô∏è‚É£ Data Preparation & QA** | Authoring, validation, token inspection | ‚Ä¢ `datasets` (Hugging Face)<br>‚Ä¢ `pandas`, `pyarrow`<br>‚Ä¢ `jsonschema` (tool-JSON checks)<br>‚Ä¢ `jq`, `yq` CLI tools | ‚Ä¢ `Label Studio` / `Argilla` (manual QA) | Validate JSONL, count tokens, enforce schema consistency. |
| **2Ô∏è‚É£ LoRA Training (fine-tuning)** | Core fine-tuning of base model | ‚Ä¢ **PyTorch with Metal (MPS)** backend<br>‚Ä¢ **transformers** (‚â• 4.44)<br>‚Ä¢ **peft** (LoRA adapter support)<br>‚Ä¢ **trl** (SFTTrainer)<br>‚Ä¢ **accelerate** (device mgmt) | ‚Ä¢ **Unsloth** (experimental faster LoRA on Apple Silicon) | ‚úÖ LoRA only ‚Äî QLoRA (4-bit) not supported natively on M1. |
| **3Ô∏è‚É£ Checkpoint Management** | Organize, resume, version runs | ‚Ä¢ `accelerate` checkpoints<br>‚Ä¢ `safetensors` format | ‚Ä¢ Manual folder versioning / `DVC` | Keep adapters under `outputs/<run_name>/lora`. |
| **4Ô∏è‚É£ Merge ‚Üí GGUF ‚Üí Quantize** | Create deployable artifacts | ‚Ä¢ **llama.cpp** tools:<br>  ‚Äì `convert-hf-to-gguf.py`<br>  ‚Äì `quantize` | ‚Ä¢ **llama.cpp finetune** (very small LoRA) | CPU-based; works well on M1. Use Q4_K_M or Q5_K_M. |
| **5Ô∏è‚É£ Serving / Runtime** | Run and test the model | ‚Ä¢ **Ollama** (macOS native)<br>‚Ä¢ **Docker Desktop Models** (Model Runner tab)<br>‚Ä¢ `llama.cpp server` (optional HTTP) | ‚Ä¢ `text-generation-webui` (Python, heavier) | Ollama supports both merged GGUF and `ADAPTER` stacking. |
| **6Ô∏è‚É£ Evaluation & Scoring** | Measure Tool / Style / Guardrail metrics | ‚Ä¢ `evaluate.py` (custom)<br>‚Ä¢ `jsonschema`<br>‚Ä¢ **ESLint**, **Prettier**, **TypeScript (tsc)**<br>‚Ä¢ **tree-sitter** / **esprima** for AST | ‚Ä¢ `promptfoo` (visual dashboards) | All work natively; AST & linting are CPU-only. |
| **7Ô∏è‚É£ Visualization & Docs** | Diagrams, notes, dashboards | ‚Ä¢ **draw.io**, **Mermaid**, **Obsidian/Markdown** | ‚Ä¢ **MkDocs** / **Docusaurus** | Keep pipeline, eval charts, and reports. |
| **8Ô∏è‚É£ Environment & Orchestration** | Reproducible envs | ‚Ä¢ **Python 3.10+ venv**<br>‚Ä¢ **pip tools** or **poetry** | ‚Ä¢ **Docker** (for training sandbox) | Stick to Metal-backed PyTorch wheels. |
| **9Ô∏è‚É£ Security / Compliance** | Sanitize data, version control | ‚Ä¢ `git-secrets`, `trufflehog` | ‚Ä¢ `syft` (SBOMs if containerizing) | Optional but recommended before publishing. |

---

## ‚úÖ Summary: Recommended Baseline Stack (M1)

| Category | Software |
|-----------|-----------|
| **Language Runtime** | Python 3.10+ |
| **Core Libraries** | PyTorch (MPS), Transformers, PEFT, TRL, Accelerate |
| **Data** | Datasets, pandas, jsonschema |
| **Conversion / Packaging** | llama.cpp |
| **Serving** | Ollama or Docker Desktop Models |
| **Evaluation** | ESLint, Prettier, TypeScript, tree-sitter, jsonschema |
| **Docs & Tracking** | Markdown + draw.io + git |
| **Optional Enhancements** | Unsloth (faster LoRA), promptfoo (eval UI), DVC (data versioning) |

---

### ‚öôÔ∏è Key Constraints & Notes
- **QLoRA** (4-bit) not yet practical on M1 ‚Äî LoRA (16-bit fp16/bf16) is the stable path.  
- Use **Metal backend** (`device="mps"`) for acceleration.  
- Quantization is post-training (CPU-friendly); M1 handles **Q4_K_M / Q5_K_M** easily.  
- For larger models or continued LoRA stacking, migrate later to a Linux/CUDA node.  

---

**End of Web4 ‚Äî M1 Software Stack Overview**
