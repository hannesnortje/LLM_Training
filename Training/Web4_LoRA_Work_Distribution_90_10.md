# üß≠ Web4 LoRA Project ‚Äî Work Distribution (90% / 10% Framework)

> üí° *90% of the work is planning, data, validation, and evaluation.*  
> *10% is the actual training and serving ‚Äî the part that looks like ‚ÄúAI‚Äù.*  

This gives you a clear management and progress-tracking overview.

---

| **Phase** | **Focus Area** | **Main Tasks** | **Approx. Share** | **Difficulty** | **Expected Outcome** |
|------------|----------------|----------------|-------------------|----------------|----------------------|
| **1Ô∏è‚É£ Data Foundation** | Dataset Design & Quality | - Define JSONL schemas (Tool-Core, Style, Guardrail)<br>- Validate every line (`jsonschema`, `jq`)<br>- Balance positive & negative examples<br>- Token count analysis (max seq length) | **25%** | Easy ‚Üí Moderate | Clean, schema-valid dataset; ready for LoRA training |
| **2Ô∏è‚É£ Template Governance** | Prompt & Format Consistency | - Define canonical chat template<br>- Align train / eval / serve formats<br>- Freeze system + user roles<br>- Document exact examples | **10%** | Moderate | Stable and deterministic prompt alignment |
| **3Ô∏è‚É£ Evaluation Infrastructure** | Metrics & Benchmarks | - Build `evaluate.py` harness<br>- JSON validation, schema checks<br>- AST/lint scoring for code style<br>- Guardrail refusal tests | **20%** | Moderate ‚Üí Hard | Repeatable quantitative evaluation of model quality |
| **4Ô∏è‚É£ Data QA Loop** | Continuous Curation | - Iterate dataset improvements<br>- Add counterexamples (Tool-Neg)<br>- Refine guardrails<br>- Track token growth vs results | **15%** | Moderate | Ongoing dataset refinement; reduced overfit and bias |
| **5Ô∏è‚É£ Training Orchestration** | LoRA Configuration & Run | - Configure TRL+PEFT on MPS<br>- Tune LR, batch size, epochs<br>- Run dry test (100 samples)<br>- Run full fine-tune (25‚Äì35k samples) | **5%** | Moderate | Stable adapter model trained on M1 |
| **6Ô∏è‚É£ Artifact Packaging** | Merge & Quantize | - Merge LoRA with base HF model<br>- Convert to GGUF<br>- Quantize (Q4_K_M / Q5_K_M)<br>- Validate checksum | **2%** | Easy | Compact `.gguf` artifact ready for serving |
| **7Ô∏è‚É£ Deployment & Runtime** | Serving & Testing | - Create Ollama Modelfile<br>- Load model locally<br>- Run smoke tests with Eval set<br>- Optionally package in Docker Desktop Models | **3%** | Easy | Locally deployable, quantized model responding correctly |
| **8Ô∏è‚É£ Documentation & Reporting** | Knowledge Capture | - Maintain Markdown logs<br>- Diagram full pipeline (draw.io)<br>- Record eval metrics over time<br>- Write model card summary | **10%** | Easy | Transparent, reproducible documentation |
| **9Ô∏è‚É£ Continuous Improvement** | Governance & Scaling | - Add CI linting<br>- Automate eval & timing<br>- Plan migration to Linux/CUDA (optional)<br>- Improve eval metrics over versions | **10%** | Moderate | Sustainable, repeatable training process |

---

## üßÆ Effort Split Overview

| **Category** | **What it covers** | **Effort Share** | **Purpose** |
|---------------|--------------------|------------------|--------------|
| **90% ‚Äî Planning, Data, Evaluation, Documentation** | Phases 1‚Äì4, 8‚Äì9 | **‚âà90% of total effort** | Build quality foundation and trust in results. |
| **10% ‚Äî Training, Conversion, Serving** | Phases 5‚Äì7 | **‚âà10% of total effort** | Run the model fine-tuning and deploy the artifact. |

---

## üß© Summary

- **Data and evaluation dominate the workload** ‚Äî you‚Äôre teaching the model your worldview, not writing ML code.  
- **Training and serving are quick once inputs and metrics are correct.**  
- The most *intellectual* part is designing *what correctness means* ‚Äî the evaluation harness.  
- The most *mechanical* part is running `trl.train()`, `llama.cpp quantize`, and `ollama create`.  

This distribution ensures your LoRA fine-tuning pipeline is **accurate, reproducible, and efficient** on a single Mac M1.

---

**End of Web4 LoRA Project ‚Äî Work Distribution (90% / 10% Framework)**
