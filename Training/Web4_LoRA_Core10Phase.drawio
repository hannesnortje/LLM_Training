<mxfile host="65bd71144e">
    <diagram id="Web4_LoRA_Core10Phase" name="Web4 LoRA Core 10% Phase">
        <mxGraphModel dx="1308" dy="711" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="1400" pageHeight="1200" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="title" value="Web4 LoRA Core 10% Phase — Train, Package, Deploy" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=24;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="300" y="20" width="800" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="subtitle" value="The 10% hands-on phase: Converting data preparation into a deployed model" style="text;html=1;strokeColor=none;fillColor=#E1E1E1;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=14;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="350" y="70" width="700" height="30" as="geometry"/>
                </mxCell>
                <object label="🧠 5️⃣ TRAINING ORCHESTRATION&#xa;Teaching the Model" tooltip="Training Orchestration (Phase 5 of Web4 LoRA Core 10% Phase) is where the base model learns Web4 framework conventions through Parameter-Efficient Fine-Tuning using LoRA (Low-Rank Adaptation) adapters. This phase infuses the base model with Web4&#39;s distinctive patterns: 5-layer OOP architecture, interface-driven design, Vitest testing conventions, and tool-use intelligence. The training process loads the base model (DeepSeek-Coder 6.7B or Qwen2.5-Coder 7B) in float16 precision on Apple Silicon Metal Performance Shaders (MPS), attaches LoRA adapters to selected attention heads for lightweight parameter updates, and runs Supervised Fine-Tuning (SFT) with curated Web4 datasets (Tool-Core for JSON tool calling, Style-Core for component generation patterns, Guardrails for security refusals, Style-Refactor for code improvement). The LoRA approach is critical for M1 Mac hardware constraints - instead of updating all 6-7 billion model parameters (requiring 100+ GB memory), LoRA trains only adapter matrices (typically 50-200MB) that modify attention layers. This enables fine-tuning on consumer hardware with 32GB unified memory while achieving 90%+ of full fine-tuning quality. Training typically runs for 1-2 epochs over 12-20 hours, monitoring loss convergence (target: 0.6-1.0 final loss) and avoiding overfitting through early stopping. The output is a set of adapter weights (adapter_model.bin ~100MB) that augment the base model without modifying it, enabling quick experimentation with different adapter versions. Training uses TRL&#39;s SFTTrainer with gradient accumulation (12-16 steps) to simulate larger batch sizes, learning rate warmup and cosine decay scheduling, and automatic mixed precision training for MPS optimization. Quality validation ensures loss curves show healthy convergence without spikes or divergence, generated samples produce valid TypeScript code following Web4 patterns, JSON tool calls are properly formatted, and no MPS out-of-memory errors occur. Success metrics include loss stabilization between 0.6-1.0, valid JSON tool call generation, correct TypeScript style adherence, comprehensive error handling patterns, and stable training without hardware issues. This phase is the core teaching moment where the model internalizes Web4 philosophy, transforming a generic code model into a Web4-specialized component generator." id="phase5_container">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FF;strokeColor=#0078D7;strokeWidth=3;verticalAlign=top;align=left;spacingLeft=10;spacingTop=5;fontSize=18;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="50" y="130" width="400" height="450" as="geometry"/>
                    </mxCell>
                </object>
                <object label="GOAL: Infuse base model with Web4 framework logic via LoRA" tooltip="The goal of Training Orchestration is to infuse the base model with Web4 framework-specific knowledge using LoRA (Low-Rank Adaptation) adapters rather than full fine-tuning. LoRA trains only small adapter matrices that modify attention layer behavior, achieving 90%+ of full fine-tuning quality while using 50-100x less memory and compute. This enables training on M1 Mac hardware with 32GB unified memory instead of requiring datacenter GPUs (Graphics Processing Units). The base model (DeepSeek-Coder 6.7B or Qwen2.5-Coder 7B) already understands general programming but needs Web4-specific patterns: 5-layer OOP (Object-Oriented Programming) architecture with proper layer separation, interface-driven design with TypeScript generics, Vitest testing conventions and coverage thresholds, JSON (JavaScript Object Notation) tool calling format and structure, component documentation standards, and security guardrails for refusing unsafe requests. Training with curated Web4 datasets teaches these patterns through supervised learning, where the model learns to predict Web4-compliant code given Web4-style prompts. The LoRA approach maintains the base model&#39;s general capabilities while specializing it for Web4, enabling quick adapter swapping to test different training configurations without retraining the entire model. Success means the model can generate Web4 components that compile, pass tests, follow framework conventions, and integrate seamlessly with existing Web4 codebases without manual refactoring." id="phase5_goal">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFFFCC;strokeColor=#D6B656;fontSize=11;fontStyle=2" parent="1" vertex="1">
                        <mxGeometry x="70" y="190" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="📊 Input Data" tooltip="Input Data for Training Orchestration consists of four carefully curated JSONL (JSON Lines - one JSON object per line) datasets totaling approximately 19,500 training examples (~12M tokens) optimized for Web4 component generation. Tool-Core dataset teaches JSON (JavaScript Object Notation) tool calling patterns where the model learns to respond with structured function calls rather than plain text, enabling integration with automated workflows and CI/CD (Continuous Integration/Continuous Deployment) pipelines. Examples demonstrate proper tool invocation syntax, parameter validation, error handling in tool responses, and multi-step tool orchestration. Style-Core dataset (largest bucket at 77% of training data) teaches complete Web4 component generation including CLI (Command Line Interface) components with argument parsing, package.json generation with framework dependencies, TypeScript configuration with strict settings, 5-layer OOP (Object-Oriented Programming) implementation with proper interfaces, comprehensive Vitest test suites with high coverage, and complete documentation with README.md files. This bucket provides the foundation for the model to generate production-ready components from scratch. Guardrails dataset teaches the model when and how to refuse inappropriate requests using the &lt;REFUSAL&gt; pattern, covering security violations (hardcoded credentials, bypassing type safety), framework non-compliance (using Jest instead of Vitest, incorrect versioning), and data protection issues (logging sensitive data, insecure storage). Style-Refactor dataset demonstrates component evolution through version upgrades, architectural improvements (procedural to OOP conversion), and performance optimizations with measurable results. All datasets follow strict quality standards: 100% JSONL format compliance, 100% schema validation with required fields, 95%+ TypeScript compilation success, 95%+ Web4 framework compliance, and comprehensive test coverage. The training data underwent rigorous validation to ensure every example teaches correct patterns, not anti-patterns or hallucinations." id="input_data">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#D5E8D4;strokeColor=#82B366;fontStyle=1" parent="1" vertex="1">
                        <mxGeometry x="70" y="235" width="120" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <object label="• Tool-Core&#xa;• Style-Core&#xa;• Guardrails&#xa;• Style-Refactor" tooltip="The four training dataset categories represent different aspects of Web4 competency. Tool-Core enables JSON (JavaScript Object Notation) tool calling and structured output generation, teaching the model to respond with executable function calls rather than conversational text. Style-Core teaches complete component generation patterns including proper architecture, testing, and documentation - this is the largest bucket at 77% because the model needs massive repetition to internalize Web4 conventions reliably. Guardrails teaches security and compliance through refusal patterns, ensuring the model never generates insecure code even when explicitly requested. Style-Refactor teaches code improvement and evolution patterns, demonstrating how to modernize components while maintaining backward compatibility. This balanced distribution (77% creation, 15% refactoring, 5% guardrails, 3% evaluation) follows cognitive science research showing 70%+ pattern repetition is required for reliable learning. Each category undergoes identical quality validation to ensure consistent training signal across all dataset types." id="data_list">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#82B366;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="200" y="235" width="230" height="70" as="geometry"/>
                    </mxCell>
                </object>
                <object label="🤖 Base Model" tooltip="The base model selection is critical for Web4 LoRA (Low-Rank Adaptation) training success. Two strong candidates are evaluated: DeepSeek-Coder 6.7B and Qwen2.5-Coder 7B. DeepSeek-Coder 6.7B is trained specifically for code generation with strong multi-language support, excellent fill-in-the-middle capabilities for code completion, good performance on HumanEval and MBPP (Mostly Basic Python Problems) benchmarks, and efficient inference on consumer hardware. It&#39;s optimized for coding tasks with context-aware code generation. Qwen2.5-Coder 7B (Qwen family) offers strong instruction-following capabilities, excellent reasoning for architectural decisions, good multilingual support including Chinese documentation, and proven fine-tuning stability across various domains. Both models are loaded in float16 (fp16 - 16-bit floating point) precision to balance memory usage and quality on M1 Mac hardware with Metal Performance Shaders (MPS) acceleration. Float16 uses half the memory of float32 while maintaining sufficient precision for training. The model comparison during Week 6 training evaluates both candidates with identical hyperparameters and datasets, measuring component generation accuracy, framework compliance, refusal behavior for guardrails, training stability and convergence, and final model size after adapter merge. The champion model becomes the production Web4 component generator. Loading in fp16 enables fitting 6-7B parameter models in 32GB unified memory with room for activation storage during training, a critical constraint for consumer hardware fine-tuning." id="base_model">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#DAE8FC;strokeColor=#6C8EBF;fontStyle=1" parent="1" vertex="1">
                        <mxGeometry x="70" y="315" width="120" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <object label="• DeepSeek-Coder 6.7B&#xa;• Qwen2.5-Coder 7B&#xa;(loaded in fp16)" tooltip="DeepSeek-Coder 6.7B and Qwen2.5-Coder 7B represent the current generation of code-specialized language models optimized for software development tasks. DeepSeek-Coder is specifically trained on massive code repositories with fill-in-the-middle pre-training enabling it to complete code segments naturally, strong performance on code benchmarks (HumanEval 70%+, MBPP (Mostly Basic Python Problems) 75%+), multilingual code support for TypeScript, Python, JavaScript, and more, and efficient architecture enabling fast inference on consumer GPUs (Graphics Processing Units) and Apple Silicon. Qwen2.5-Coder brings excellent instruction-following from the Qwen family&#39;s chat fine-tuning, strong reasoning capabilities for explaining architectural decisions, good context utilization with 32K token windows, and proven stability during fine-tuning with LoRA (Low-Rank Adaptation) adapters. Both models are loaded in fp16 precision (16-bit floating point numbers) which uses 2 bytes per parameter instead of 4 bytes (fp32 - 32-bit floating point), enabling a 7B parameter model to fit in ~14GB memory plus overhead rather than 28GB+. This memory efficiency is crucial for M1 Mac training where 32GB unified memory must accommodate model weights, optimizer states, gradients, and activations simultaneously. The fp16 precision maintains sufficient numeric accuracy for training (gradients down to 6e-5 are representable) while enabling practical fine-tuning on consumer hardware. During Week 6 training, both models undergo identical training runs with same hyperparameters, datasets, and hardware to enable fair comparison and selection of the champion model for production deployment." id="model_examples">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#6C8EBF;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="200" y="315" width="230" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <object label="🧰 Tools/Libraries" tooltip="The training technology stack combines industry-standard libraries optimized for efficient LoRA (Low-Rank Adaptation) fine-tuning on Apple Silicon. PyTorch with Metal Performance Shaders (MPS) backend enables GPU (Graphics Processing Unit) acceleration on M1 Mac hardware, utilizing unified memory architecture for efficient data transfer between CPU (Central Processing Unit) and GPU, supporting float16 mixed precision training for memory efficiency, and providing automatic memory management to prevent OOM (Out-of-Memory) errors. MPS delivers 5-10x speedup over CPU training. Transformers library (Hugging Face, version 4.44+) provides model loading and tokenization with AutoModelForCausalLM and AutoTokenizer for base model initialization, generation utilities for inference and sampling, and trainer infrastructure for training loops and evaluation. PEFT (Parameter-Efficient Fine-Tuning) implements LoRA adapters through low-rank matrix decomposition (r=8, alpha=16 typical), selective layer targeting for attention modules only, and adapter merging utilities to combine adapters with base models. TRL (Transformer Reinforcement Learning) provides SFTTrainer for supervised fine-tuning with automatic dataset formatting, gradient accumulation for memory-efficient training, and loss computation and metric tracking. Accelerate handles device management and distributed training infrastructure, automatic mixed precision training configuration, and memory optimization strategies. This stack enables training 7B models on M1 Mac with 32GB RAM (Random Access Memory) through careful memory management, efficient MPS utilization, and LoRA&#39;s parameter efficiency. Alternative stacks like llama.cpp or MLX were considered but lack the ecosystem maturity for production fine-tuning workflows, while cloud TPU (Tensor Processing Unit)/GPU training sacrifices the local development advantages critical for Web4&#39;s decentralized philosophy." id="training_tools">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F8CECC;strokeColor=#B85450;fontStyle=1" parent="1" vertex="1">
                        <mxGeometry x="70" y="385" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="PyTorch (MPS) • Transformers • PEFT • TRL • Accelerate" tooltip="The five core libraries work together to enable efficient LoRA (Low-Rank Adaptation) fine-tuning on M1 hardware. PyTorch with Metal Performance Shaders (MPS) backend leverages Apple Silicon&#39;s unified memory and GPU (Graphics Processing Unit) cores for accelerated tensor operations, automatic memory optimization, and native float16 support. Transformers 4.44+ provides the model infrastructure with pretrained model loading, tokenizer configuration, generation and sampling methods, and training loop integration. PEFT (Parameter-Efficient Fine-Tuning) implements LoRA through rank decomposition matrices that modify attention layers without updating original model weights, typical configuration uses rank r=8 (adapter size) with alpha=16 (scaling factor) and dropout=0.05 for regularization, targeting only query and value projection layers to minimize trainable parameters (~50-200MB adapters vs 14GB full model). TRL&#39;s (Transformer Reinforcement Learning) SFTTrainer simplifies supervised fine-tuning with automatic batch padding and formatting, gradient accumulation to simulate larger batches, learning rate scheduling with warmup and decay, and checkpoint saving for recovery. Accelerate provides cross-device compatibility, memory profiling and optimization, and mixed precision training configuration. Together these libraries enable training that would typically require datacenter GPUs to run on consumer hardware through parameter-efficient techniques (LoRA), memory optimization (fp16 - 16-bit floating point, gradient accumulation), and hardware-specific acceleration (MPS), achieving training throughput of 0.5-1.0 iterations per second on M1 Max with reasonable convergence in 12-20 hours for the full 19,500-example dataset." id="tools_list">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#B85450;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="70" y="420" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="📦 Output: LoRA Adapter Weights" tooltip="Training Orchestration outputs LoRA (Low-Rank Adaptation) adapter weights as the primary artifact - lightweight parameter updates that augment the base model without modifying it. The adapter weights are stored in efficient formats for downstream usage. adapter_model.bin (or safetensors format) contains the trained LoRA matrices in binary format, typically 50-200MB in size (0.3-1% of base model size), using float16 (16-bit floating point) precision for memory efficiency, organized by layer and attention head for selective application. This small size enables rapid experimentation with different adapter configurations without retraining the base model. adapter_config.json (JSON - JavaScript Object Notation) stores the LoRA hyperparameters used during training including rank (r=8), alpha scaling factor (16), dropout rate (0.05), target modules (query, value projections), base model identifier for compatibility checking, and task type (CAUSAL_LM - Causal Language Model for code generation). This configuration enables reproducible adapter loading and validation. trainer_state.json records the training history with loss curves and metrics over time, learning rate schedule and optimization state, best checkpoint selection criteria, and epoch/step counts for debugging. tokenizer files (tokenizer.json, special_tokens_map.json) ensure consistency between training and inference tokenization. The lightweight nature of these artifacts (total ~100-300MB) enables version control, rapid testing of different adapter configurations, adapter composition and merging strategies, and efficient distribution to users without transferring multi-gigabyte base models. The next phase (Artifact Packaging) merges these adapters with the base model, converts to GGUF (GPT-Generated Unified Format) format for efficient inference with llama.cpp, and quantizes to Q4_K_M (4-bit K-quantization Medium) for 75% size reduction while maintaining 98%+ quality." id="training_output">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE6CC;strokeColor=#D79B00;fontStyle=1" parent="1" vertex="1">
                        <mxGeometry x="70" y="460" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="adapter_model.bin • adapter_config.json • trainer_state.json" tooltip="The three critical output files from training orchestration form a complete training artifact package. adapter_model.bin contains the actual trained LoRA (Low-Rank Adaptation) weight matrices stored in PyTorch binary format or safetensors format (preferred for security and speed), typically 80-150MB for r=8 adapters on 7B models, using float16 (16-bit floating point) precision matching training precision, and organized hierarchically by model layer, module type, and weight matrix (query, value, key projections). The file is platform-independent and can be loaded on different hardware. adapter_config.json is a human-readable JSON (JavaScript Object Notation) file specifying the exact LoRA configuration: base_model_name_or_path identifies which model these adapters augment, peft_type confirms &#39;LORA&#39; adapter type, r (rank) defines adapter capacity (8 is common, higher = more capacity but larger files), lora_alpha sets the scaling factor (typically 2*r = 16 for r=8), lora_dropout provides regularization (0.05 = 5% dropout), target_modules lists which attention layers to modify (typically &#39;q_proj&#39; and &#39;v_proj&#39; for query and value projections), task_type specifies &#39;CAUSAL_LM&#39; (Causal Language Model) for autoregressive generation, and inference_mode=false during training. This config enables exact reproduction of the adapter&#39;s behavior. trainer_state.json tracks training progress: global_step and epoch counters, best_model_checkpoint path for the lowest loss checkpoint, log_history array with loss values over time, learning rate schedule and actual rates used, and early stopping criteria if training terminated early. Together these files enable adapter loading with PEFT (Parameter-Efficient Fine-Tuning), validation of training convergence, debugging training issues, and reproduction of training runs. They&#39;re tracked in version control alongside source code, enabling experimentation with different adapter configurations while maintaining base model compatibility." id="training_files">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#D79B00;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="70" y="495" width="360" height="25" as="geometry"/>
                    </mxCell>
                </object>
                <object label="🧱 6️⃣ ARTIFACT PACKAGING&#xa;Turning Training Into a Deployable Model" tooltip="Artifact Packaging (Phase 6 of Web4 LoRA Core 10% Phase) transforms the trained LoRA (Low-Rank Adaptation) adapter weights from Phase 5 into a single, portable, production-ready model file optimized for efficient inference. This phase bridges the gap between training artifacts and deployment-ready models through a three-step process: merging LoRA adapters with the base model to create a unified model, converting from PyTorch/Hugging Face format to GGUF (GPT-Generated Unified Format) for llama.cpp compatibility, and quantizing to reduce model size by 75% while maintaining 98%+ accuracy. The process starts with lightweight adapter files (~100-200MB) and produces a compact quantized model (4-8GB) ready for local inference on consumer hardware. Step 1 (Merge Adapters) uses PEFT (Parameter-Efficient Fine-Tuning) library&#39;s merge_and_unload functionality to integrate LoRA weight updates into the base model parameters, creating a complete standalone model in Hugging Face format stored in the merged_model/ directory (typically 16-20GB unquantized). Step 2 (Convert to GGUF) uses llama.cpp&#39;s convert-hf-to-gguf.py script to transform the Hugging Face model into GGUF format, a binary format optimized for fast loading and efficient inference with CPU (Central Processing Unit) and Apple Silicon, producing model.gguf file (16-20GB). Step 3 (Quantize for Efficiency) applies k-quantization to compress the model from 16-bit to 4-bit precision using the Q4_K_M (4-bit K-quantization Medium) scheme, reducing file size from 16-20GB to 4-8GB (75% reduction) with negligible quality loss (1-2% accuracy degradation). The quantization process groups weights intelligently, preserving critical high-precision values while compressing less sensitive parameters aggressively. Quality validation throughout packaging ensures the final model loads without errors, passes checksum verification, maintains evaluation scores within 2% of pre-quantization performance, and demonstrates proper inference behavior. The resulting .gguf file is platform-independent, runs efficiently on M1 Mac hardware without GPU (Graphics Processing Unit) requirements, loads quickly from disk (2-3 seconds vs 30+ seconds for unquantized), and provides inference speeds of ~1 token/sec on consumer hardware. This packaging workflow is reproducible and versioned, enabling consistent model deployment across development, testing, and production environments. The entire process runs locally on M1 Mac using only CPU resources, taking approximately 30-60 minutes to complete all three steps." id="phase6_container">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF4E6;strokeColor=#FF8C00;strokeWidth=3;verticalAlign=top;align=left;spacingLeft=10;spacingTop=5;fontSize=18;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="500" y="130" width="400" height="450" as="geometry"/>
                    </mxCell>
                </object>
                <object label="GOAL: Transform LoRA adapter into single, portable, efficient .gguf" tooltip="The goal of Artifact Packaging is to transform the lightweight LoRA (Low-Rank Adaptation) adapter weights from training into a single, self-contained, deployment-ready model file in GGUF (GPT-Generated Unified Format) format. The training phase produces adapter files (~100-200MB) that modify a base model&#39;s behavior but cannot run independently - they must be combined with the original base model for inference. Artifact Packaging solves this by merging adapters with the base model, creating a complete unified model that includes all trained knowledge. The .gguf format is chosen for its efficiency advantages: optimized binary layout for fast memory mapping and loading (2-3 seconds vs 30+ for PyTorch), efficient quantization support enabling 75% size reduction, excellent CPU (Central Processing Unit) inference performance without requiring GPU (Graphics Processing Unit), and cross-platform compatibility (M1 Mac, Linux, Windows). The packaging process is deterministic and reproducible, ensuring identical output given the same inputs, enabling proper versioning and deployment workflows. The resulting portable model file can be distributed as a single artifact (4-8GB), loaded quickly on target hardware, runs efficiently without external dependencies beyond llama.cpp runtime, and maintains quality within 2% of the original unquantized model. This portability is critical for Web4&#39;s decentralized philosophy where models must run locally on consumer hardware without cloud dependencies. The efficient .gguf format enables deployment scenarios that would be impractical with unquantized models: running on laptops without discrete GPUs, serving multiple inference requests on modest hardware, distributing models via standard package managers, and enabling offline AI capabilities." id="phase6_goal">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFFFCC;strokeColor=#D6B656;fontSize=11;fontStyle=2" parent="1" vertex="1">
                        <mxGeometry x="520" y="190" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="STEP 1: Merge Adapters" tooltip="Step 1 (Merge Adapters) integrates the trained LoRA (Low-Rank Adaptation) weight deltas into the base model to create a complete standalone model. During training, LoRA only modifies small adapter matrices (rank r=8 typically) attached to attention layers, keeping the base model unchanged. For deployment, these adapters must be merged back into the base model parameters so the model can run independently without adapter loading overhead. The merge process uses PEFT (Parameter-Efficient Fine-Tuning) library&#39;s merge_and_unload() method which mathematically combines adapter weights with original model weights: for each attention layer, the LoRA update (delta_W = B @ A where B and A are low-rank matrices) is added to the original weight matrix W_new = W_original + alpha * delta_W. This creates a full-precision model containing all learned patterns from fine-tuning. The merged model is saved in Hugging Face Transformers format in the merged_model/ directory, typically 16-20GB for 7B parameter models in float16 (16-bit floating point) precision. This directory contains model shards (pytorch_model.bin or model.safetensors files split into chunks for large models), configuration files (config.json specifying model architecture), tokenizer files (tokenizer.json, tokenizer_config.json), and generation config (generation_config.json with default parameters). The merge is lossless - the resulting model mathematically equivalent to loading the base model with adapters but with faster inference since no runtime adapter computation is needed. Verification ensures all model parameters are present, no NaN (Not a Number) or Inf (Infinity) values exist, configuration matches original architecture, and test inferences produce expected outputs. This merged model serves as the foundation for the next conversion step to GGUF format." id="step1_merge">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#9C27B0;fontStyle=1;fontSize=11" parent="1" vertex="1">
                        <mxGeometry x="520" y="235" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="peft merge_and_unload&#xa;→ merged_model/ directory" tooltip="The peft merge_and_unload command from the PEFT (Parameter-Efficient Fine-Tuning) library performs the mathematical integration of LoRA (Low-Rank Adaptation) adapters into the base model. The command loads the base model and adapter weights, computes the full weight update for each modified layer using the formula W_new = W_base + (lora_alpha / r) * (B @ A) where B and A are the LoRA matrices and r is the rank, applies these updates to create modified model parameters, and saves the resulting complete model to the merged_model/ directory. The output directory structure follows Hugging Face conventions: pytorch_model.bin or model.safetensors files containing the actual neural network weights (safetensors format preferred for faster loading and better security), config.json defining model architecture (hidden size, number of layers, attention heads, vocabulary size), tokenizer files (tokenizer.json with vocabulary, special tokens, and encoding logic), generation_config.json with default inference parameters (temperature, top_p, max length), and README.md with model metadata. For a 7B parameter model in fp16 (16-bit floating point) precision, the merged_model/ directory typically consumes 14-16GB (7 billion parameters × 2 bytes per parameter) plus overhead for tokenizer and config files. The merge operation is CPU-bound and memory-intensive, requiring enough RAM (Random Access Memory) to hold both the base model and adapters simultaneously (~20GB for 7B models). The process typically completes in 5-10 minutes on M1 Mac hardware. The resulting merged model is a standard Hugging Face model that can be used with transformers library for inference or further processing, no longer requiring the separate adapter files." id="merge_details">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#9C27B0;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="520" y="270" width="360" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <object label="STEP 2: Convert to GGUF" tooltip="Step 2 (Convert to GGUF) transforms the Hugging Face format merged model into GGUF (GPT-Generated Unified Format), the native format for llama.cpp inference engine. GGUF is a binary format designed for efficient model loading and inference on CPU (Central Processing Unit) and Apple Silicon hardware. The conversion uses convert-hf-to-gguf.py script from llama.cpp repository, which reads the Hugging Face model files (pytorch_model.bin or safetensors), extracts model architecture and parameters, maps Hugging Face tensor names to GGUF equivalents, converts weight tensors to GGUF format, embeds tokenizer vocabulary and configuration, and writes the complete model to a single model.gguf file. GGUF format advantages include single-file distribution (no directory structure required), memory-mapped loading for fast startup (model loaded directly from disk without copying to RAM (Random Access Memory)), optimized tensor layouts for CPU inference, native quantization support for various precision levels, and cross-platform compatibility (works on M1 Mac, x86_64 Linux/Windows, ARM devices). The conversion process is lossless - the GGUF model produces identical outputs to the Hugging Face model at full precision. The unquantized model.gguf file is typically 16-20GB for 7B parameter models, matching the merged_model/ directory size. Conversion requires sufficient disk space for both input (merged_model/) and output (model.gguf) simultaneously. The script runs on CPU and completes in 10-15 minutes for 7B models. Verification ensures tensor dimensions match source model, vocabulary matches tokenizer, test prompts produce expected outputs, and file integrity verified via checksum. The resulting GGUF file is ready for quantization (next step) or can be used directly for inference with llama.cpp or Ollama." id="step2_convert">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#9C27B0;fontStyle=1;fontSize=11" parent="1" vertex="1">
                        <mxGeometry x="520" y="320" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="python3 convert-hf-to-gguf.py&#xa;→ model.gguf (16-20 GB unquantized)" tooltip="The python3 convert-hf-to-gguf.py command executes the conversion script from llama.cpp that transforms Hugging Face models to GGUF (GPT-Generated Unified Format). The script is invoked with the path to the merged_model/ directory as input. The conversion process performs several operations: loads model architecture from config.json to determine layer structure, attention patterns, and dimensions, reads weight tensors from pytorch_model.bin or safetensors files, maps Hugging Face layer names to GGUF tensor names (e.g., model.layers.0.self_attn.q_proj.weight becomes blk.0.attn_q.weight), converts tensors to GGUF format with proper data type and shape metadata, extracts tokenizer vocabulary and special tokens from tokenizer.json, embeds metadata like model name, parameter count, and architecture in GGUF headers, and writes everything to a single model.gguf file using memory-mapped I/O (Input/Output) for efficiency. The output file model.gguf contains the complete unquantized model at full fp16 (16-bit floating point) precision, typically 16-20GB for 7B parameter models (7 billion × 2 bytes = 14GB core weights + embeddings and layer norms). The GGUF format uses a structured binary layout: file header with magic bytes and version, metadata section with key-value pairs (model name, architecture, quantization type), tensor info section with names, dimensions, and data types, and tensor data section with actual model weights in contiguous memory layout. This single-file format enables memory-mapped loading where the model is accessed directly from disk without loading entirely into RAM, significantly speeding up model initialization (2-3 seconds vs 30+ for PyTorch). The unquantized GGUF serves as the high-quality reference for the next quantization step." id="convert_details">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#9C27B0;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="520" y="355" width="360" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <object label="STEP 3: Quantize for Efficiency" tooltip="Step 3 (Quantize for Efficiency) compresses the unquantized GGUF (GPT-Generated Unified Format) model from 16-bit to 4-bit precision using advanced quantization techniques, achieving 75% size reduction with minimal quality loss. Quantization reduces model size and memory bandwidth requirements, enabling efficient inference on consumer hardware. The process uses llama.cpp&#39;s quantize utility which implements k-quantization algorithms that group weights intelligently rather than uniformly quantizing all parameters. The Q4_K_M (4-bit K-quantization Medium) scheme balances quality and compression: attention query/key/value weights use 4-bit quantization with medium accuracy (important but robust to quantization), attention output and FFN (Feed-Forward Network) down projection use 5-bit for slightly better quality, FFN up and gate projections use 6-bit to preserve critical path accuracy, and embedding and output layers remain at higher precision to maintain token quality. This mixed-precision approach preserves model accuracy better than uniform quantization. The quantization algorithm computes per-block scales and zero points (typically 32 or 64 weights per block), quantizes weights to 4-bit integers using computed scales, stores scale factors at higher precision for reconstruction, and optimizes quantization parameters to minimize mean squared error. The result is model.Q4_K_M.gguf file typically 4-8GB for 7B models (75% smaller than 16GB unquantized), with accuracy degradation of only 1-2% on most benchmarks. Benefits include 75% reduction in disk space and RAM (Random Access Memory) requirements, 3-4x faster loading times due to smaller file size, improved inference speed through reduced memory bandwidth, and practical deployment on laptops and consumer devices. The quantization process runs on CPU (Central Processing Unit), completing in 5-10 minutes for 7B models. Verification compares quantized model output to unquantized on test prompts, ensuring perplexity increase remains under 2% and generation quality is indistinguishable for most use cases." id="step3_quantize">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#9C27B0;fontStyle=1;fontSize=11" parent="1" vertex="1">
                        <mxGeometry x="520" y="405" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="./quantize model.gguf model.Q4_K_M.gguf Q4_K_M&#xa;→ 4-8 GB (75% size reduction, &lt;2% accuracy loss)" tooltip="The ./quantize command from llama.cpp performs the actual quantization operation. The command syntax is: ./quantize [input_model] [output_model] [quantization_type]. In this case: model.gguf is the input unquantized GGUF (GPT-Generated Unified Format) file from Step 2 (16-20GB), model.Q4_K_M.gguf is the output quantized file (4-8GB), and Q4_K_M specifies the quantization scheme (4-bit K-quantization Medium). The quantization process loads the unquantized model structure and weights, analyzes weight distributions per layer to determine optimal quantization parameters, applies k-quantization algorithm which clusters weights and quantizes each cluster optimally, computes scale factors and zero points for each quantization block (typically 32-64 weights), converts fp16 (16-bit floating point) weights to 4-bit integers using computed parameters, stores quantization metadata for runtime dequantization, and writes the complete quantized model to output file. The Q4_K_M scheme uses mixed precision: most layers at 4-bit (2^4 = 16 discrete values per weight), critical layers at 5-6 bit for accuracy preservation, embeddings and norms at higher precision, and scale factors stored at fp16 for reconstruction accuracy. The resulting file is 4-8GB (average 5-6GB for 7B models), representing 75% size reduction from 16-20GB original. The accuracy loss is minimal: typically 1-2% increase in perplexity, negligible impact on most generation tasks, preserved semantic understanding, and maintained code generation quality for Web4 tasks. The quantization is optimized for inference efficiency: 4x reduction in memory bandwidth requirements (fewer bytes to read per weight), faster loading from disk (smaller file), maintained inference speed despite dequantization overhead (modern CPUs (Central Processing Units) can dequantize efficiently), and enabled deployment scenarios impossible with full-precision models. Quality validation compares outputs on evaluation dataset, measuring perplexity degradation, generation quality on held-out prompts, and task-specific metrics (code compilation rate, framework compliance) to ensure production readiness." id="quantize_details">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#9C27B0;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="520" y="440" width="360" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <object label="🧰 llama.cpp • Python 3.10+ • safetensors" tooltip="The three core tools required for Artifact Packaging enable the complete merge-convert-quantize pipeline. llama.cpp is a C++ implementation of LLaMA (Large Language Model Meta AI) inference optimized for CPU (Central Processing Unit) and Apple Silicon, providing convert-hf-to-gguf.py script for Hugging Face to GGUF (GPT-Generated Unified Format) conversion and quantize binary for applying k-quantization compression. The llama.cpp project prioritizes efficiency through pure C/C++ implementation with no Python runtime overhead, optimized CPU kernels using AVX2/AVX512 on x86 and NEON on ARM (Advanced RISC Machines), memory-mapped file loading for fast startup, and quantization support from 2-bit to 16-bit precision. Python 3.10+ is required for running the conversion script which uses modern Python features like type hints and pattern matching, handles PyTorch and safetensors file formats, performs tensor format conversions, and writes GGUF binary format. The script dependencies include PyTorch or safetensors for reading model files, numpy for tensor operations, and GGUF library for writing output format. safetensors is a safe tensor serialization format developed by Hugging Face that prevents arbitrary code execution exploits possible with pickle-based PyTorch format, provides faster loading through memory-mapped access (same concept as GGUF), uses a simple binary format with clear structure, and is becoming the standard for model distribution. The packaging workflow uses these tools sequentially: PEFT (Parameter-Efficient Fine-Tuning) library (Python) merges LoRA (Low-Rank Adaptation) adapters, convert-hf-to-gguf.py (Python with llama.cpp library) converts to GGUF, and quantize binary (C++ from llama.cpp) applies compression. All tools run efficiently on M1 Mac without requiring GPU (Graphics Processing Unit) acceleration, completing the full pipeline in 30-60 minutes total for 7B models. The tools are open-source and actively maintained, ensuring long-term support and compatibility with new model architectures." id="package_tools">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F8CECC;strokeColor=#B85450;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="520" y="495" width="360" height="25" as="geometry"/>
                    </mxCell>
                </object>
                <object label="🚀 7️⃣ DEPLOYMENT &amp; RUNTIME&#xa;Making It Usable" tooltip="Deployment and Runtime (Phase 7 of Web4 LoRA Core 10% Phase) transforms the packaged model from Phase 6 into a running, interactive AI system ready for production use, testing, and evaluation. This final phase serves the trained and quantized model locally using either Ollama or Docker Desktop Models, enabling inference requests, tool calls, and code generation while maintaining the decentralized philosophy of local AI execution. The deployment options prioritize M1 Mac compatibility, avoiding cloud dependencies, and providing simple integration with development workflows. Ollama (Option A - Recommended) is a lightweight model management tool optimized for local LLM (Large Language Model) deployment, providing simple CLI (Command Line Interface) for model registration and serving, automatic memory management and batching, built-in API (Application Programming Interface) server with OpenAI-compatible endpoints, and native Metal Performance Shaders (MPS) acceleration on Apple Silicon. Docker Desktop Models (Option B) offers containerized deployment with visual model management interface, integration with Docker ecosystem and DevContainers, version control and rollback capabilities, and API endpoints for programmatic access. Both options load the quantized .gguf model (4-8GB) into memory, initialize the inference engine with proper tokenization, expose endpoints for text generation requests, and maintain model state for efficient multi-request serving. The deployment process validates model loading without errors (no corruption during packaging), verifies tokenization matches training configuration, confirms generation produces coherent outputs, and ensures acceptable performance metrics (latency, throughput, memory usage). Post-deployment testing evaluates the complete training-packaging-deployment pipeline through comprehensive test suites: Tool-JSON tests verify the model generates valid JSON (JavaScript Object Notation) tool calls matching expected schema, Style tests confirm TypeScript code follows Web4 conventions and compiles, Guardrails tests ensure proper refusal patterns for security violations, Latency tests measure token generation speed (~1 token/sec target on M1 for Q4 models), and Memory tests monitor footprint (22-26GB shared memory for 7B models typical). The deployed model becomes the production Web4 component generator, serving requests locally without network dependencies, maintaining user data privacy through local processing, enabling offline AI capabilities, and providing consistent, reproducible outputs. This phase completes the 10% hands-on segment, converting weeks of data preparation work into a tangible, usable AI system running entirely on consumer hardware." id="phase7_container">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E8F5E9;strokeColor=#4CAF50;strokeWidth=3;verticalAlign=top;align=left;spacingLeft=10;spacingTop=5;fontSize=18;fontStyle=1;" parent="1" vertex="1">
                        <mxGeometry x="950" y="130" width="400" height="450" as="geometry"/>
                    </mxCell>
                </object>
                <object label="GOAL: Serve trained and quantized model locally" tooltip="The goal of Deployment and Runtime is to serve the trained and quantized model as a running inference service on local hardware, enabling interactive testing, evaluation, and production use without cloud dependencies. Unlike cloud-based AI services that send user prompts to remote servers (privacy concerns, network dependency, usage costs), local deployment keeps all processing on-device, ensuring data privacy and security (prompts never leave the machine), enabling offline operation (no network required), providing consistent performance (no API rate limits or quotas), and maintaining full control over model behavior and updates. The deployment transforms the static .gguf model file from Phase 6 into a dynamic service that loads the model into RAM (Random Access Memory), initializes inference engine with optimized kernels, prepares tokenizer for encoding prompts and decoding outputs, and exposes interfaces (CLI (Command Line Interface), API (Application Programming Interface)) for submitting generation requests. Local serving on M1 Mac hardware leverages unified memory architecture where CPU (Central Processing Unit) and GPU (Graphics Processing Unit) share memory space, Metal Performance Shaders for accelerated inference, efficient quantized model format (Q4_K_M) requiring only 4-8GB RAM, and optimized llama.cpp or Ollama runtime for fast token generation (~1 token/sec for 7B Q4 models). The local deployment enables use cases critical for Web4 development: interactive component generation during development, automated testing in CI/CD (Continuous Integration/Continuous Deployment) pipelines, fine-tuning evaluation on held-out test sets, dogfooding and iteration on training data quality, and production serving for team internal tools. The goal encompasses not just getting the model running, but ensuring it meets production quality standards: stable operation without crashes or memory leaks, acceptable latency for interactive use, consistent outputs given same prompts (temperature=0), proper error handling for malformed requests, and comprehensive logging for debugging and monitoring." id="phase7_goal">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFFFCC;strokeColor=#D6B656;fontSize=11;fontStyle=2" parent="1" vertex="1">
                        <mxGeometry x="970" y="190" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="🟩 Option A: Ollama (Recommended M1)" tooltip="Option A (Ollama - Recommended for M1 Mac) provides the simplest and most efficient deployment path for local LLM (Large Language Model) inference on Apple Silicon. Ollama is an open-source tool designed specifically for running LLMs locally with minimal configuration and optimal performance. Key features include automatic model management (load, unload, version tracking), native Metal Performance Shaders (MPS) acceleration for M1/M2 chips, built-in API server with OpenAI-compatible endpoints, efficient memory management with automatic context window handling, and simple CLI (Command Line Interface) for operations. Ollama advantages for Web4 deployment: zero-configuration serving (no complex setup or dependencies), optimized for Apple Silicon out-of-the-box (no manual kernel compilation), automatic quantization format detection (.gguf native support), built-in concurrent request handling and batching, and seamless integration with development workflows (curl, HTTP clients, SDKs). The Modelfile (Ollama&#39;s configuration format) specifies model behavior: FROM directive points to the .gguf model file, PARAMETER settings control generation (temperature, top_p, top_k), TEMPLATE defines prompt format and structure, SYSTEM sets default system message, and ADAPTER enables runtime LoRA (Low-Rank Adaptation) loading. Ollama runtime provides production features: automatic model warm-up (preloading for fast first request), request queueing and prioritization, memory pooling for efficient multi-model serving, telemetry and performance metrics, and graceful shutdown with request draining. Performance on M1 Mac with Q4_K_M (4-bit K-quantization Medium) 7B model: ~1-1.5 tokens/sec generation speed for single requests, ~22-26GB total memory footprint (model + context + overhead), 2-3 second initial load time from disk, sub-second response time for short prompts, and scalable to 2-3 concurrent requests on 32GB RAM (Random Access Memory) systems. The Ollama deployment workflow is reproducible: install Ollama via homebrew or binary, create Modelfile with desired configuration, register model with ollama create, test with ollama run, and serve via ollama serve for API access. This approach is recommended for Web4 development due to simplicity, performance, and active community support with frequent updates and model compatibility improvements." id="option_a">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#388E3C;fontStyle=1;fontSize=11" parent="1" vertex="1">
                        <mxGeometry x="970" y="235" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="1. Create Modelfile&#xa;2. ollama create web4-lora -f Modelfile&#xa;3. ollama run web4-lora&#xa;4. Test tool-use, code style, guardrails" tooltip="The four-step Ollama deployment workflow establishes the Web4 fine-tuned model as a running service. Step 1 (Create Modelfile): Write a Modelfile configuration file specifying model source (FROM ./model.Q4_K_M.gguf points to quantized model from Phase 6), generation parameters (PARAMETER temperature 0 for deterministic output, PARAMETER num_ctx 4096 for context window), prompt template (TEMPLATE defines system message and user prompt structure), and optional adapters (ADAPTER for runtime LoRA (Low-Rank Adaptation) application). The Modelfile uses declarative syntax similar to Dockerfile, making it familiar for developers and version-controllable. Step 2 (ollama create): Register the model with Ollama model registry using ollama create web4-lora -f Modelfile command. This validates Modelfile syntax, loads and verifies the .gguf model file, computes model fingerprint for version tracking, stores model in Ollama registry (~/.ollama/models/), and creates manifest with metadata (name, parameters, base model, creation time). The create step ensures the model is properly recognized by Ollama runtime. Step 3 (ollama run): Launch interactive inference session with ollama run web4-lora command. This loads model into memory (~22-26GB for 7B Q4 model), initializes Metal Performance Shaders (MPS) backend for acceleration, prepares tokenizer and context buffers, starts interactive REPL (Read-Eval-Print Loop) for prompt input, and maintains session state for conversation continuity. The run command provides immediate feedback on model behavior and quality. Step 4 (Test comprehensive scenarios): Validate the deployed model across Web4 use cases: Tool-use tests submit prompts requiring JSON (JavaScript Object Notation) tool calls and verify valid schema, correct tool selection, and proper parameter extraction. Code style tests request TypeScript component generation and verify Web4 naming conventions, 5-layer OOP (Object-Oriented Programming) architecture, proper imports and exports, Vitest test structure, and README.md documentation. Guardrails tests attempt security violations (hardcoded credentials, missing validation) and confirm proper &lt;REFUSAL&gt; responses with alternative guidance. These tests validate that the fine-tuning, packaging, and deployment pipeline successfully produced a production-ready Web4 code generator. Any failures indicate issues in earlier phases requiring investigation: training data quality problems, quantization errors, or Modelfile configuration issues." id="ollama_steps">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#388E3C;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="970" y="270" width="360" height="70" as="geometry"/>
                    </mxCell>
                </object>
                <object label="🟦 Option B: Docker Desktop Models" tooltip="Option B (Docker Desktop Models) provides containerized model deployment with visual management interface and Docker ecosystem integration. Docker Desktop Models is a feature in Docker Desktop 4.34+ (2024) that enables local AI model hosting with familiar Docker tooling. This option suits teams already using Docker for development, requiring visual model management, needing container isolation for multiple models, or deploying across heterogeneous environments (macOS, Windows, Linux). Docker Desktop Models advantages: visual GUI (Graphical User Interface) for non-technical users to import and manage models, integration with DevContainers for reproducible dev environments, version control through Docker image tags, resource limits and isolation per model container, and standardized deployment across platforms. The deployment workflow uses Docker Desktop&#39;s Models tab: import .gguf file through drag-and-drop or file picker, assign model metadata (name following Docker image conventions, version tag for tracking iterations, description documenting model purpose and training), configure runtime parameters (memory limits to prevent OOM (Out-of-Memory), CPU (Central Processing Unit) affinity for performance, environment variables for config), and launch model server exposing REST API (Representational State Transfer Application Programming Interface) endpoint. Docker Desktop handles model containerization automatically: creates lightweight container with llama.cpp runtime, mounts .gguf model as read-only volume, configures network port for API access (typically 8080 or 11434), sets up logging and monitoring, and manages container lifecycle (start, stop, restart). The API endpoint provides OpenAI-compatible interface supporting /v1/chat/completions for conversational generation, /v1/completions for text completion, /v1/embeddings for vector generation, and /health for monitoring liveness and readiness. Performance considerations: Docker adds minimal overhead (~50-100MB memory, &lt;5% CPU), container networking slightly increases latency (1-2ms typically negligible), volume mounting provides direct .gguf access without copying, and Metal acceleration works through container runtime. Docker Desktop Models suits enterprise scenarios requiring standardized deployment, visual management for non-developers, container orchestration integration, and cross-platform consistency. However, for pure M1 Mac development focused on maximum performance and simplicity, Ollama (Option A) remains recommended due to lower overhead, better MPS (Metal Performance Shaders) integration, and simpler mental model without container abstraction." id="option_b">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1976D2;fontStyle=1;fontSize=11" parent="1" vertex="1">
                        <mxGeometry x="970" y="355" width="360" height="30" as="geometry"/>
                    </mxCell>
                </object>
                <object label="1. Open Docker Desktop → Models&#xa;2. Import .gguf file&#xa;3. Assign metadata (name, version, description)&#xa;4. Launch local API endpoint" tooltip="The four-step Docker Desktop Models deployment workflow provides GUI-based (Graphical User Interface) model management. Step 1 (Open Docker Desktop Models): Launch Docker Desktop application and navigate to Models tab in sidebar. This interface shows all locally available models, running model containers, resource usage statistics, and import/export options. The Models UI (User Interface) provides at-a-glance visibility into deployed models: model names and versions, container status (running/stopped), memory and CPU (Central Processing Unit) usage, active API endpoints, and recent request metrics. Step 2 (Import .gguf file): Click Import button and select model.Q4_K_M.gguf file from Phase 6 packaging output. Docker Desktop validates file format (GGUF (GPT-Generated Unified Format) magic bytes), reads model metadata (parameter count, architecture, quantization), computes file checksum for integrity verification, copies model to Docker volume storage, and prepares model for containerization. The import process handles large files efficiently through streaming, provides progress indication, and validates model can be loaded by llama.cpp runtime. Step 3 (Assign metadata): Fill out model registration form with name (e.g., web4-lora following Docker naming conventions), version tag (e.g., v1.0-q4km or git commit SHA for traceability), and description documenting model purpose (Web4 TypeScript component generator fine-tuned on 19,500 examples), training date, and base model. This metadata enables model discovery, version management, and documentation for team members. Docker stores metadata in model manifest, making it searchable and filterable in the Models UI. Step 4 (Launch local API endpoint): Click Start button to launch model container. Docker Desktop creates container with llama.cpp server runtime, mounts .gguf model at /models/, exposes API port (default 8080, configurable), configures environment variables (model path, parameters, concurrency limits), and starts health monitoring. The API endpoint becomes available at http://localhost:8080 supporting OpenAI-compatible routes: POST /v1/chat/completions for conversational prompts with role-based messages, POST /v1/completions for raw text generation, and GET /health for readiness checks. Test the endpoint with curl or Postman to verify successful deployment and expected output quality. The Docker Desktop approach provides excellent visibility through the UI showing real-time logs, resource usage graphs, request/response metrics, and error diagnostics, making it ideal for teams preferring visual tools over CLI (Command Line Interface) workflows." id="docker_steps">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#1976D2;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="970" y="390" width="360" height="70" as="geometry"/>
                    </mxCell>
                </object>
                <object label="🧪 Post-Deployment Tests" tooltip="Post-Deployment Tests validate that the complete training-packaging-deployment pipeline successfully produced a production-ready Web4 code generation model. These comprehensive tests evaluate the deployed model across multiple dimensions critical for Web4 component generation: correctness (does it generate valid, working code?), compliance (does it follow Web4 framework conventions?), safety (does it refuse inappropriate requests?), and performance (is it fast enough for interactive use?). The test suite serves multiple purposes: validates training data quality and fine-tuning effectiveness, confirms packaging process preserved model quality (no quantization degradation beyond acceptable limits), ensures deployment configuration is correct (proper parameters, tokenization, template), provides baseline metrics for future model iterations, and builds confidence for production deployment. Tests should be automated and reproducible, running against standardized prompts with deterministic generation settings (temperature=0) to enable regression detection. The five test categories cover all aspects of Web4 model behavior: Tool-JSON tests verify structured output generation matching expected schemas, Style tests validate code quality and framework compliance, Guardrails tests ensure security and safety refusals work correctly, Latency tests measure inference performance for user experience, and Memory tests confirm resource usage within acceptable bounds. Pass/fail criteria should be clearly defined with quantitative thresholds: Tool-JSON must achieve 95%+ schema validity, Style must reach 90%+ compilation success, Guardrails must maintain 98%+ refusal accuracy, Latency must stay under 2 seconds per token average, and Memory must not exceed 30GB peak usage. Failed tests indicate issues requiring root cause analysis: training data problems (if model generates incorrect patterns), quantization errors (if output quality degraded significantly), deployment misconfiguration (if parameters incorrect), or hardware issues (if performance below expectations). The test results should be logged with full context (prompt, generated output, metrics, timestamp) enabling debugging and long-term quality tracking across model versions." id="testing">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF9C4;strokeColor=#F57F17;fontStyle=1;fontSize=11" parent="1" vertex="1">
                        <mxGeometry x="970" y="475" width="360" height="25" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Tool-JSON • Style • Guardrails • Latency • Memory" tooltip="The five test types provide comprehensive coverage of Web4 model requirements. Tool-JSON tests evaluate structured output generation: submit prompts requiring tool calls (create component, run tests, analyze code) and validate generated JSON (JavaScript Object Notation) matches expected schema with required fields present and correctly typed, tool names matching defined set, parameter values within valid ranges, and no syntax errors or malformed JSON. Target: 95%+ schema validity across diverse prompts. Style tests verify code generation quality: request TypeScript component generation and validate output follows Web4 conventions including PascalCase for classes and interfaces, camelCase for methods and variables, ALLCAPS for constants, 5-layer OOP (Object-Oriented Programming) architecture, proper imports and exports, Vitest test structure, and comprehensive README.md documentation. Compile generated code with TypeScript strict mode and measure compilation success rate. Target: 90%+ compilation success, 85%+ framework compliance. Guardrails tests ensure safety and security: attempt to elicit inappropriate outputs including hardcoded credentials in code, bypassing type safety checks, mixing architecture layers improperly, using deprecated APIs or patterns, and logging sensitive user data. Verify model responds with &lt;REFUSAL&gt; pattern explaining why the request violates security/compliance rules and providing correct alternative approach. Target: 98%+ refusal accuracy for violations, &lt;1% false positives for benign requests. Latency tests measure inference performance: submit prompts of varying complexity (simple tool call, medium component, complex refactoring) and measure time to first token (TTFT - should be &lt;500ms for good UX (User Experience)), tokens per second (target ~1 token/sec on M1 for Q4 (4-bit quantized) 7B models), and total generation time for complete outputs. Track P50, P95, P99 latencies to understand distribution. Memory tests monitor resource consumption: measure peak RAM (Random Access Memory) usage during inference (target &lt;30GB for 7B Q4 models on 32GB systems), shared memory allocation between CPU (Central Processing Unit) and GPU (Graphics Processing Unit) on unified memory architecture, memory growth over extended sessions (detect leaks), and memory release after request completion. These five test categories together provide confidence that the deployed model meets production standards for Web4 component generation." id="test_types">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#F57F17;fontSize=10" parent="1" vertex="1">
                        <mxGeometry x="970" y="505" width="360" height="20" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="arrow1" value="" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#0078D7;endArrow=classic;endFill=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="training_output" target="step1_merge" edge="1">
                    <mxGeometry relative="1" as="geometry">
                        <mxPoint x="450" y="475" as="sourcePoint"/>
                        <mxPoint x="500" y="250" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow1_label" value="LoRA Adapters" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=10;fontStyle=2;fillColor=#FFFFFF" parent="arrow1" vertex="1" connectable="0">
                    <mxGeometry x="-0.2" y="1" relative="1" as="geometry">
                        <mxPoint x="5" y="-90" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow2" value="" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;strokeWidth=3;strokeColor=#FF8C00;endArrow=classic;endFill=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="quantize_details" target="option_a" edge="1">
                    <mxGeometry relative="1" as="geometry">
                        <mxPoint x="900" y="460" as="sourcePoint"/>
                        <mxPoint x="950" y="250" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow2_label" value="Quantized .gguf" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=10;fontStyle=2;fillColor=#FFFFFF" parent="arrow2" vertex="1" connectable="0">
                    <mxGeometry x="-0.2" y="1" relative="1" as="geometry">
                        <mxPoint x="5" y="-100" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="success_title" value="✅ Success Criteria by Phase" style="text;html=1;strokeColor=none;fillColor=#4CAF50;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=14;fontStyle=1;fontColor=#FFFFFF" parent="1" vertex="1">
                    <mxGeometry x="50" y="610" width="1300" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="success_training" value="🧠 Training" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1F5FF;strokeColor=#0078D7;fontStyle=1;fontSize=11" parent="1" vertex="1">
                    <mxGeometry x="50" y="655" width="400" height="25" as="geometry"/>
                </mxCell>
                <mxCell id="success_training_list" value="• Loss stabilizes 0.6–1.0&#xa;• Valid JSON tool calls&#xa;• Correct TypeScript style&#xa;• No MPS OOM errors" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#0078D7;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="50" y="685" width="400" height="70" as="geometry"/>
                </mxCell>
                <mxCell id="success_packaging" value="🧱 Packaging" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF4E6;strokeColor=#FF8C00;fontStyle=1;fontSize=11" parent="1" vertex="1">
                    <mxGeometry x="500" y="655" width="400" height="25" as="geometry"/>
                </mxCell>
                <mxCell id="success_packaging_list" value="• .gguf loads without error&#xa;• Eval scores drop ≤2%&#xa;• Disk footprint reduced ~75%&#xa;• Checksum verified" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#FF8C00;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="500" y="685" width="400" height="70" as="geometry"/>
                </mxCell>
                <mxCell id="success_deployment" value="🚀 Deployment" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E8F5E9;strokeColor=#4CAF50;fontStyle=1;fontSize=11" parent="1" vertex="1">
                    <mxGeometry x="950" y="655" width="400" height="25" as="geometry"/>
                </mxCell>
                <mxCell id="success_deployment_list" value="• Model responds to prompts&#xa;• ~1 token/sec on M1 (Q4)&#xa;• Memory footprint: 22-26 GB&#xa;• All test types pass" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#4CAF50;align=left;spacingLeft=10;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="950" y="685" width="400" height="70" as="geometry"/>
                </mxCell>
                <mxCell id="deliverables_title" value="📦 Final Deliverables" style="text;html=1;strokeColor=none;fillColor=#9C27B0;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=14;fontStyle=1;fontColor=#FFFFFF" parent="1" vertex="1">
                    <mxGeometry x="50" y="790" width="1300" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="deliverable1" value="adapter_model.bin" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#9C27B0;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="80" y="840" width="180" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="deliverable2" value="merged_model.gguf" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#9C27B0;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="280" y="840" width="180" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="deliverable3" value="model.Q4_K_M.gguf" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#9C27B0;fontSize=10;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="480" y="840" width="180" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="deliverable4" value="Modelfile" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#9C27B0;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="680" y="840" width="180" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="deliverable5" value="eval_log.md" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#9C27B0;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="880" y="840" width="180" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="deliverable6" value="drawio_diagram.png" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E1BEE7;strokeColor=#9C27B0;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="1080" y="840" width="180" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="why_matters" value="💡 Why This 10% Matters" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFD700;strokeColor=#FF8C00;fontSize=14;fontStyle=1;verticalAlign=top;align=center;spacingTop=10" parent="1" vertex="1">
                    <mxGeometry x="50" y="900" width="1300" height="120" as="geometry"/>
                </mxCell>
                <mxCell id="why1" value="✓ Converts weeks of data work into a real, usable AI model" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=11;spacingLeft=30" parent="1" vertex="1">
                    <mxGeometry x="70" y="930" width="600" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="why2" value="✓ Establishes a repeatable training loop: data → LoRA → GGUF → deploy" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=11;spacingLeft=30" parent="1" vertex="1">
                    <mxGeometry x="70" y="955" width="600" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="why3" value="✓ Keeps everything local, secure, and verifiable (Web4 governance)" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=11;spacingLeft=30" parent="1" vertex="1">
                    <mxGeometry x="700" y="930" width="600" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="why4" value="✓ Enables incremental re-training (fine-tune only new adapters)" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=11;spacingLeft=30" parent="1" vertex="1">
                    <mxGeometry x="700" y="955" width="600" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="recap_title" value="🧭 Recap" style="text;html=1;strokeColor=none;fillColor=#607D8B;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=14;fontStyle=1;fontColor=#FFFFFF" parent="1" vertex="1">
                    <mxGeometry x="50" y="1050" width="1300" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="recap_header1" value="Step" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#455A64;strokeColor=#263238;fontStyle=1;fontSize=11;fontColor=#FFFFFF" parent="1" vertex="1">
                    <mxGeometry x="50" y="1090" width="150" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="recap_header2" value="Goal" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#455A64;strokeColor=#263238;fontStyle=1;fontSize=11;fontColor=#FFFFFF" parent="1" vertex="1">
                    <mxGeometry x="200" y="1090" width="350" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="recap_header3" value="Tools" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#455A64;strokeColor=#263238;fontStyle=1;fontSize=11;fontColor=#FFFFFF" parent="1" vertex="1">
                    <mxGeometry x="550" y="1090" width="400" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="recap_header4" value="Outcome" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#455A64;strokeColor=#263238;fontStyle=1;fontSize=11;fontColor=#FFFFFF" parent="1" vertex="1">
                    <mxGeometry x="950" y="1090" width="400" height="30" as="geometry"/>
                </mxCell>
                <mxCell id="recap_train1" value="🧠 Train" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#E1F5FF;strokeColor=#0078D7;fontStyle=1;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="50" y="1120" width="150" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_train2" value="Teach Web4 philosophy via LoRA" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#0078D7;fontSize=10;align=left;spacingLeft=10" parent="1" vertex="1">
                    <mxGeometry x="200" y="1120" width="350" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_train3" value="PEFT + TRL on MPS" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#0078D7;fontSize=10;align=left;spacingLeft=10" parent="1" vertex="1">
                    <mxGeometry x="550" y="1120" width="400" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_train4" value="Adapter weights" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#0078D7;fontSize=10;align=left;spacingLeft=10" parent="1" vertex="1">
                    <mxGeometry x="950" y="1120" width="400" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_package1" value="🧱 Package" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFF4E6;strokeColor=#FF8C00;fontStyle=1;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="50" y="1160" width="150" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_package2" value="Merge + Quantize for performance" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#FF8C00;fontSize=10;align=left;spacingLeft=10" parent="1" vertex="1">
                    <mxGeometry x="200" y="1160" width="350" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_package3" value="llama.cpp scripts" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#FF8C00;fontSize=10;align=left;spacingLeft=10" parent="1" vertex="1">
                    <mxGeometry x="550" y="1160" width="400" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_package4" value=".gguf model" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#FF8C00;fontSize=10;align=left;spacingLeft=10" parent="1" vertex="1">
                    <mxGeometry x="950" y="1160" width="400" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_deploy1" value="🚀 Deploy" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#E8F5E9;strokeColor=#4CAF50;fontStyle=1;fontSize=10" parent="1" vertex="1">
                    <mxGeometry x="50" y="1200" width="150" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_deploy2" value="Serve locally in Ollama or Docker" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#4CAF50;fontSize=10;align=left;spacingLeft=10" parent="1" vertex="1">
                    <mxGeometry x="200" y="1200" width="350" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_deploy3" value="Ollama / Docker Desktop" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#4CAF50;fontSize=10;align=left;spacingLeft=10" parent="1" vertex="1">
                    <mxGeometry x="550" y="1200" width="400" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="recap_deploy4" value="Interactive local AI model" style="rounded=0;whiteSpace=wrap;html=1;fillColor=#FFFFFF;strokeColor=#4CAF50;fontSize=10;align=left;spacingLeft=10" parent="1" vertex="1">
                    <mxGeometry x="950" y="1200" width="400" height="40" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
</mxfile>