<mxfile host="65bd71144e">
    <diagram name="Work Distribution 90/10" id="work-dist-90-10">
        <mxGraphModel dx="1829" dy="711" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="1600" pageHeight="1200" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <mxCell id="title" value="Web4 LoRA Project — Work Distribution (90% / 10% Framework)" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=24;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="300" y="20" width="1000" height="40" as="geometry"/>
                </mxCell>
                <mxCell id="subtitle" value="💡 90% of the work is planning, data, validation, and evaluation. 10% is the actual training and serving." style="text;html=1;strokeColor=none;fillColor=#FFF4E6;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=1;fontSize=14;fontStyle=2" parent="1" vertex="1">
                    <mxGeometry x="300" y="70" width="1000" height="30" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#0D47A1&quot;&gt;90% — PLANNING, DATA, EVALUATION, DOCUMENTATION&lt;/font&gt;&#xa;Purpose: Intellectual foundation and quality assurance" tooltip="The 90 percent Planning, Data, Evaluation, and Documentation phase represents the intellectual foundation of the Web4 LoRA project. This substantial effort allocation reflects a fundamental truth in machine learning: the quality of your training data and evaluation methodology determines model success far more than training hyperparameters or architecture choices. This phase encompasses nine distinct activities: Data Foundation (25 percent) creates clean, schema-valid datasets with proper JSONL formatting and balanced examples. Template Governance (10 percent) ensures consistent chat formatting across training, evaluation, and serving to prevent prompt misalignment. Evaluation Infrastructure (20 percent) builds the quantitative harness for measuring model quality with metrics like JSON validity, code style compliance, and guardrail refusal accuracy. Data QA Loop (15 percent) continuously refines the dataset by adding counterexamples, fixing edge cases, and tracking improvement over iterations. Documentation and Reporting (10 percent) maintains transparent records of decisions, metrics, and pipeline design. Continuous Improvement (10 percent) adds governance like CI linting and automated evaluation to ensure sustainability. The 90 percent allocation is not arbitrary - it reflects real-world experience that data quality issues, evaluation design challenges, and documentation gaps consume the majority of project time. Training itself (5 percent) is relatively quick once you have clean data and know what success looks like. The expected outcome is a quality foundation that enables fast, confident training iterations. When data is clean and evaluation is trustworthy, you can quickly identify and fix issues rather than guessing whether poor results are due to data problems, prompt misalignment, or training configuration. This front-loaded investment in quality pays dividends throughout the project lifecycle by eliminating the most common sources of training failures and enabling reproducible, scientific iteration on model improvements." id="planning-box">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E3F2FD;strokeColor=#1976D2;strokeWidth=3;arcSize=5;fontSize=20;fontStyle=1;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="50" y="140" width="1100" height="700" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1B5E20&quot;&gt;1️⃣ DATA FOUNDATION — 25%&lt;/font&gt;&#xa;&#xa;Dataset Design and Quality&#xa;• Define JSONL schemas (Tool-Core, Style, Guardrail)&#xa;• Validate every line (jsonschema, jq)&#xa;• Balance positive and negative examples&#xa;• Token count analysis&#xa;🎯 Difficulty: Easy → Moderate" tooltip="Data Foundation (25 percent of total effort) establishes the quality foundation for the entire Web4 LoRA project by creating a clean, well-structured dataset that teaches the model correct Web4 patterns. Define JSONL schemas means creating strict schemas for three main buckets: Tool-Core (structured tool-calling with JSON output), Style (Web4 coding conventions and architecture patterns), and Guardrail (refusal patterns for inappropriate requests). Each schema specifies required fields (task_type, instruction, input, output), data types, and validation rules to ensure consistency across all training examples. Validate every line uses automated tools like jsonschema for schema compliance and jq for JSON parsing to catch malformed data before training. This prevents training failures due to corrupt data and ensures every example is properly formatted. Balance positive and negative examples means including both correct patterns (what to do) and counterexamples (what not to do). For Tool-Core, this includes Tool-Neg examples showing incorrect tool usage. For Guardrails, this means both refusal patterns and acceptable requests. The balance prevents the model from being too permissive or too restrictive. Token count analysis ensures all training examples fit within the model maximum sequence length (typically 2048 or 4096 tokens). This involves tokenizing each example, measuring token counts, identifying outliers, and either splitting long examples or removing them. Proper token analysis prevents truncation during training that would corrupt learning. The 25 percent effort allocation reflects that dataset quality is the single most important factor in fine-tuning success - garbage in, garbage out. Expected outcome is a clean, schema-valid dataset ready for LoRA training with confidence that every example teaches correct patterns. Difficulty ranges from Easy (basic JSONL formatting) to Moderate (balancing examples, handling edge cases) but does not require ML expertise, just attention to detail and data quality discipline." id="phase1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#388E3C;strokeWidth=2;fontSize=11;fontStyle=0;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="80" y="220" width="500" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#1B5E20&quot;&gt;2️⃣ TEMPLATE GOVERNANCE — 10%&lt;/font&gt;&#xa;&#xa;Prompt and Format Consistency&#xa;• Define canonical chat template&#xa;• Align train / eval / serve formats&#xa;• Freeze system + user roles&#xa;• Document exact examples&#xa;🎯 Difficulty: Moderate" tooltip="Template Governance (10 percent of total effort) ensures consistent chat formatting across training, evaluation, and serving to prevent the most common and frustrating source of fine-tuning failures: prompt misalignment. Define canonical chat template means establishing the exact format for system messages, user messages, and assistant responses that will be used throughout the project. For example, ChatML format with specific tags or Llama-2 style with special tokens. The template must be frozen early and never changed mid-project. Align train, eval, and serve formats ensures the model sees the same prompt structure during training (TRL/PEFT), evaluation (your test harness), and production serving (Ollama/vLLM). Misalignment here causes mysterious performance drops where the model works in training but fails in production because the prompt format changed. Freeze system and user roles means deciding upfront what instructions go in the system message versus user message and maintaining this separation consistently. For Web4, this might mean system message contains tool schemas while user message contains the actual request. Document exact examples creates reference templates showing the complete formatting including special tokens, whitespace, and newlines. These examples serve as the source of truth for all code that generates prompts. The 10 percent effort allocation reflects that while prompt formatting seems trivial, getting it right across multiple tools (training framework, evaluation scripts, serving platform) requires careful attention and validation. Common pitfalls include extra newlines that change tokenization, missing special tokens that confuse the model, and inconsistent role assignments that break tool-calling. Expected outcome is stable and deterministic prompt alignment where the model receives consistent formatting throughout its lifecycle. Difficulty is Moderate because it requires understanding how different tools handle chat templates and debugging subtle tokenization differences that are hard to spot visually." id="phase2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#388E3C;strokeWidth=2;fontSize=11;fontStyle=0;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="620" y="220" width="500" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#0D47A1&quot;&gt;3️⃣ EVALUATION INFRASTRUCTURE — 20%&lt;/font&gt;&#xa;&#xa;Metrics and Benchmarks&#xa;• Build evaluate.py harness&#xa;• JSON validation, schema checks&#xa;• AST/lint scoring for code style&#xa;• Guardrail refusal tests&#xa;🎯 Difficulty: Moderate → Hard" tooltip="Evaluation Infrastructure (20 percent of total effort) builds the quantitative harness for measuring model quality, answering the critical question: how do you know if your model is good. Build evaluate.py harness creates a standalone script that runs the model on a hold-out test set and produces numerical scores for different capabilities. This harness must be deterministic (same input always produces same output) and automated (no manual inspection required for pass/fail decisions). JSON validation and schema checks verify that the model generates syntactically correct JSON that matches expected schemas. For tool-calling models, this measures whether the model outputs parseable JSON with required fields and correct data types. A model that generates 95 percent valid JSON is usable in production, while one that generates 60 percent valid JSON is not. AST and lint scoring for code style uses automated tools like ESLint or Prettier to measure whether generated code follows style guidelines. For Web4, this might check proper naming conventions, correct imports, and proper class structure. AST (Abstract Syntax Tree) analysis verifies code structure beyond surface formatting. Guardrail refusal tests measure whether the model correctly refuses inappropriate requests using the REFUSAL tag. This tests both precision (does it refuse bad requests) and recall (does it accept good requests). The evaluation harness must test edge cases, adversarial inputs, and common failure modes, not just happy path examples. The 20 percent effort allocation reflects that designing good evaluation metrics is intellectually challenging - you must define what correctness means quantitatively. Poor evaluation leads to over-optimizing on the wrong metrics or missing critical failures. Expected outcome is repeatable quantitative evaluation of model quality that provides trustworthy signals for improvement. You should be able to run evaluate.py, see scores, make changes, and confidently know whether the change helped or hurt. Difficulty is Moderate to Hard because it requires both programming skill (building the harness) and domain expertise (knowing what to measure and how)." id="phase3">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1976D2;strokeWidth=2;fontSize=11;fontStyle=0;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="80" y="390" width="500" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#0D47A1&quot;&gt;4️⃣ DATA QA LOOP — 15%&lt;/font&gt;&#xa;&#xa;Continuous Curation&#xa;• Iterate dataset improvements&#xa;• Add counterexamples (Tool-Neg)&#xa;• Refine guardrails&#xa;• Track token growth vs results&#xa;🎯 Difficulty: Moderate" tooltip="Data QA Loop (15 percent of total effort) continuously refines the dataset based on evaluation results and observed model failures, implementing a scientific improvement cycle. Iterate dataset improvements means using evaluation metrics to identify weaknesses (model generates invalid JSON 20 percent of the time, model refuses valid requests 5 percent of the time) and systematically adding training examples to address those weaknesses. This is evidence-based dataset curation, not random example generation. Add counterexamples (Tool-Neg) specifically targets common model mistakes by including negative examples. If the model often generates prose instead of JSON, add Tool-Neg examples showing the wrong way with corrections. If the model hallucinates tool names, add examples with similar-but-wrong names and proper refusals. Counterexamples teach the model what not to do, which is often more valuable than additional positive examples. Refine guardrails involves tuning the balance between safety and usefulness. If the model refuses too many benign requests (high false positive rate), add more examples of acceptable requests. If it accepts dangerous requests (low refusal rate), add more refusal examples with explanations. This iterative tuning is essential for production-ready guardrails. Track token growth versus results monitors dataset size and measures whether adding more examples continues to improve metrics or shows diminishing returns. If adding 1000 examples improves JSON validity from 85 percent to 90 percent but the next 1000 only improves it to 90.5 percent, you have hit the data ceiling and need qualitative improvements, not just more quantity. The 15 percent effort allocation reflects that quality improvement is iterative - you will not get the dataset perfect on the first try. Multiple rounds of train, evaluate, identify failures, add examples, and retrain are normal and expected. Expected outcome is ongoing dataset refinement that reduces overfit and bias while systematically addressing model weaknesses. Difficulty is Moderate because it requires analytical thinking to diagnose failures and creativity to design examples that fix specific issues rather than just adding more random data." id="phase4">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1976D2;strokeWidth=2;fontSize=11;fontStyle=0;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="620" y="390" width="500" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#827717&quot;&gt;8️⃣ DOCUMENTATION AND REPORTING — 10%&lt;/font&gt;&#xa;&#xa;Knowledge Capture&#xa;• Maintain Markdown logs&#xa;• Diagram full pipeline (draw.io)&#xa;• Record eval metrics over time&#xa;• Write model card summary&#xa;🎯 Difficulty: Easy" tooltip="Documentation and Reporting (10 percent of total effort) maintains transparent records of decisions, metrics, and pipeline design to ensure reproducibility and knowledge transfer. Maintain Markdown logs creates running documentation of key decisions, experiments tried, and lessons learned. This includes rationale for hyperparameter choices, explanations of why certain dataset changes were made, and records of what worked versus what failed. These logs are invaluable when revisiting the project months later or onboarding new team members. Diagram full pipeline using draw.io or similar tools creates visual representations of the complete training workflow: data preparation, validation, training, evaluation, and deployment. Diagrams communicate complex processes more effectively than text and serve as reference documentation for understanding system architecture. Record eval metrics over time tracks quantitative improvements across training iterations. This means logging scores from evaluate.py after each training run with timestamps, dataset versions, and hyperparameters. Tracking metrics over time reveals trends (are we improving or plateauing), validates that changes have intended effects, and provides data for retrospectives. Write model card summary documents the final model capabilities, limitations, intended use cases, and known failure modes following model card best practices. This transparency is essential for responsible AI deployment and helps users understand what the model can and cannot do. The 10 percent effort allocation recognizes that documentation often feels like overhead but pays massive dividends in project sustainability. Undocumented projects become unmaintainable as team members forget decisions or leave. Expected outcome is transparent, reproducible documentation that enables anyone to understand the complete training pipeline, reproduce results, and continue improving the model. Difficulty is Easy because it requires diligence and discipline, not specialized skills. The hard part is doing it consistently throughout the project rather than scrambling to document everything at the end." id="phase8">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F0F4C3;strokeColor=#AFB42B;strokeWidth=2;fontSize=11;fontStyle=0;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="80" y="550" width="500" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#827717&quot;&gt;9️⃣ CONTINUOUS IMPROVEMENT — 10%&lt;/font&gt;&#xa;&#xa;Governance and Scaling&#xa;• Add CI linting&#xa;• Automate eval and timing&#xa;• Plan migration to Linux/CUDA (optional)&#xa;• Improve eval metrics over versions&#xa;🎯 Difficulty: Moderate" tooltip="Continuous Improvement (10 percent of total effort) adds governance and automation to ensure the training process is sustainable and can scale beyond the initial prototype. Add CI linting integrates automated checks into version control using GitHub Actions or similar CI/CD systems. This includes JSONL format validation, schema checking, and code linting to catch errors before they reach production. CI prevents regression where working datasets accidentally get corrupted by manual edits. Automate eval and timing creates scripts that automatically run evaluation after training completes and record timing metrics for each pipeline stage. Automation eliminates manual toil and ensures consistent evaluation methodology across all training runs. For example, a post-training hook that runs evaluate.py and appends results to a CSV log with timestamps. Plan migration to Linux or CUDA for optional future scaling documents the path to move from M1 Mac prototype to larger-scale training infrastructure. This includes identifying bottlenecks (is M1 too slow for larger datasets), researching cloud GPU options, and planning data migration strategies. Even if you never execute the migration, having a plan reduces risk. Improve eval metrics over versions means continuously refining what you measure and how. As you learn more about model failures, add new evaluation categories. If you discover the model has edge case failures not caught by existing metrics, design new tests. Evaluation methodology should evolve with your understanding. The 10 percent effort allocation reflects that sustainability work is often deferred during initial development but becomes critical for long-term success. Projects without governance accumulate technical debt and become hard to maintain. Expected outcome is a sustainable, repeatable training process with automated quality checks and clear evolution path. The project can continue improving over multiple iterations without requiring heroic manual effort. Difficulty is Moderate because it requires DevOps knowledge (CI setup, automation) and strategic thinking (planning future scaling) beyond basic training skills." id="phase9">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F0F4C3;strokeColor=#AFB42B;strokeWidth=2;fontSize=11;fontStyle=0;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="620" y="550" width="500" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="summary-90" value="✅ Clean dataset ready for training&lt;br&gt;✅ Stable prompt alignment&lt;br&gt;✅ Repeatable evaluation harness&lt;br&gt;✅ Ongoing data refinement&lt;br&gt;✅ Transparent documentation&lt;br&gt;✅ Sustainable process" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#E8F5E9;strokeColor=#2E7D32;strokeWidth=2;align=left;verticalAlign=middle;fontSize=12;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="80" y="720" width="1040" height="100" as="geometry"/>
                </mxCell>
                <object label="&lt;font color=&quot;#E65100&quot;&gt;10% — TRAINING &amp; SERVING&lt;/font&gt;&#xa;Purpose: Model fine-tuning and deployment" tooltip="The 10 percent Training and Serving phase represents the mechanical execution of the Web4 LoRA project after the intellectual foundation has been established. This modest effort allocation reflects a fundamental truth in machine learning: once you have clean data, proper evaluation metrics, and clear success criteria, the actual training and deployment are relatively quick and straightforward operations. This phase encompasses three distinct activities: Training (5 percent) configures and runs LoRA fine-tuning using TRL and PEFT on Mac M1 Metal, tuning hyperparameters like learning rate, batch size, and epochs, running dry tests with 100 samples to validate the pipeline, then executing full fine-tuning on 25-35k samples. Artifact Packaging (2 percent) merges LoRA adapters with the base Hugging Face model, converts to GGUF format using llama.cpp tools, applies quantization (Q4_K_M or Q5_K_M) to reduce file size while maintaining quality, and validates checksums to ensure artifact integrity. Deployment (3 percent) creates Ollama Modelfiles with proper configuration, loads the quantized model locally for testing, runs smoke tests using the evaluation dataset to verify correct behavior, and optionally packages the model for Docker Desktop Models for professional deployment. The 10 percent allocation is not arbitrary - it reflects real-world experience that training itself is fast (hours to days) compared to the weeks spent on data preparation and evaluation design. The expected outcome is a stable LoRA adapter trained on M1 hardware, a compact GGUF artifact ready for serving (typically 2-4GB for Q4 quantization), and a locally deployable quantized model responding correctly to Web4 component generation requests. When the 90 percent foundation is solid, this 10 percent execution phase proceeds smoothly with minimal debugging or iteration. The difficulty ranges from Easy (packaging and deployment are well-documented processes) to Moderate (training requires understanding hyperparameters and monitoring for overfitting), but none of these tasks require advanced ML expertise - just careful following of established procedures and validation that each step produces expected outputs." id="training-box">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFE0B2;strokeColor=#E65100;strokeWidth=3;arcSize=5;fontSize=20;fontStyle=1;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="1200" y="140" width="350" height="700" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#BF360C&quot;&gt;5️⃣ TRAINING — 5%&lt;/font&gt;&#xa;&#xa;LoRA Configuration and Run&#xa;• Configure TRL+PEFT on MPS&#xa;• Tune LR, batch size, epochs&#xa;• Run dry test (100 samples)&#xa;• Run full fine-tune (25–35k samples)&#xa;🎯 Difficulty: Moderate" tooltip="Training (5 percent of total effort) is where the actual LoRA fine-tuning happens using TRL (Transformer Reinforcement Learning) and PEFT (Parameter Efficient Fine-Tuning) libraries on Mac M1 Metal Performance Shaders backend. Configure TRL and PEFT means setting up the training pipeline with appropriate LoRA configuration including rank (typically 8-32), alpha scaling factor (typically 16-64), target modules (q_proj, v_proj for attention layers), dropout rate (0.05-0.1), and bias handling (none or all). The configuration also specifies training arguments like output directory, number of epochs (2-4 for fine-tuning), warmup steps, logging intervals, and save strategies. Tune learning rate, batch size, and epochs involves finding the right balance between training speed and model quality. Learning rates for LoRA typically range from 1e-4 to 3e-4, much higher than full fine-tuning because only a small subset of parameters are being trained. Batch size depends on available unified memory (4-16 on M1 with 32GB RAM using gradient accumulation) and affects training stability. Epoch count balances learning (too few and the model underfits) and overfitting (too many and it memorizes training data). Run dry test with 100 samples validates the entire pipeline end-to-end before committing to full training. This catches configuration errors, data format issues, memory problems, and training instability early when iteration is cheap. The dry test should complete in 10-30 minutes and produce a basic model that shows some learning on training examples. Run full fine-tune on 25-35k samples is the main training job, typically taking 4-12 hours on M1 depending on model size, sequence length, and batch size. During training, monitor loss curves (should decrease smoothly), gradient norms (should be stable), and checkpoint metrics (should improve over epochs). The 5 percent effort allocation reflects that once data is clean and configuration is correct, training is mostly automated waiting - the machine does the work. Expected outcome is a stable LoRA adapter (typically 50-200MB) with training metrics showing successful learning (loss decreased, evaluation metrics improved, no NaN or Inf values). Difficulty is Moderate because it requires understanding hyperparameters, recognizing overfitting or underfitting, and debugging training failures, but established best practices and frameworks like TRL make this much easier than implementing training loops from scratch." id="phase5">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=2;fontSize=11;fontStyle=0;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="1230" y="260" width="290" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#BF360C&quot;&gt;6️⃣ ARTIFACT PACKAGING — 2%&lt;/font&gt;&#xa;&#xa;Merge and Quantize&#xa;• Merge LoRA with base HF model&#xa;• Convert to GGUF format&#xa;• Quantize (Q4_K_M or Q5_K_M)&#xa;• Validate checksum&#xa;🎯 Difficulty: Easy" tooltip="Artifact Packaging (2 percent of total effort) transforms the trained LoRA adapter into a production-ready, quantized GGUF model suitable for efficient inference and deployment. This phase bridges the gap between training artifacts (PyTorch checkpoints) and serving formats (quantized GGUF). Merge LoRA with base Hugging Face model combines the learned LoRA weights (typically 50-200MB) with the original base model (typically 13GB for a 7B model) to create a single, self-contained model that incorporates all fine-tuned knowledge. This process uses libraries like PEFT to apply the low-rank updates to the base model weights, resulting in a complete model file that no longer requires separate adapter loading. The merge typically takes 5-15 minutes and requires sufficient disk space for both source and output models (at least 30GB for a 7B model). Convert to GGUF format uses llama.cpp conversion tools (convert.py or convert-hf-to-gguf.py) to transform the merged Hugging Face model into GGUF (GGML Universal Format), which is optimized for CPU and Metal inference with llama.cpp and Ollama. The conversion handles tokenizer files, model architecture metadata, and weight tensor formats. This step typically takes 5-10 minutes and validates that the conversion maintains model integrity. Quantize to Q4_K_M or Q5_K_M applies post-training quantization to reduce model size and increase inference speed with minimal quality loss. Q4_K_M (4-bit mixed quantization) reduces a 7B model from 13GB to approximately 4GB with negligible quality degradation for most tasks. Q5_K_M (5-bit) provides slightly better quality at approximately 5GB. The quantization process uses llama.cpp quantize tool and takes 3-8 minutes. Validate checksum ensures the final artifact has not been corrupted during the merge, conversion, and quantization pipeline. This involves computing SHA256 hashes and comparing file sizes against expected ranges. The 2 percent effort allocation reflects that this is a well-established, mostly automated process with clear steps and error messages. Expected outcome is a compact GGUF artifact (2-5GB depending on quantization level) that loads quickly in Ollama, runs efficiently on M1 hardware, and maintains the fine-tuned model quality. Difficulty is Easy because the tools (PEFT, llama.cpp) handle complexity automatically and the process rarely requires debugging beyond ensuring sufficient disk space and memory." id="phase6">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=2;fontSize=11;fontStyle=0;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="1230" y="420" width="290" height="130" as="geometry"/>
                    </mxCell>
                </object>
                <object label="&lt;font color=&quot;#BF360C&quot;&gt;7️⃣ DEPLOYMENT — 3%&lt;/font&gt;&#xa;&#xa;Serving and Testing&#xa;• Create Ollama Modelfile&#xa;• Load model locally&#xa;• Run smoke tests&#xa;• Package for deployment (optional)&#xa;🎯 Difficulty: Easy" tooltip="Deployment (3 percent of total effort) makes the quantized GGUF model available for actual use through Ollama, validates it works correctly, and optionally packages it for distribution or production deployment. This final phase ensures the fine-tuned model is accessible and performs as expected in real-world scenarios. Create Ollama Modelfile defines the model configuration including the FROM directive pointing to the quantized GGUF file, TEMPLATE specifying the chat format (must match training template for consistency), PARAMETER settings like temperature, top_p, and stop tokens, and SYSTEM message defining the model role and behavior. The Modelfile acts as a declarative specification for model serving and typically takes 2-5 minutes to create and validate. Load model locally uses the ollama create command to import the Modelfile and register the model in Ollama model registry, followed by ollama run to start an interactive session or ollama serve to launch a persistent API server. Loading typically takes 5-15 seconds for quantized models thanks to GGUF efficient memory mapping. Once loaded, the model is ready for inference through CLI, REST API, or integration with applications. Run smoke tests validates the deployed model produces correct responses using the hold-out evaluation dataset. This involves testing key capabilities: correct tool-calling format for Tool-Core examples, adherence to Web4 coding standards for Style examples, proper refusal behavior for Guardrail examples, and general reasoning and instruction-following. Smoke tests should complete in 5-10 minutes and catch issues like prompt misalignment, quality regressions, or serving configuration problems before production use. Package for deployment (optional) creates distributable artifacts like Docker containers with ollama and the model pre-loaded, OCI model artifacts for Docker Desktop Models tab, or tar archives for sharing with team members. Packaging enables professional deployment scenarios with version control, reproducibility, and easy distribution. The 3 percent effort allocation reflects that Ollama abstracts away most serving complexity - no need to write serving code, handle batching, or manage GPU memory manually. Expected outcome is a locally deployable quantized model responding correctly to Web4 component generation requests with proper formatting, framework compliance, and fast inference times (typically 10-50 tokens per second on M1). Difficulty is Easy because Ollama provides excellent defaults and clear documentation, and the process rarely requires debugging beyond ensuring the chat template matches training format." id="phase7">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=2;fontSize=11;fontStyle=0;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="1230" y="570" width="290" height="130" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="summary-10" value="🚀 Stable adapter trained&lt;br&gt;🚀 Compact GGUF artifact&lt;br&gt;🚀 Locally deployable model" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFF3E0;strokeColor=#EF6C00;strokeWidth=2;align=left;verticalAlign=middle;fontSize=12;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="1230" y="720" width="290" height="60" as="geometry"/>
                </mxCell>
                <mxCell id="arrow1" value="" style="endArrow=classic;html=1;rounded=0;strokeWidth=4;strokeColor=#FF6F00;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;" parent="1" source="planning-box" target="training-box" edge="1">
                    <mxGeometry width="50" height="50" relative="1" as="geometry">
                        <mxPoint x="1160" y="490" as="sourcePoint"/>
                        <mxPoint x="1210" y="440" as="targetPoint"/>
                    </mxGeometry>
                </mxCell>
                <mxCell id="arrow-label" value="Quality Foundation&lt;br&gt;Enables Fast Training" style="edgeLabel;html=1;align=center;verticalAlign=middle;resizable=0;points=[];fontSize=13;fontStyle=1;fontColor=#E65100;fillColor=#FFF8E1;strokeColor=#FF8F00;rounded=1;" parent="arrow1" vertex="1" connectable="0">
                    <mxGeometry x="-0.1" y="2" relative="1" as="geometry">
                        <mxPoint x="-3" y="-38" as="offset"/>
                    </mxGeometry>
                </mxCell>
                <object label="🧩 KEY INSIGHT: Data and evaluation dominate the workload. Training and serving are quick once inputs and metrics are correct.&lt;br&gt;&lt;b&gt;&lt;font color=&quot;#4A148C&quot;&gt;The most intellectual part is designing what correctness means&lt;/font&gt;&lt;/b&gt; — the evaluation harness. The most mechanical part is running the training." tooltip="This key insight captures the fundamental asymmetry in machine learning project effort distribution that surprises most newcomers to fine-tuning. Data and evaluation dominate the workload means that 90 percent of project time is spent on activities that do not look like traditional AI work: cleaning datasets, writing validation scripts, designing test cases, debugging data format issues, and documenting decisions. This is not inefficiency - it reflects the reality that model quality is determined by input quality and measurement quality, not training hyperparameters. Training and serving are quick once inputs and metrics are correct means that the actual LoRA training (5 percent), model packaging (2 percent), and deployment (3 percent) are relatively fast, well-documented procedures that rarely require extensive debugging when the foundation is solid. A typical training run takes 4-12 hours on M1 hardware, packaging takes 15-25 minutes, and deployment takes minutes. Compare this to weeks spent on dataset curation and evaluation design. The most intellectual part is designing what correctness means - this is the hardest and most important challenge in any ML project. You must define quantitative metrics that capture model quality: What does it mean for generated code to be correct? Is 95 percent JSON validity good enough for production? How do you measure whether guardrails are too permissive or too restrictive? These questions require domain expertise, critical thinking, and iterative refinement. The evaluation harness is your ground truth - if your metrics are wrong, you will optimize for the wrong objectives and ship a model that fails in production despite passing all tests. This intellectual work cannot be automated or delegated to frameworks. The most mechanical part is running the training refers to the fact that once you have clean data and good evaluation metrics, the actual training process is relatively straightforward: configure TRL and PEFT with established best practices, run the training script, monitor loss curves, and wait for convergence. Frameworks like TRL handle the complexity of training loops, gradient accumulation, and checkpointing. This is mechanical work in the sense that you are following established procedures rather than solving novel problems. The contrast between intellectual (evaluation design) and mechanical (training execution) highlights where human expertise adds value versus where automation suffices. This insight has practical implications for project planning: allocate senior talent to evaluation design and dataset quality, use junior talent or automation for training execution and deployment. Recognize that the lack of visible training activity does not mean the project is stalled - the foundational work in the 90 percent phase is what enables the 10 percent execution phase to succeed quickly and reliably." id="bottom-summary">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F3E5F5;strokeColor=#7B1FA2;strokeWidth=2;align=center;verticalAlign=middle;fontSize=13;fontStyle=2" parent="1" vertex="1">
                        <mxGeometry x="50" y="880" width="1500" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="legend-title" value="LEGEND" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=12;fontStyle=1" parent="1" vertex="1">
                    <mxGeometry x="50" y="960" width="100" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend1" value="" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#C8E6C9;strokeColor=#388E3C;strokeWidth=2;" parent="1" vertex="1">
                    <mxGeometry x="50" y="990" width="30" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend1-text" value="Data Foundation (25% + 10%)" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=11;" parent="1" vertex="1">
                    <mxGeometry x="90" y="990" width="200" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend2" value="" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#BBDEFB;strokeColor=#1976D2;strokeWidth=2;" parent="1" vertex="1">
                    <mxGeometry x="300" y="990" width="30" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend2-text" value="Evaluation &amp; QA (20% + 15%)" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=11;" parent="1" vertex="1">
                    <mxGeometry x="340" y="990" width="200" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend3" value="" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#F0F4C3;strokeColor=#AFB42B;strokeWidth=2;" parent="1" vertex="1">
                    <mxGeometry x="550" y="990" width="30" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend3-text" value="Documentation &amp; Governance (10% + 10%)" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=11;" parent="1" vertex="1">
                    <mxGeometry x="590" y="990" width="260" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend4" value="" style="rounded=1;whiteSpace=wrap;html=1;fillColor=#FFCCBC;strokeColor=#D84315;strokeWidth=2;" parent="1" vertex="1">
                    <mxGeometry x="860" y="990" width="30" height="20" as="geometry"/>
                </mxCell>
                <mxCell id="legend4-text" value="Training, Packaging, Deployment (5% + 2% + 3%)" style="text;html=1;strokeColor=none;fillColor=none;align=left;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontSize=11;" parent="1" vertex="1">
                    <mxGeometry x="900" y="990" width="320" height="20" as="geometry"/>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
</mxfile>