<mxfile host="65bd71144e">
    <diagram name="Continue Architecture - VSCode SSH with Ollama" id="continue-architecture">
        <mxGraphModel dx="1542" dy="711" grid="1" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="1" pageScale="1" pageWidth="1400" pageHeight="1000" math="0" shadow="0">
            <root>
                <mxCell id="0"/>
                <mxCell id="1" parent="0"/>
                <object label="Continue Architecture: VSCode over SSH with Ollama Offline Models" tooltip="Complete architecture for using Continue extension in VSCode running on Kubuntu, while leveraging powerful offline AI models (via Ollama) hosted on a remote Mac M1. The connection uses SSH for workspace access and an additional tunnel for the Ollama API, enabling seamless access to local models from the remote development environment." id="title">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=18;fontStyle=1;fillColor=#dae8fc;strokeColor=#6c8ebf;verticalAlign=top;spacingTop=10;spacingBottom=10;" parent="1" vertex="1">
                        <mxGeometry x="40" y="20" width="1320" height="60" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Kubuntu 24.04 Machine" tooltip="The local development machine running Kubuntu 24.04 with VSCode and Continue extension v1.2.9. This machine provides the user interface and development environment while connecting to the remote Mac M1 for workspace access and AI model capabilities. The machine handles SSH connections, tunnel management, and provides the primary development interface for the user." id="kubuntu-title">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=16;fontStyle=1;fillColor=#d5e8d4;strokeColor=#82b366;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="40" y="100" width="600" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <object label="VSCode on Kubuntu&lt;br&gt;&lt;br&gt;• Main development interface&lt;br&gt;• File operations &amp; editing&lt;br&gt;• Remote workspace access&lt;br&gt;• Hosts Continue extension" tooltip="Visual Studio Code running on Kubuntu 24.04, providing the main development interface. VSCode handles file operations, code editing, and workspace management through its Remote - SSH extension connection to the Mac M1. It serves as the primary user interface where developers write code, manage files, and interact with the Continue extension for AI-powered assistance." id="vscode-main">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=12;fillColor=#d5e8d4;strokeColor=#82b366;verticalAlign=top;spacingTop=5;" vertex="1" parent="1">
                        <mxGeometry x="60" y="160" width="250" height="110" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Continue Extension v1.2.9&lt;br&gt;&lt;br&gt;• AI-powered code assistance&lt;br&gt;• Natural language prompts&lt;br&gt;• Tool calling capabilities&lt;br&gt;• Connects to Ollama models" tooltip="Continue extension running inside VSCode, providing AI-powered code assistance and tool calling capabilities. Continue enables developers to interact with AI models through natural language prompts, code generation, refactoring assistance, and automated tool usage. The extension connects to Ollama models via SSH tunnel and can perform file operations, run terminal commands, and execute various development tasks through its comprehensive tool set." id="continue-extension">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=12;fillColor=#d5e8d4;strokeColor=#82b366;verticalAlign=top;spacingTop=5;" vertex="1" parent="1">
                        <mxGeometry x="60" y="300" width="250" height="110" as="geometry"/>
                    </mxCell>
                </object>
                <object label="SSH Client &amp; Terminal&lt;br&gt;&lt;br&gt;• Remote connections&lt;br&gt;• Port forwarding&lt;br&gt;• Tunnel management&lt;br&gt;• Dual connection types" tooltip="SSH client for establishing remote connections to the Mac M1 and terminal for managing SSH tunnels. The SSH client handles the Remote - SSH connection for workspace access, while the terminal is used to establish and maintain the port forwarding tunnel (11434) required for Continue to access the Ollama API on the remote machine. Manages both VSCode workspace access and Continue AI model access through different connection mechanisms." id="ssh-client">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=12;fillColor=#d5e8d4;strokeColor=#82b366;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="410" y="160" width="140" height="260" as="geometry"/>
                    </mxCell>
                </object>
                <object label="SSH Remote Connection" tooltip="Handles VSCode Remote workspace access to Mac M1 file system." id="ssh-remote-section">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fontSize=10;fillColor=#ffe6cc;strokeColor=#d79b00;verticalAlign=top;spacingTop=2;align=center;" vertex="1" parent="1">
                        <mxGeometry x="415" y="270" width="130" height="50" as="geometry"/>
                    </mxCell>
                </object>
                <object label="SSH Tunnel (Port 11434)" tooltip="Handles port forwarding tunnel for Continue to access Ollama API on Mac M1." id="ssh-tunnel-section">
                    <mxCell style="rounded=0;whiteSpace=wrap;html=1;fontSize=10;fillColor=#ffe6cc;strokeColor=#FF6B35;verticalAlign=top;spacingTop=2;" vertex="1" parent="1">
                        <mxGeometry x="415" y="340" width="130" height="50" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Continue Tools Available&lt;br&gt;&lt;br&gt;• read_file&lt;br&gt;• create_new_file&lt;br&gt;• edit_existing_file&lt;br&gt;• run_terminal_command&lt;br&gt;• file_glob_search&lt;br&gt;• view_diff&lt;br&gt;• create_rule_block&lt;br&gt;• fetch_url_content" tooltip="Continue provides a comprehensive set of tools that can be called by supported models to automate coding tasks. These tools enable AI models to interact directly with the development environment, file system, and external resources. File operations include read_file (reads files from specified paths), create_new_file (creates new files with provided content), and edit_existing_file (modifies existing files with specific changes). Terminal integration includes run_terminal_command (executes shell commands and scripts). Search capabilities include file_glob_search (finds files matching patterns) and view_diff (shows git differences). Specialized functions include create_rule_block (creates or edits code generation rules) and fetch_url_content (retrieves content from web URLs). Most offline models struggle with reliable tool calling, with DeepSeek being the most capable for consistent tool usage, though even it requires careful prompting and may need multiple attempts." id="continue-tools">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#fff2cc;strokeColor=#d6b656;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="60" y="433" width="250" height="160" as="geometry"/>
                    </mxCell>
                </object>
                <object label="🏆 RECOMMENDED MODEL SEQUENCE&lt;br&gt;&lt;br&gt;Best models to use with Continue&lt;br&gt;in order of preference:&lt;br&gt;&lt;br&gt;🥇 DeepSeek Coder (6.7b)&lt;br&gt;• Best overall experience&lt;br&gt;• More reliable tool usage&lt;br&gt;• Better answers&lt;br&gt;&lt;br&gt;🥈 Qwen2.5 Coder (7b)&lt;br&gt;• Good for general coding&lt;br&gt;• Inconsistent tool calling&lt;br&gt;&lt;br&gt;🥉 StarCoder2 (7b)&lt;br&gt;• Basic coding assistance&lt;br&gt;• Limited tool support" tooltip="Based on extensive practical usage experience with Continue and offline models, this is the recommended sequence of best models to use, ranked in order of preference and performance. DeepSeek Coder (6.7b-instruct-q4_K_M) consistently provides the best overall experience with superior answer quality, more reliable tool calling capabilities, and better understanding of complex coding tasks. It demonstrates the most consistent behavior when using Continue&#39;s tools like read_file, edit_existing_file, and run_terminal_command. However, even DeepSeek requires careful prompting and may need multiple attempts for complex tool operations. Qwen2.5 Coder (7b-instruct-q4_K_M) excels at general coding tasks and provides good code generation quality, but struggles with consistent tool calling - sometimes working perfectly and other times failing to use tools appropriately. StarCoder2 (7b) offers basic coding assistance and can handle simple code completion tasks, but has very limited tool support and often claims it cannot perform tasks that other models handle successfully. All models are quantized (q4_K_M) for efficient local inference while maintaining reasonable performance. The tool calling inconsistency is a common challenge with offline models, making DeepSeek the most practical choice for Continue integration despite its occasional limitations." id="model-recommendations">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#e1d5e7;strokeColor=#9673a6;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="600" y="380" width="270" height="270" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Mac M1 (32GB RAM)" tooltip="The remote Mac M1 machine with 32GB RAM hosting the Ollama server and AI models. This machine provides the computational power for running large language models locally, ensuring privacy and offline operation. The Mac M1 serves both as the workspace host (via SSH Remote) and the AI model server (via Ollama API), enabling seamless integration between development environment and AI capabilities." id="mac-title">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=16;fontStyle=1;fillColor=#dae8fc;strokeColor=#6c8ebf;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="760" y="100" width="600" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <object label="SSH Server (sshd)&lt;br&gt;&lt;br&gt;• Accepts connections&lt;br&gt;• Routes requests&lt;br&gt;• Port forwarding&lt;br&gt;• Authentication" tooltip="SSH server running on Mac M1 that accepts incoming connections from the Kubuntu SSH client. The server handles both the VSCode Remote workspace connection and the port forwarding tunnel for Ollama API access. It manages authentication, connection routing, and port forwarding to enable seamless communication between Kubuntu and Mac M1 services." id="ssh-server">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=12;fillColor=#dae8fc;strokeColor=#6c8ebf;verticalAlign=top;spacingTop=5;" vertex="1" parent="1">
                        <mxGeometry x="760" y="180" width="100" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="VSCode Remote Workspace&lt;br&gt;&lt;br&gt;• File system access&lt;br&gt;• Project files storage&lt;br&gt;• Development environment&lt;br&gt;• Continue configuration" tooltip="The remote workspace on Mac M1 accessed via SSH Remote - SSH extension. This provides the actual file system and development environment where code is stored and executed. The workspace contains the project files, Continue configuration, and serves as the target for all development operations. The user interacts with this workspace through the VSCode interface running on Kubuntu." id="remote-workspace">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=12;fillColor=#dae8fc;strokeColor=#6c8ebf;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="935" y="160" width="210" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Ollama Server (localhost:11434)&lt;br&gt;&lt;br&gt;• AI model hosting&lt;br&gt;• REST API endpoints&lt;br&gt;• Local inference&lt;br&gt;• Privacy &amp; performance" tooltip="Ollama server running on Mac M1 serving AI models on localhost:11434. The server hosts multiple models including DeepSeek Coder (6.7b-instruct-q4_K_M), Qwen2.5 Coder (7b-instruct-q4_K_M), and StarCoder2 (7b). The server provides REST API endpoints for model inference, chat completion, and tool calling capabilities. Models are loaded and managed locally, ensuring fast response times and complete privacy." id="ollama-server">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=12;fillColor=#dae8fc;strokeColor=#6c8ebf;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="935" y="270" width="210" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Continue Configuration&lt;br&gt;(.continue/config.yaml)&lt;br&gt;&lt;br&gt;• providers: ollama&lt;br&gt;• apiBase: http://127.0.0.1:11434&lt;br&gt;• models: [DeepSeek, Qwen, StarCoder2]&lt;br&gt;• toolCalling: true&lt;br&gt;• Enables Ollama connection" tooltip="Continue configuration file on Mac M1 defining model providers, API endpoints, and model settings. The configuration specifies Ollama as the provider with apiBase set to http://127.0.0.1:11434, and defines multiple models with toolCalling enabled. This configuration enables Continue to connect to the local Ollama server and use the available models for code generation and tool calling. The YAML structure includes providers section specifying Ollama as the model provider, apiBase pointing to the local Ollama server endpoint, and models array defining the available models (DeepSeek Coder, Qwen2.5 Coder, StarCoder2) with their specific configurations. Each model entry includes the model name, provider reference, model identifier, and toolCalling: true to enable Continue&#39;s tool usage capabilities. The configuration file is located in the .continue directory on the Mac M1 and is read by Continue when it starts up, establishing the connection to the Ollama server through the SSH tunnel. This configuration is essential for Continue to know which models are available, how to connect to them, and which capabilities each model supports." id="continue-config">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#ffe6cc;strokeColor=#d79b00;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="1170" y="320" width="220" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Available Models on Ollama&lt;br&gt;&lt;br&gt;• deepseek-coder:6.7b-instruct-q4_K_M&lt;br&gt;• qwen2.5-coder:7b-instruct-q4_K_M&lt;br&gt;• starcoder2:7b&lt;br&gt;• All with toolCalling: true&lt;br&gt;• Quantized for efficiency&lt;br&gt;• Local inference only" tooltip="Local AI models hosted on Ollama server on Mac M1, providing offline AI capabilities for Continue. DeepSeek Coder (6.7b-instruct-q4_K_M) offers the best tool support and user experience, with superior understanding of complex coding tasks and more reliable tool calling capabilities. Qwen2.5 Coder (7b-instruct-q4_K_M) excels at general coding tasks and provides good code generation quality, though with inconsistent tool calling behavior. StarCoder2 (7b) provides basic coding assistance and simple code completion tasks, but with limited tool support. All models are configured with toolCalling: true in the Continue configuration to enable Continue&#39;s tool usage capabilities. Models are quantized (q4_K_M) for efficient local inference while maintaining reasonable performance, allowing them to run on the Mac M1&#39;s 32GB RAM. The local inference ensures complete privacy and offline operation, with no data sent to external services. Models are loaded and managed by Ollama server, providing REST API endpoints for model inference, chat completion, and tool calling capabilities that Continue can access through the SSH tunnel." id="available-models">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#ffe6cc;strokeColor=#d79b00;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="1170" y="160" width="220" height="140" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Data Flow &amp; Communication" tooltip="The complete data flow and communication architecture showing how user interactions in VSCode on Kubuntu are processed through dual SSH connections to utilize AI models on Mac M1. This comprehensive flow demonstrates the sophisticated interaction between local development environment and remote AI capabilities. The architecture supports two distinct communication paths: workspace operations via SSH Remote connection for file system access, and AI model requests via SSH tunnel for Ollama API access. The data flow begins when a user interacts with Continue in VSCode on Kubuntu, providing natural language prompts, code generation requests, or refactoring instructions. Continue then sends requests to localhost:11434 on Kubuntu, which appears to be a local Ollama server but is actually forwarded through the SSH tunnel to the Mac M1 Ollama server. The SSH tunnel (established with &#39;ssh -N -L 11434:127.0.0.1:11434 mac-via-home&#39;) forwards the API request from Kubuntu&#39;s localhost:11434 to Mac M1&#39;s localhost:11434 where Ollama is running. The Ollama server processes the request using the selected offline model (DeepSeek, Qwen, or StarCoder2), generating responses and potentially executing tool calls as requested by Continue. Model responses, generated code, and tool execution results are then sent back through the SSH tunnel to Continue in VSCode on Kubuntu, completing the round-trip communication. This architecture enables seamless integration between the local development environment and powerful AI capabilities while maintaining complete privacy through offline model execution. The dual SSH connection approach allows for efficient workspace management and AI model access without compromising security or performance." id="data-flow-title">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=16;fontStyle=1;fillColor=#d5e8d4;strokeColor=#82b366;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="40" y="670" width="1320" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <object label="1. User Interaction&lt;br&gt;&lt;br&gt;• Natural language prompts&lt;br&gt;• Code generation requests&lt;br&gt;• Refactoring instructions&lt;br&gt;• Triggers AI communication" tooltip="The data flow begins when a user interacts with Continue in VSCode on Kubuntu, providing natural language prompts, code generation requests, or refactoring instructions through the Continue extension interface. This interaction represents the starting point of the entire AI-assisted development process. Users can request various types of assistance including code completion, bug fixes, feature implementations, code refactoring, documentation generation, and complex architectural changes. The Continue extension provides an intuitive chat interface where developers can describe their needs in natural language, ask questions about code, request specific modifications, or seek explanations of complex code patterns. This step is crucial as it establishes the context and requirements for the AI model to process. The user&#39;s input is processed by Continue, which then determines the appropriate tools to use and prepares the request for the AI model. This interaction can range from simple code completions to complex multi-step refactoring tasks that require understanding of the entire codebase structure and dependencies." id="step1">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#d5e8d4;strokeColor=#82b366;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="60" y="750" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="2. Continue API Request&lt;br&gt;&lt;br&gt;• Sends to localhost:11434&lt;br&gt;• Appears as local Ollama&lt;br&gt;• Actually tunneled request&lt;br&gt;• Prepares for forwarding" tooltip="Continue extension processes the user&#39;s request and attempts to communicate with what appears to be a local Ollama API on localhost:11434 on Kubuntu. This step is crucial as it demonstrates the transparency of the SSH tunnel architecture - Continue believes it&#39;s communicating with a local Ollama server, but the request is actually being prepared for forwarding through the SSH tunnel. The extension formats the request according to Ollama&#39;s API specification, including the user&#39;s prompt, any context from the current file or workspace, and specifications for which tools should be available to the model. Continue also includes metadata about the current session, file context, and any previous interactions that might be relevant. The request is structured as a standard HTTP API call to localhost:11434, which would normally connect to a local Ollama instance, but in this architecture, the SSH tunnel intercepts this connection and forwards it to the remote Mac M1. This transparent forwarding allows Continue to work exactly as it would with a local Ollama installation, while actually leveraging the powerful remote models." id="step2">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#d5e8d4;strokeColor=#82b366;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="310" y="750" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="3. SSH Tunnel Forwarding&lt;br&gt;&lt;br&gt;• Port 11434 forwarding&lt;br&gt;• Kubuntu → Mac M1&lt;br&gt;• Transparent routing&lt;br&gt;• Secure connection" tooltip="The SSH tunnel (established with &#39;ssh -N -L 11434:127.0.0.1:11434 mac-via-home&#39;) forwards the API request from Kubuntu&#39;s localhost:11434 to Mac M1&#39;s localhost:11434 where Ollama is running. This step represents the critical network routing that makes the entire architecture possible. The SSH tunnel creates a secure, encrypted connection between the two machines, forwarding all traffic from Kubuntu&#39;s port 11434 to Mac M1&#39;s port 11434. This port forwarding is transparent to both Continue (which thinks it&#39;s talking to a local server) and Ollama (which receives requests as if they came from localhost). The tunnel maintains the connection state, handles authentication, and ensures data integrity throughout the communication. The &#39;-N&#39; flag prevents the SSH session from executing remote commands, making it purely a tunnel for port forwarding. The &#39;-L&#39; flag specifies local port forwarding, binding Kubuntu&#39;s localhost:11434 to Mac M1&#39;s localhost:11434. This creates a seamless bridge that allows Continue to access remote AI models as if they were running locally, while maintaining the security and privacy benefits of the SSH connection." id="step3">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#ffe6cc;strokeColor=#d79b00;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="550" y="750" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="4. Ollama Model Processing&lt;br&gt;&lt;br&gt;• Offline model inference&lt;br&gt;• DeepSeek/Qwen/StarCoder2&lt;br&gt;• Tool calling execution&lt;br&gt;• Response generation" tooltip="Ollama server on Mac M1 processes the request using the selected offline model (DeepSeek, Qwen, or StarCoder2), generating responses and potentially executing tool calls as requested by Continue. This step represents the core AI processing where the actual model inference occurs. The Ollama server receives the formatted request from Continue and routes it to the appropriate model based on the configuration. The selected model (typically DeepSeek for best tool support, Qwen for general coding, or StarCoder2 for basic assistance) processes the user&#39;s prompt along with any provided context, file contents, or workspace information. During processing, the model may decide to use Continue&#39;s tools to perform actions like reading files, creating new files, editing existing code, running terminal commands, or searching for specific patterns. The model generates a response that includes both natural language explanations and potentially tool calls that need to be executed. This processing happens entirely offline on the Mac M1, ensuring complete privacy and no data transmission to external services. The model&#39;s response is then formatted according to Ollama&#39;s API specification and prepared for transmission back through the SSH tunnel to Continue." id="step4">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#dae8fc;strokeColor=#6c8ebf;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="820" y="750" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="5. Response Return Path&lt;br&gt;&lt;br&gt;• Model responses&lt;br&gt;• Generated code&lt;br&gt;• Tool execution results&lt;br&gt;• Complete round-trip" tooltip="Model responses, generated code, and tool execution results are sent back through the SSH tunnel to Continue in VSCode on Kubuntu, completing the round-trip communication. This final step represents the completion of the entire AI-assisted development cycle. The Ollama server packages the model&#39;s response, which may include natural language explanations, generated code snippets, refactored code, documentation, or results from tool executions. The response is formatted according to Ollama&#39;s API specification and transmitted back through the same SSH tunnel that carried the original request. The tunnel ensures secure, encrypted transmission of the response data back to Kubuntu. Continue receives the response and processes it, potentially executing any tool calls that the model requested (like file modifications, terminal commands, or searches). The extension then presents the results to the user through the VSCode interface, which may include displaying generated code, applying changes to files, showing explanations, or providing interactive elements for the user to review and accept changes. This completes the full cycle from user input to AI processing to result delivery, all while maintaining the privacy and security benefits of the offline, tunneled architecture. The user can then continue the conversation, request modifications, or start new tasks, beginning the cycle again." id="step5">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#ffe6cc;strokeColor=#d79b00;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="1085" y="750" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Common Issues &amp; Solutions" tooltip="Comprehensive troubleshooting guide for common problems encountered when using Continue with SSH and Ollama setup. This section addresses the most frequent issues that users face when setting up and using the Continue extension with offline models through SSH tunnels. Connection issues include SSH tunnel failures, port forwarding problems, authentication errors, and network connectivity issues that prevent Continue from reaching the Ollama server. Model response problems cover issues with slow or unresponsive models, context length limitations, memory constraints, and model loading failures that affect the quality and speed of AI assistance. Tool calling difficulties address problems with Continue&#39;s tool integration, including file access permissions, terminal command execution failures, search functionality issues, and tool response formatting problems. Performance issues include slow model inference, high memory usage, CPU bottlenecks, and system resource constraints that impact the overall user experience. Configuration problems cover incorrect Continue settings, Ollama configuration issues, SSH key problems, and environment variable conflicts that prevent proper system operation. Security concerns include SSH key management, tunnel security, file permission issues, and access control problems that could compromise system security. This comprehensive troubleshooting guide provides step-by-step solutions, diagnostic commands, configuration examples, and best practices to resolve these common issues and ensure smooth operation of the Continue extension with offline models." id="troubleshooting-title">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=16;fontStyle=1;fillColor=#f8cecc;strokeColor=#b85450;verticalAlign=top;spacingTop=10;" parent="1" vertex="1">
                        <mxGeometry x="40" y="929" width="1320" height="40" as="geometry"/>
                    </mxCell>
                </object>
                <object label="SSH Tunnel Problems&lt;br&gt;&lt;br&gt;• Connection failures&lt;br&gt;• Port forwarding issues&lt;br&gt;• Authentication errors&lt;br&gt;• Network connectivity" tooltip="SSH tunnel problems are among the most common issues when setting up Continue with Ollama over SSH. These problems typically manifest as Continue being unable to connect to the Ollama server, resulting in error messages or complete failure to establish communication. Connection failures occur when the SSH tunnel cannot be established due to network issues, incorrect host configurations, or firewall restrictions blocking the connection. Port forwarding issues arise when the local port 11434 cannot be properly forwarded to the remote Ollama server, often due to port conflicts, incorrect tunnel syntax, or SSH configuration problems. Authentication errors happen when SSH key authentication fails, passwords are incorrect, or SSH agent forwarding is not properly configured. Network connectivity issues include DNS resolution problems, routing issues, or network timeouts that prevent the SSH connection from being established. Solutions include verifying the SSH tunnel command syntax (&#39;ssh -N -L 11434:127.0.0.1:11434 mac-via-home&#39;), checking SSH key permissions and authentication, ensuring the remote Ollama server is running and accessible, testing network connectivity with ping and telnet commands, verifying firewall settings on both local and remote machines, checking SSH configuration files for correct host settings, and using verbose SSH logging (&#39;ssh -v&#39;) to diagnose connection issues. Diagnostic commands include &#39;curl http://localhost:11434/api/tags&#39; to verify tunnel connectivity, &#39;ssh -T mac-via-home&#39; to test SSH connection, and &#39;netstat -an | grep 11434&#39; to check port binding." id="tunnel-issues">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#f8cecc;strokeColor=#b85450;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="60" y="989" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Model Response Issues&lt;br&gt;&lt;br&gt;• No responses received&lt;br&gt;• Out-of-context answers&lt;br&gt;• Slow processing times&lt;br&gt;• Memory constraints" tooltip="Model response issues are frequent challenges when using offline LLMs through Continue, particularly affecting the quality and reliability of AI assistance. No responses received occurs when prompts are sent but the model fails to generate any output, often due to context length limitations, memory constraints, or model loading failures. Out-of-context answers happen when the model provides responses that don&#39;t match the user&#39;s intent or the current conversation context, typically caused by insufficient context information, model confusion, or prompt formatting issues. Slow processing times result from model inference delays, system resource constraints, or network latency through the SSH tunnel, making the development experience less responsive. Memory constraints occur when the model runs out of available memory for processing large contexts or complex requests, leading to truncated responses or complete failures. Solutions include rephrasing prompts to be more specific and clear, breaking complex requests into smaller, manageable parts, providing more context about the current task and codebase, using shorter prompts to avoid context length limitations, restarting the Ollama server to clear memory issues, checking system resources (CPU, RAM) on the Mac M1, monitoring model performance and switching to lighter models if needed, and implementing prompt engineering techniques to improve response quality. Best practices include being explicit about desired outputs, providing clear examples in prompts, using consistent formatting for code requests, and maintaining conversation context by referencing previous interactions." id="response-issues">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#f8cecc;strokeColor=#b85450;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="415" y="989" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Tool Calling Problems&lt;br&gt;&lt;br&gt;• Models refuse tasks&lt;br&gt;• Inconsistent tool execution&lt;br&gt;• Permission errors&lt;br&gt;• Tool response failures" tooltip="Tool calling problems represent significant challenges in getting offline models to effectively use Continue&#39;s tools for file operations, terminal commands, and code manipulation. Models refuse tasks when they claim to be AI assistants that cannot perform specific actions, often due to training limitations, safety constraints, or misunderstanding of their capabilities in the Continue environment. Inconsistent tool execution occurs when tools work sometimes but fail unpredictably, typically caused by context switching issues, tool availability problems, or model confusion about when and how to use specific tools. Permission errors arise when the model attempts to perform file operations or terminal commands that require elevated privileges or access to restricted directories, often due to SSH user permissions or file system restrictions. Tool response failures happen when tools are called but don&#39;t return expected results, including file read/write failures, terminal command execution problems, or search functionality issues. Solutions include using DeepSeek models which have better tool calling capabilities and understanding, being explicit about wanting to use tools in prompts, breaking complex tasks into smaller, more manageable steps, providing clear instructions about which tools to use and how, checking file permissions and SSH user access rights, verifying that required tools and commands are available on the remote system, using specific tool names and parameters in requests, and implementing error handling for tool failures. Best practices include starting with simple tool operations to test functionality, providing examples of desired tool usage, using consistent tool calling patterns, and monitoring tool execution logs for debugging issues." id="tool-issues">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#f8cecc;strokeColor=#b85450;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="800" y="989" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Connection Drops&lt;br&gt;&lt;br&gt;• SSH tunnel failures&lt;br&gt;• Mac M1 unavailability&lt;br&gt;• Network interruptions&lt;br&gt;• Service restarts needed" tooltip="Connection drops are disruptive issues that occur when the SSH tunnel between Kubuntu and Mac M1 fails, or when the Mac M1 becomes unavailable, resulting in complete loss of Continue functionality. SSH tunnel failures happen when the established tunnel connection is lost due to network interruptions, SSH server restarts, or connection timeouts, leaving Continue unable to communicate with the remote Ollama server. Mac M1 unavailability occurs when the remote machine becomes unreachable due to network issues, system crashes, sleep mode activation, or service failures, preventing any communication with the Ollama server. Network interruptions include temporary connectivity issues, router problems, or ISP outages that break the connection between the local and remote machines. Service restarts needed arise when the Ollama server on Mac M1 stops responding, requires updates, or encounters errors that necessitate restarting the service. Symptoms include Continue showing no response to prompts, VSCode displaying warnings about installing Ollama locally, error messages about connection failures, and complete loss of AI assistance functionality. Solutions include re-establishing the SSH tunnel with the correct command (&#39;ssh -N -L 11434:127.0.0.1:11434 mac-via-home&#39;), checking Mac M1 connectivity with ping and SSH tests, restarting the Ollama service on the remote machine, verifying network connectivity and router status, checking for system updates or restarts on Mac M1, implementing SSH connection keep-alive settings, using SSH multiplexing for more stable connections, and setting up automatic tunnel reconnection scripts. Preventive measures include monitoring connection stability, implementing health checks for the remote system, using reliable network connections, and maintaining up-to-date SSH and Ollama configurations." id="connection-issues">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=11;fillColor=#f8cecc;strokeColor=#b85450;verticalAlign=top;spacingTop=5;" parent="1" vertex="1">
                        <mxGeometry x="1130" y="989" width="200" height="100" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Legend&lt;br&gt;&lt;br&gt;🔵 Light Blue: Titles &amp; Mac M1&lt;br&gt;🟢 Green: Kubuntu Components&lt;br&gt;🟠 Orange: SSH &amp; Connections&lt;br&gt;🟡 Yellow: Continue Tools&lt;br&gt;🟣 Purple: Model Recommendations&lt;br&gt;🔴 Red: Troubleshooting Issues" tooltip="The Legend provides a comprehensive guide to understanding the visual elements and color coding system used throughout the Continue Architecture diagram. Light Blue (#dae8fc) represents main titles, section headers, and all Mac M1 components including SSH server, VSCode remote workspace, Ollama server, and step 4 (Ollama Model Processing). Green (#d5e8d4) identifies all Kubuntu-related components including the machine section, VSCode, Continue extension, SSH client, and data flow steps 1 and 2 (User Interaction and Continue API Request) that occur on Kubuntu. Orange (#ffe6cc) represents SSH tunnel components, connection infrastructure, Continue configuration, available models, and data flow steps 3 and 5 (SSH Tunnel Forwarding and Response Return Path) that show the communication between machines. Yellow (#fff2cc) indicates Continue tools and capabilities that are available for AI model interaction and automation. Purple (#e1d5e7) highlights the special model recommendation block that provides guidance on the best models to use. Red (#f8cecc) identifies troubleshooting sections and problem-related elements that require attention or represent potential issues. This logical color coding system helps users quickly identify component types, understand the architecture flow, distinguish between local (green) and remote (blue) elements, and navigate the complex diagram structure effectively. The consistent use of these colors throughout the diagram creates visual coherence and makes it easier to understand the relationships between different components in the Continue architecture." id="legend">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=10;fillColor=#f5f5f5;strokeColor=#666666;align=center;verticalAlign=top;" parent="1" vertex="1">
                        <mxGeometry x="400" y="1180" width="200" height="130" as="geometry"/>
                    </mxCell>
                </object>
                <object label="Connection Legend&lt;br&gt;&lt;br&gt;━━━ Solid Lines: Direct Connections&lt;br&gt;┅┅┅ Dashed Lines: Data Flow&lt;br&gt;↕️ Dual Direction: VSCode-Continue&lt;br&gt;→ Unidirectional: SSH Connections&lt;br&gt;🔄 Round-trip: Complete Cycle" tooltip="The Connection Legend explains the visual indicators and connection types used throughout the Continue Architecture diagram. Solid lines (━━━) represent direct, persistent connections between components, such as the SSH Remote connection between VSCode and the Mac M1 workspace, and the SSH tunnel connection between Continue and Ollama. These connections are established once and maintained throughout the session. Dashed lines (┅┅┅) represent data flow and communication patterns, showing how information moves through the system, including the 5-step data flow process from user interaction to response return. Dual direction arrows (↕️) indicate two-way communication using separate arrows in each direction, such as the intimate relationship between VSCode and Continue where Continue can execute VSCode commands and VSCode can provide context to Continue. Unidirectional arrows (→) show one-way communication flows, such as SSH connections where the client initiates connections to the server. Round-trip indicators (🔄) represent complete communication cycles, such as the full data flow from user input through Continue to Ollama and back to the user. This legend helps users understand the different types of relationships and communication patterns in the architecture, making it easier to follow the flow of data and understand how components interact with each other." id="connection-legend">
                    <mxCell style="rounded=1;whiteSpace=wrap;html=1;fontSize=10;fillColor=#f0f8ff;strokeColor=#4682b4;align=center;verticalAlign=top;" vertex="1" parent="1">
                        <mxGeometry x="840" y="1180" width="200" height="110" as="geometry"/>
                    </mxCell>
                </object>
                <mxCell id="vscode-to-continue" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#82b366;dashed=1;" edge="1" parent="1" source="vscode-main" target="continue-extension">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="continue-to-vscode" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#82b366;dashed=1;" edge="1" parent="1" source="continue-extension" target="vscode-main">
                    <mxGeometry relative="1" as="geometry">
                        <Array as="points">
                            <mxPoint x="30" y="355"/>
                            <mxPoint x="30" y="215"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
                <mxCell id="vscode-to-ssh" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#6c8ebf;" edge="1" parent="1" source="vscode-main" target="ssh-client">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="continue-to-ssh" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=-0.027;entryY=0.779;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#FF6B35;entryPerimeter=0;" edge="1" parent="1" source="continue-extension" target="ssh-client">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="ssh-client-to-server" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=4;strokeColor=#6c8ebf;" edge="1" parent="1" source="ssh-client" target="ssh-server">
                    <mxGeometry relative="1" as="geometry">
                        <Array as="points">
                            <mxPoint x="655" y="290"/>
                            <mxPoint x="655" y="250"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
                <mxCell id="ssh-server-to-workspace" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.3;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#6c8ebf;" edge="1" parent="1" source="ssh-server" target="remote-workspace">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="ssh-server-to-ollama" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.7;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=3;strokeColor=#FF6B35;dashed=1;" edge="1" parent="1" source="ssh-server" target="ollama-server">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="flow1" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#82b366;" parent="1" source="step1" target="step2" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="flow2" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#82b366;" parent="1" source="step2" target="step3" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="flow3" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#82b366;" parent="1" source="step3" target="step4" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="flow4" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0.5;exitY=1;exitDx=0;exitDy=0;entryX=0.5;entryY=0;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#82b366;" parent="1" source="step4" target="step5" edge="1">
                    <mxGeometry relative="1" as="geometry"/>
                </mxCell>
                <mxCell id="return-flow" style="edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;strokeWidth=2;strokeColor=#82b366;dashed=1;" parent="1" source="step5" target="step1" edge="1">
                    <mxGeometry relative="1" as="geometry">
                        <Array as="points">
                            <mxPoint x="1330" y="800"/>
                            <mxPoint x="1330" y="900"/>
                            <mxPoint x="20" y="900"/>
                            <mxPoint x="20" y="800"/>
                        </Array>
                    </mxGeometry>
                </mxCell>
            </root>
        </mxGraphModel>
    </diagram>
</mxfile>